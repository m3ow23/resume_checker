{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Used: https://huggingface.co/bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Applications\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import sys\n",
    "sys.path.append('..//..')\n",
    "from utils.tokenizer import sent_tokenize\n",
    "from dataset.job_description import job_descriptions\n",
    "\n",
    "# import dataset \n",
    "dataset = pd.read_csv('../../dataset/resume_dataset.csv')\n",
    "\n",
    "# drop unecessary columns\n",
    "dataset = dataset.drop(columns=['ID', 'Resume_html', 'Category'])\n",
    "\n",
    "# import bert\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hr administrator marketing associate', 'hr administrator', 'dedicated customer service manager with [NUMBER] years of experience in hospitality and customer service management', 'respected builder and leader of customer-focused teams strives to instill a shared enthusiastic commitment to customer service', 'focused on customer satisfaction', 'team management', 'marketing savvy', 'conflict resolution techniques', 'training and development', 'skilled multi-tasker', 'client relations specialist', 'missouri dot supervisor training certification', 'certified by ihg in customer loyalty and marketing by segment', 'hilton worldwide general manager training certification', 'accomplished trainer for cross server hospitality systems such as', 'hilton onq', 'opera pms', 'reservation system ors', 'completed courses and seminars in customer service sales strategies inventory control loss prevention safety time management leadership and performance assessment', 'hr administrator marketing associate', 'hr administrator', 'helps to develop policies directs and coordinates activities such as employment compensation labor relations benefits training and employee services', 'prepares employee separation notices and related documentation', 'keeps records of benefits plans participation such as insurance and pension plan personnel transactions such as hires promotions transfers performance reviews and terminations and employee statistics for government reporting', 'advises management in appropriate resolution of employee relations issues', 'administers benefits programs such as life health dental insurance pension plans vacation sick leave leave of absence and employee assistance', 'marketing associate', 'designed and created marketing collateral for sales meetings trade shows and company executives', 'managed the in-house advertising program consisting of print and media collateral pieces', \"assisted in the complete design and launch of the company's website in [NUMBER] months\", 'created an official company page on facebook to facilitate interaction with customers', 'analyzed ratings and programming features of competitors to evaluate the effectiveness of marketing strategies', 'advanced medical claims analyst', 'reviewed medical bills for the accuracy of the treatments tests and hospital stays prior to sanctioning the claims', 'trained to interpret the codes icd-[NUMBER] cpt and terminology commonly used in medical billing to fully understand the paperwork that is submitted by healthcare providers', 'required to have organizational and analytical skills as well as computer skills knowledge of medical terminology and procedures statistics billing standards data analysis and laws regarding medical billing', 'assistant general manager', 'performed duties including but not limited to budgeting and financial management accounting human resources payroll and purchasing', 'established and maintained close working relationships with all departments of the hotel to ensure maximum operation productivity morale and guest service', 'handled daily operations and reported directly to the corporate office', 'hired and trained staff on overall objectives and goals with an emphasis on high customer service', 'marketing and advertising working on public relations with the media government and local businesses and chamber of commerce', 'executive support marketing assistant', 'provided assistance to various department heads executive marketing customer service human resources', 'managed front-end operations to ensure friendly and efficient transactions', 'ensured the swift resolution of customer issues to preserve customer loyalty while complying with company policies', 'exemplified the second-to-none customer service delivery in all interactions with customers and potential clients', 'reservation front office manager', 'owner partner', 'price integrity coordinator', 'business administration', 'jefferson college', 'business administration', 'marketing advertising', 'high school diploma', 'college prep studies', 'sainte genevieve senior high', 'awarded american shrubel leadership scholarship to jefferson college', 'accounting ads advertising analytical skills benefits billing budgeting clients customer service data analysis delivery documentation employee relations financial management government relations human resources insurance labor relations layout marketing marketing collateral medical billing medical terminology office organizational payroll performance reviews personnel policies posters presentations public relations purchasing reporting statistics website']\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(job_desc_sentences)\n\u001b[0;32m      7\u001b[0m tokenized_resume_sentences \u001b[39m=\u001b[39m tokenizer(resume_sentences, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m tokenized_job_desc_sentences \u001b[39m=\u001b[39m tokenizer(job_desc_sentences, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     10\u001b[0m \u001b[39m# resume embeddings\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2523\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2522\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2523\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[0;32m   2524\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2609\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2604\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2605\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2606\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2607\u001b[0m         )\n\u001b[0;32m   2608\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[1;32m-> 2609\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2610\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2611\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2612\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2613\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2614\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2615\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2616\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2617\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2618\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2619\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2620\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2621\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2622\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2623\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2624\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2625\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2626\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2627\u001b[0m     )\n\u001b[0;32m   2628\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2630\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2631\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2647\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2648\u001b[0m     )\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2800\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2790\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2791\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2792\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2793\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2797\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2798\u001b[0m )\n\u001b[1;32m-> 2800\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   2801\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2802\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2803\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2804\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2805\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2806\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2807\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2808\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2809\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2810\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2811\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2812\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2813\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2814\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2815\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2816\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2817\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2818\u001b[0m )\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils.py:737\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    734\u001b[0m     second_ids \u001b[39m=\u001b[39m get_input_ids(pair_ids) \u001b[39mif\u001b[39;00m pair_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     input_ids\u001b[39m.\u001b[39mappend((first_ids, second_ids))\n\u001b[1;32m--> 737\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_prepare_for_model(\n\u001b[0;32m    738\u001b[0m     input_ids,\n\u001b[0;32m    739\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    740\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[0;32m    741\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[0;32m    742\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m    743\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m    744\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m    745\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m    746\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m    747\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m    748\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m    749\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m    750\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m    751\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    752\u001b[0m )\n\u001b[0;32m    754\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils.py:809\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[1;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[0;32m    806\u001b[0m             batch_outputs[key] \u001b[39m=\u001b[39m []\n\u001b[0;32m    807\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[1;32m--> 809\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad(\n\u001b[0;32m    810\u001b[0m     batch_outputs,\n\u001b[0;32m    811\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding_strategy\u001b[39m.\u001b[39;49mvalue,\n\u001b[0;32m    812\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m    813\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m    814\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m    815\u001b[0m )\n\u001b[0;32m    817\u001b[0m batch_outputs \u001b[39m=\u001b[39m BatchEncoding(batch_outputs, tensor_type\u001b[39m=\u001b[39mreturn_tensors)\n\u001b[0;32m    819\u001b[0m \u001b[39mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2939\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   2937\u001b[0m \u001b[39m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[0;32m   2938\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m encoded_inputs:\n\u001b[1;32m-> 2939\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2940\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2941\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat includes \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, but you provided \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(encoded_inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2942\u001b[0m     )\n\u001b[0;32m   2944\u001b[0m required_input \u001b[39m=\u001b[39m encoded_inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]]\n\u001b[0;32m   2946\u001b[0m \u001b[39mif\u001b[39;00m required_input \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(required_input, Sized) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(required_input) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []"
     ]
    }
   ],
   "source": [
    "resume_sentences = sent_tokenize(dataset['Resume_str'][0])\n",
    "job_desc_sentences = sent_tokenize(job_descriptions[3])\n",
    "\n",
    "print(resume_sentences)\n",
    "print(job_desc_sentences)\n",
    "\n",
    "tokenized_resume_sentences = tokenizer(resume_sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "tokenized_job_desc_sentences = tokenizer(job_desc_sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# resume embeddings\n",
    "with torch.no_grad():\n",
    "    resume_embeddings = model(**tokenized_resume_sentences).last_hidden_state\n",
    "    job_desc_embeddings = model(**tokenized_job_desc_sentences).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.4319e-02, -6.5156e-03,  4.2019e-03,  1.1836e-01,  2.6145e-01,\n",
      "          7.6716e-02, -2.4464e-03,  2.1378e-01, -7.7215e-03, -2.2571e-01,\n",
      "         -7.7868e-02,  2.0465e-01,  2.5795e-01, -3.6866e-02, -2.9201e-02,\n",
      "          3.4238e-02,  2.9965e-02,  1.7317e-01,  5.2314e-02, -5.1877e-02,\n",
      "         -2.8945e-01, -1.1396e-01, -3.6949e-02, -2.1253e-02,  8.6654e-02,\n",
      "          1.9215e-01, -1.3624e-01,  4.2553e-02, -3.9289e-01,  8.4857e-02,\n",
      "          1.0184e-01, -3.4405e-01,  2.1968e-01,  1.3380e-01, -1.4706e-01,\n",
      "         -2.2257e-01, -2.0132e-02,  2.3860e-02, -3.2726e-01,  4.6076e-03,\n",
      "         -2.1926e-01, -4.5307e-02,  1.6769e-01, -2.2148e-01, -1.8203e-02,\n",
      "         -2.2088e-01, -9.5639e-02, -1.1149e-01, -1.8860e-01,  7.8613e-02,\n",
      "         -4.3233e-01,  2.9616e-01, -1.2446e-01,  1.4864e-02, -1.7383e-02,\n",
      "          3.0980e-01,  2.8847e-01, -1.7778e-01, -1.7220e-01, -1.8221e-01,\n",
      "          1.2692e-01, -1.3830e-01,  6.5575e-02,  1.1152e-01,  3.4218e-01,\n",
      "         -1.1447e-01,  3.3031e-01,  2.6149e-01, -2.6522e-01, -9.0677e-02,\n",
      "         -1.9990e-01,  4.4073e-02,  1.8556e-02,  2.8952e-02, -1.1641e-01,\n",
      "         -1.3601e-01, -8.2195e-02,  2.7695e-01,  1.5666e-01, -2.2815e-01,\n",
      "          1.4849e-01,  3.2226e-01,  2.8457e-02,  1.1376e-02, -4.6598e-02,\n",
      "          1.8400e-01,  7.5966e-02, -4.8740e-02, -1.8338e-01,  3.1024e-01,\n",
      "         -5.3337e-02,  2.3452e-02, -8.0331e-02,  5.7775e-03, -3.9410e-02,\n",
      "         -1.9359e-01, -5.5467e-02,  4.2234e-02,  2.0964e-02, -1.0995e-01,\n",
      "         -1.1376e-02, -5.9791e-02, -1.0867e-01,  8.6411e-02, -5.5259e-02,\n",
      "         -1.4142e-02,  3.3716e-02, -2.0564e-01, -1.6667e-01,  1.5674e-01,\n",
      "         -6.3186e-02, -1.9257e-01, -6.5930e-03, -9.3642e-02, -2.1764e-01,\n",
      "          4.7172e-01,  4.9370e-02,  2.7143e-02, -2.2632e-02,  3.2538e-02,\n",
      "          1.9684e-01,  8.5702e-02, -1.3313e-01,  3.9834e-01, -3.6784e-02,\n",
      "          1.1578e-01, -1.0702e-01, -1.0756e-01,  8.3136e-03, -3.3489e-01,\n",
      "          1.3243e-01,  1.5289e-01, -1.5793e-01,  1.3734e-01, -3.6996e-02,\n",
      "          8.4443e-02, -2.5733e-02, -5.9044e-02,  7.5523e-02, -1.2558e-01,\n",
      "         -1.0235e-01,  6.5163e-02,  2.7546e-01, -5.0017e-01,  1.3162e-01,\n",
      "         -1.5665e-01,  8.8102e-03,  1.8218e-01, -3.3545e-01,  1.3635e-01,\n",
      "          6.2697e-02,  1.7287e-01, -2.2798e-01, -2.2190e-01, -2.6635e-03,\n",
      "          2.1359e-01,  2.1115e-01,  4.4789e-02, -1.6579e-01,  1.5540e-01,\n",
      "          3.5845e-01,  2.7322e-01, -3.7825e-01,  2.0763e-01, -2.5162e-01,\n",
      "         -2.5330e-02, -2.5429e-01,  2.6577e-01,  2.2049e-02, -1.7005e-02,\n",
      "         -1.7242e-01, -8.4873e-02,  4.2467e-01, -1.0624e-01, -6.0149e-02,\n",
      "          2.2537e-01,  5.6858e-03,  8.3142e-02,  4.1823e-03,  1.9266e-01,\n",
      "         -3.2046e-01,  2.4045e-01,  5.7862e-02, -1.6038e-01,  2.0296e-01,\n",
      "         -1.1737e-01,  2.1241e-01, -2.0591e-01,  1.6520e-01, -9.9573e-02,\n",
      "         -2.6429e-01, -2.6930e-01, -9.2957e-02,  5.7217e-02,  7.1197e-02,\n",
      "         -5.9248e-02,  9.5305e-02, -7.6055e-02,  1.5232e-01, -1.5548e-01,\n",
      "          3.2635e-01, -8.2688e-02,  1.0471e-01,  1.1899e-01, -2.3893e-01,\n",
      "         -3.6594e-02,  2.9775e-01, -2.4288e-01, -1.5894e-01,  8.8012e-02,\n",
      "         -2.0987e-01,  1.3490e-01, -7.8957e-02,  2.4285e-02,  2.7236e-01,\n",
      "          4.4808e-02, -1.6145e-01,  6.0142e-02, -1.5551e-01,  8.6441e-02,\n",
      "          3.4396e-01, -1.1728e-01, -1.6560e-01,  3.3994e-01, -4.2788e-01,\n",
      "          9.2246e-02,  1.1211e-01, -3.0029e-01,  2.4064e-01,  2.3122e-01,\n",
      "          9.1937e-02, -1.5894e-01,  1.0744e-01, -1.9255e-01, -3.7663e-02,\n",
      "          3.4665e-01, -3.5467e-01, -1.2625e-01,  9.3553e-02, -3.0419e-02,\n",
      "         -1.8680e-01,  1.5254e-01, -3.2553e-02,  2.0444e-01,  1.5993e-01,\n",
      "          1.6432e-01,  1.0609e-01,  1.4879e-02,  3.0311e-02,  9.0938e-02,\n",
      "          3.7614e-03, -1.2140e-01,  1.0544e-01, -2.4350e-01, -1.9051e-01,\n",
      "         -3.4545e-01,  3.4361e-02, -4.5059e-02,  1.5051e-01, -1.6090e-02,\n",
      "          2.5728e-01,  1.6041e-01,  4.5565e-01, -2.0971e-01,  9.9302e-03,\n",
      "         -2.8808e-01,  5.6778e-02, -1.2285e-01,  2.7165e-02, -9.4674e-03,\n",
      "         -7.2757e-02, -1.8246e-01,  3.0867e-02,  1.4626e-01, -9.5740e-02,\n",
      "         -2.5112e-01,  2.4445e-01,  6.9906e-02, -1.3161e-01, -1.9782e-01,\n",
      "          8.7201e-02,  1.2493e-01, -2.9306e-01, -6.9376e-02,  1.5855e-01,\n",
      "         -1.7772e-01, -2.6525e-01, -2.0959e-01, -1.8717e-01, -1.5852e-01,\n",
      "         -2.2170e-01,  3.0091e-01, -1.0339e-01, -4.9196e-02,  3.7288e-01,\n",
      "         -2.3116e-02, -5.2116e-02,  2.5430e-01,  1.2177e-01, -1.7510e-01,\n",
      "         -1.8586e-01, -6.6128e-02, -1.0776e-01, -2.0643e-01,  2.4973e-01,\n",
      "         -2.6710e-02, -3.5558e-02, -2.0015e-01, -5.7506e+00, -3.8516e-02,\n",
      "         -8.9500e-02, -8.1460e-02,  2.1505e-01, -7.0957e-02,  1.8065e-01,\n",
      "          5.4943e-02, -2.6860e-01,  1.6288e-02,  1.3021e-01,  2.1525e-01,\n",
      "          2.1586e-01,  2.2064e-01,  1.2542e-01,  9.3748e-02,  1.1784e-01,\n",
      "         -2.3617e-01, -1.5599e-01,  3.7642e-01,  6.8481e-03, -5.3430e-01,\n",
      "          1.2164e-01, -7.3617e-03,  3.9117e-02,  1.5997e-01, -1.7777e-01,\n",
      "          3.8557e-01, -5.3832e-01, -2.3442e-01,  7.3321e-02,  2.4153e-01,\n",
      "         -2.0149e-01,  2.1316e-01, -3.1766e-02, -2.6337e-01, -2.6275e-02,\n",
      "         -6.5759e-02,  3.9508e-01, -1.9079e-01,  3.2276e-01, -6.3351e-02,\n",
      "         -6.7032e-02,  1.6472e-01,  3.0370e-01, -1.3554e-01,  1.8263e-01,\n",
      "          1.1891e-02,  3.5914e-02,  8.2035e-02,  2.9390e-01,  2.9025e-02,\n",
      "         -4.5987e-02, -2.4476e-01, -8.9807e-02, -3.6587e-02,  3.8540e-01,\n",
      "         -1.8096e-02, -3.3491e-01, -8.4102e-02,  2.3134e-01, -4.0776e-01,\n",
      "         -1.1798e-01,  9.7338e-02, -4.5535e-01,  5.3950e-02, -8.2592e-02,\n",
      "         -5.2159e-01, -1.1749e-01,  7.4408e-02,  7.9057e-02,  2.4518e-01,\n",
      "          5.6789e-02, -5.5184e-01, -1.1365e-01, -2.9762e-01, -6.6562e-02,\n",
      "         -1.1198e-01,  2.1073e-01, -3.3080e-01,  7.6490e-02,  1.3317e-01,\n",
      "          3.3527e-01,  1.6115e-01,  6.6353e-03,  1.1750e-01,  1.7473e-01,\n",
      "          8.1080e-02, -3.2899e-01, -1.5830e-02,  2.7719e-01,  3.1675e-02,\n",
      "          4.2697e-01,  2.2306e-01,  3.2024e-01,  6.5633e-02,  3.9506e-01,\n",
      "         -2.7886e-02, -7.0278e-02, -1.9735e-01, -2.0837e-01, -1.9291e-01,\n",
      "          1.9798e-01, -1.8494e-01,  8.5623e-02, -3.5588e-02, -3.3786e-01,\n",
      "         -2.4915e-01,  2.1973e-01, -2.1829e-02,  1.3837e-01, -7.8226e-02,\n",
      "          1.5546e-01,  1.9412e-01, -2.0466e-01,  7.9792e-04, -2.3452e-02,\n",
      "          4.4544e-01,  8.4362e-02,  1.6634e-01, -7.1774e-02,  1.6624e-01,\n",
      "          4.4951e-02,  1.6072e-01, -2.4744e-01, -1.7139e-01, -9.6178e-02,\n",
      "         -1.0576e-01, -4.0510e-02, -1.1438e-02,  7.3631e-02,  2.0869e-01,\n",
      "         -7.2341e-02, -2.4362e-02,  1.8844e-01,  5.4317e-03, -1.7960e-02,\n",
      "         -7.7921e-02,  4.9260e-02,  1.5658e-01,  2.0249e-01, -2.3923e-02,\n",
      "          5.8808e-02, -1.9260e-01,  2.3341e-01, -4.2771e-02, -2.8134e-01,\n",
      "          2.7472e-01,  2.2790e-01,  3.6436e-01, -4.6932e-02, -2.8630e-01,\n",
      "          2.0515e-01,  1.0869e-01, -2.6535e-01, -1.8153e-01,  1.5616e-02,\n",
      "         -1.5750e-01,  5.0513e-02, -3.8255e-01,  1.6314e-01, -1.3171e-01,\n",
      "          1.2217e-01,  5.9519e-02,  1.2116e-01,  4.6474e-01, -1.1410e-01,\n",
      "         -6.5669e-02, -2.9127e-01,  2.5277e-01,  1.4341e-01, -1.7378e-01,\n",
      "         -1.7831e-01, -2.4142e-01, -1.2931e-01,  1.0787e-01,  3.3861e-01,\n",
      "         -5.4172e-02, -2.4100e-01,  3.2069e-01,  1.8693e-01, -4.6147e-01,\n",
      "         -7.3609e-02,  7.8712e-02,  1.4283e-01,  1.6707e-01, -2.6506e-01,\n",
      "         -1.2675e-01,  6.1828e-02,  2.0698e-01,  3.8345e-02, -2.3413e-01,\n",
      "          6.3623e-02,  2.2912e-02,  3.3866e-02, -7.1355e-02,  2.6131e-01,\n",
      "          2.9827e-01,  1.0127e-01, -3.9820e-02, -7.2131e-02, -2.0757e-01,\n",
      "         -1.5789e-01, -7.4970e-02, -9.4373e-03,  1.1626e-01,  3.4052e-02,\n",
      "         -4.0866e-02, -2.9376e-01, -6.6632e-02,  3.0664e-02,  4.2150e-02,\n",
      "         -1.5039e-01, -8.8990e-02, -1.4466e-01,  5.5032e-02,  2.0107e-02,\n",
      "         -3.1759e-02, -2.2268e-01,  2.9725e-02, -1.7175e-01, -1.8712e-01,\n",
      "         -7.2391e-02,  3.5211e-03, -6.9047e-02,  1.2137e-01, -1.4097e-01,\n",
      "         -9.8320e-02,  2.6054e-02, -5.1546e-02, -1.4372e-01,  6.0834e-02,\n",
      "         -1.3182e-02,  9.1171e-02,  2.6460e-01,  6.6231e-02, -3.1077e-01,\n",
      "         -2.4877e-02,  2.6381e-01, -1.8007e-01, -2.9423e-01, -1.8511e-01,\n",
      "         -2.4194e-01, -2.0962e-01,  3.2882e-02, -3.7521e-02,  1.6144e-01,\n",
      "          4.2653e-02,  2.6475e-01,  4.0420e-01, -1.2191e-02, -1.5990e-02,\n",
      "         -3.9621e-02, -1.0999e-01,  2.6799e-01, -3.1541e-01, -1.8216e-01,\n",
      "         -3.3531e-01, -1.3741e-01, -1.7835e-02, -1.1752e-01,  2.0404e-01,\n",
      "         -2.8075e-01,  1.5233e-01, -1.2239e-01,  1.6669e-01, -2.0640e-01,\n",
      "         -3.1298e-02,  3.4384e-02, -9.1763e-02, -3.9792e-02, -1.5544e-01,\n",
      "          2.1599e-01,  2.0897e-01,  9.4833e-02,  4.6068e-02,  2.1688e-01,\n",
      "         -3.7382e-01,  2.0201e-02,  7.5037e-02, -1.7873e-01,  2.1120e-01,\n",
      "          1.6841e-01,  9.7511e-02,  2.8809e-01, -3.7207e-01, -3.2139e-01,\n",
      "          7.4056e-03, -4.6657e-01, -1.5125e-01,  2.3816e-01, -7.0576e-02,\n",
      "         -1.0315e-01,  2.1350e-01, -5.0633e-01,  1.3200e-01, -4.9298e-02,\n",
      "          1.1575e-01, -1.0954e-01,  2.3384e-01, -4.2596e-02,  3.4501e-01,\n",
      "          2.3723e-01, -1.9524e-01, -6.8091e-02,  3.4581e-01,  5.8999e-02,\n",
      "         -4.4888e-01,  5.8934e-03, -1.0987e-01, -1.4396e-01, -3.5726e-01,\n",
      "          2.1950e-01,  3.3785e-01, -3.9275e-01, -2.6265e-01,  3.0888e-02,\n",
      "         -3.1900e-01,  1.4463e-01,  5.2066e-03, -1.4126e-01, -1.2948e-01,\n",
      "          4.6984e-01,  1.9063e-01,  2.7925e-01,  3.2928e-01, -2.1720e-01,\n",
      "          1.2631e-01,  1.6540e-01,  1.2841e-01, -5.1628e-02,  5.0139e-02,\n",
      "          1.6902e-01,  1.0476e-01,  9.0001e-02,  1.4311e-01,  8.7796e-02,\n",
      "         -1.6823e-01, -7.5480e-02, -6.0827e-03, -6.6011e-02,  3.8958e-01,\n",
      "          1.1605e-01,  3.0877e-01,  8.2165e-02,  7.5274e-02, -8.1386e-02,\n",
      "          7.4352e-03,  1.2722e-01,  2.0117e-01, -1.0612e-01, -1.4091e-01,\n",
      "          3.7252e-02, -5.6001e-02,  1.7021e-01, -9.2777e-02,  1.4698e-01,\n",
      "          3.7160e-01, -1.0042e-01,  1.5711e-01, -7.9626e-02,  1.5733e-02,\n",
      "          2.4098e-01, -1.1140e-01, -1.1942e-01, -6.4308e-02,  7.2720e-02,\n",
      "         -4.3199e-01, -1.6025e-02,  1.5409e-01, -1.8089e-01,  5.6166e-02,\n",
      "         -2.2103e-01,  1.9592e-01,  1.2085e-01,  2.5310e-02,  1.3107e-01,\n",
      "         -2.3953e-02, -1.0821e-01,  2.8540e-01,  1.0824e-02, -3.0022e-01,\n",
      "         -1.3674e-01, -2.2394e-01,  1.1420e-01, -4.6585e-03, -1.3831e-03,\n",
      "          9.2651e-02,  2.2623e-01, -1.5031e-01,  3.1205e-02, -1.0112e-01,\n",
      "          1.9666e-01,  9.4808e-02,  1.2263e-01,  1.1509e-01, -2.4142e-03,\n",
      "         -1.2213e-01,  3.7747e-01,  4.9406e-02,  1.1245e-01, -1.5645e-02,\n",
      "          2.8359e-02,  6.6055e-02, -1.2657e-01, -1.0545e-01,  1.2956e-01,\n",
      "          1.8567e-01, -3.7511e-01, -3.2465e-01,  3.4692e-02,  1.6708e-01,\n",
      "         -2.1584e-01,  2.0944e-01, -2.0469e-02,  2.3667e-01,  1.5615e-01,\n",
      "          1.7709e-01,  3.4712e-02, -1.7430e-01,  4.5584e-02,  4.0869e-02,\n",
      "          2.2909e-01,  2.5480e-01, -2.1047e-01,  9.7692e-02, -3.1897e-01,\n",
      "          1.7435e-01,  1.0185e-01,  1.3869e-01, -3.3320e-01,  6.5792e-02,\n",
      "         -1.9894e-01,  1.9723e-01, -1.3841e-02, -1.2059e-01, -2.0189e-01,\n",
      "          1.8599e-01,  2.0412e-01, -3.1503e-01, -2.5411e-01,  2.3581e-02,\n",
      "          4.4762e-02, -6.3831e-02, -1.0889e-01, -8.8458e-02, -1.0113e-01,\n",
      "         -2.1184e-01, -8.7337e-02, -9.4890e-02,  3.4057e-01, -2.9928e-01,\n",
      "          1.1847e-01,  4.5379e-03,  1.0419e-01, -2.6970e-01, -1.0338e-01,\n",
      "         -1.8410e-01, -5.8224e-02, -1.2575e-03]])\n",
      "tensor([[ 5.4310e-01,  4.2889e-01, -7.9970e-03, -7.1561e-02, -3.4609e-01,\n",
      "         -1.4609e-01, -1.6388e-01,  4.6614e-01, -2.9224e-01, -4.3918e-02,\n",
      "          1.9123e-01, -3.8969e-01, -2.1653e-01,  1.7407e-01, -2.5713e-01,\n",
      "          5.7473e-02,  1.6937e-01,  1.2307e-01, -2.1090e-01,  7.0530e-02,\n",
      "          3.3643e-01,  1.1704e-01, -1.4514e-01,  4.6743e-01,  1.0484e-01,\n",
      "          1.9192e-01, -1.8267e-01,  9.8136e-02, -3.8486e-01, -3.6015e-01,\n",
      "          1.4325e-01,  4.6602e-02,  3.7988e-02,  1.0224e-01, -2.4832e-01,\n",
      "         -2.7227e-01,  1.1298e-01, -1.0690e-01, -4.0346e-01, -1.6177e-01,\n",
      "         -3.7756e-01,  1.0559e-01, -2.8614e-01, -6.9690e-02, -3.6741e-01,\n",
      "         -2.8342e-01,  3.8071e-01, -2.8579e-02,  1.6537e-02,  1.4007e-01,\n",
      "          2.6070e-01,  9.3944e-02, -4.7394e-01, -4.2336e-02,  4.0123e-01,\n",
      "          5.1534e-01,  2.5341e-01, -1.0003e+00, -3.7262e-01, -1.9969e-01,\n",
      "          2.0824e-01, -2.8631e-02,  1.5393e-01,  3.2748e-01,  1.4584e-01,\n",
      "         -2.8435e-02, -3.3099e-03,  5.7582e-01, -7.7004e-01, -3.3215e-01,\n",
      "         -4.6014e-01, -2.2224e-01,  8.7430e-02, -1.5562e-01, -3.9172e-01,\n",
      "         -1.1136e-01,  1.9099e-01,  3.5680e-01,  9.8441e-02,  3.5077e-01,\n",
      "         -1.6614e-01,  7.9316e-01, -3.8194e-01,  2.0470e-01, -3.1732e-01,\n",
      "          5.2347e-02, -4.8118e-01, -4.6237e-02, -1.7832e-01,  2.2305e-01,\n",
      "         -8.8606e-02, -4.2735e-02,  1.6126e-01,  2.1006e-01,  3.4105e-01,\n",
      "         -4.3091e-01, -3.6546e-01, -1.2528e-01,  2.2056e-02,  4.4174e-02,\n",
      "         -1.1465e-02, -3.3876e-01, -1.9303e-01,  2.9659e-01,  3.4045e-02,\n",
      "         -5.2748e-02,  5.4789e-02,  2.2141e-01,  3.5649e-01,  3.6683e-02,\n",
      "          9.3999e-02, -3.9693e-01,  1.8860e-01, -3.1790e-01, -1.5016e-01,\n",
      "          6.9609e-02, -2.3566e-01, -3.5062e-01,  1.6635e-01, -2.6157e-01,\n",
      "         -1.3898e-02, -1.4039e-01,  2.9909e-01,  6.4697e-01,  1.2468e-01,\n",
      "         -1.2703e-01, -5.9117e-01,  2.6089e-02, -8.0642e-02, -5.6965e-01,\n",
      "          3.1846e-01,  3.0413e-01,  6.3194e-01,  7.6798e-02, -9.8637e-02,\n",
      "          3.2776e-01,  3.1961e-01,  1.9228e-01, -4.4502e-02,  4.7562e-02,\n",
      "          7.1326e-02,  3.0707e-01, -6.3622e-02, -3.1211e-01,  2.2326e-02,\n",
      "          1.4384e-01,  2.5389e-01, -1.7321e-01,  2.7356e-02, -2.6596e-01,\n",
      "          2.3559e-01,  4.9837e-01,  7.6335e-02, -3.3820e-01, -7.3822e-02,\n",
      "         -2.3036e-01,  1.1138e-01,  1.9341e-01,  5.9776e-01,  1.6378e-01,\n",
      "         -3.2975e-04, -2.7152e-01,  1.8695e-01,  2.2992e-01, -1.3078e-02,\n",
      "         -1.1175e-01,  1.3231e-01, -1.6207e-01,  1.6212e-01,  1.2465e-02,\n",
      "         -2.6559e-01, -4.3835e-01,  9.5969e-01,  2.0024e-01,  3.6703e-01,\n",
      "          2.3576e-01, -1.2270e-01,  1.2500e-01,  2.9567e-01,  3.7598e-01,\n",
      "         -1.7344e+00,  3.3723e-01,  2.5476e-01, -2.3432e-01, -1.3353e-01,\n",
      "          7.3025e-02,  6.3069e-02, -2.0483e-01,  3.4589e-02,  4.2497e-01,\n",
      "         -2.9764e-01,  2.0032e-01,  6.9929e-02, -7.5924e-02,  4.1878e-01,\n",
      "         -4.4785e-01, -4.1552e-01, -2.7875e-01, -2.0101e-01,  2.1155e-04,\n",
      "         -1.2129e-01, -1.6825e-02,  2.4820e-01, -6.6597e-02, -3.3684e-01,\n",
      "          4.8532e-01,  3.4463e-01, -1.9627e-01, -4.0853e-01,  4.3861e-01,\n",
      "         -2.7680e-01, -9.8093e-03, -9.8764e-02, -3.8224e-01, -1.0793e-01,\n",
      "          4.9551e-02,  5.4755e-02, -5.7842e-01, -1.5824e-01,  5.8517e-02,\n",
      "          2.4993e-01, -7.5013e-02, -5.2315e-01,  6.2890e-01, -2.3591e-01,\n",
      "          3.0011e-01,  1.5190e-01,  2.0189e-01,  1.2329e-01,  4.1929e-01,\n",
      "         -1.6802e-01, -9.2488e-03,  1.4430e-01,  3.5969e-01,  5.0453e-02,\n",
      "         -2.2465e-01, -3.4462e-01,  3.3030e-02, -3.8681e-02,  1.2714e-01,\n",
      "         -3.1173e-01,  4.4223e-01,  1.8459e-01,  4.3775e-01, -4.5320e-02,\n",
      "          3.6774e-01,  1.0193e-01,  1.9549e-01,  4.7557e-02,  3.3570e-01,\n",
      "         -1.2305e-01, -6.1956e-01, -3.4888e-01, -3.4446e-01,  4.7086e-01,\n",
      "          9.6775e-03, -1.3266e-01,  5.8596e-01,  1.1158e-01,  1.8857e-02,\n",
      "          1.6351e-01,  7.1466e-02, -3.1746e-02,  3.1477e-01, -1.0991e-01,\n",
      "         -1.0195e-01,  1.2644e-01,  3.3937e-01,  1.6940e-02,  1.8191e-01,\n",
      "          2.5562e-01, -2.2984e-01, -1.3623e-01,  1.8822e-01, -3.6794e-01,\n",
      "         -1.1946e-01,  4.1584e-01, -5.7238e-02, -4.4701e-02, -3.7478e-01,\n",
      "          8.5569e-02,  3.0550e-01, -1.8569e-01,  1.6066e-01,  2.3132e-01,\n",
      "         -1.3937e-01,  1.5659e-01,  1.6476e-01, -1.7867e-01, -4.1592e-01,\n",
      "         -2.4614e-01,  7.2830e-02, -7.9307e-02, -1.1368e-01,  8.0493e-01,\n",
      "         -6.1205e-02,  4.4946e-01, -1.5621e-01, -6.4795e-02, -2.2251e-01,\n",
      "          2.9918e-01, -4.1551e-01, -3.7875e-02,  9.3013e-02, -4.7285e-01,\n",
      "          1.2809e-01, -5.2878e-01, -2.8812e-01, -3.1098e+00,  1.4752e-01,\n",
      "         -1.8401e-01, -2.5117e-01,  2.9338e-01,  6.8510e-02, -1.8845e-01,\n",
      "         -2.1238e-01, -4.6315e-01, -5.1506e-01, -7.4551e-02, -1.1933e-01,\n",
      "          3.4902e-01,  3.3966e-01,  2.4231e-01, -1.5238e-01, -2.0694e-02,\n",
      "         -2.9584e-01, -5.4600e-01,  2.4613e-01, -3.0125e-01, -3.4460e-02,\n",
      "          2.8839e-01, -2.2743e-01,  3.7752e-01,  8.4060e-01, -1.5172e-01,\n",
      "          1.1176e-02, -6.2306e-02,  4.2228e-02, -2.8653e-01, -3.2686e-01,\n",
      "         -8.7809e-02,  8.5165e-02,  1.3008e-01, -1.7746e-01,  6.3610e-02,\n",
      "         -3.4189e-01,  1.4444e-01, -1.4596e-01, -9.4810e-04, -2.1648e-01,\n",
      "          4.7740e-02, -3.2212e-01,  1.3169e-01, -1.8027e-01, -1.1729e-01,\n",
      "         -7.1279e-01, -2.4685e-01,  4.1544e-01, -1.6598e-01, -6.1459e-02,\n",
      "         -2.8416e-01, -1.1072e-01, -3.0882e-01, -4.5792e-01,  2.4558e-01,\n",
      "          2.1910e-01, -3.5274e-01, -1.2884e-01,  7.3698e-02, -4.0611e-02,\n",
      "         -3.5647e-03,  2.6229e-01,  1.4343e-01,  2.4855e-01, -4.2299e-01,\n",
      "         -2.8319e-01,  6.5685e-01, -3.5219e-01,  1.6950e-01,  3.4522e-01,\n",
      "         -6.0162e-01, -7.6604e-01, -3.3766e-01, -1.7668e-01, -2.5519e-01,\n",
      "         -1.9242e-02,  2.9075e-01, -1.2031e-02, -2.9553e-01, -4.3970e-01,\n",
      "         -2.1847e-01, -3.3654e-01, -3.0088e-01, -4.0407e-01, -4.9308e-02,\n",
      "         -3.0163e-01,  2.8104e-01, -3.7033e-01,  2.7961e-01,  3.6410e-01,\n",
      "          5.6572e-01,  2.3578e-01,  1.6262e-01,  2.0361e-01, -1.9051e-01,\n",
      "         -3.4192e-01, -1.6109e-01,  2.8720e-01,  1.5541e-01,  2.2424e-01,\n",
      "          2.8050e-01,  1.1374e-01,  4.9930e-01, -2.5222e-01, -5.9623e-01,\n",
      "          2.5436e-02, -1.7195e-01,  1.9357e-01,  3.3157e-01, -1.5483e-01,\n",
      "          3.8432e-01, -8.0926e-02, -1.3943e-02, -1.5967e-01, -3.2472e-03,\n",
      "          5.1113e-01, -3.3376e-01, -7.9393e-02,  3.4572e-01,  5.6213e-01,\n",
      "         -3.2960e-01, -3.8815e-01, -4.9795e-01, -1.1031e-01, -1.8156e-01,\n",
      "         -4.8272e-01, -1.6660e-01,  9.6646e-02, -2.1945e-01, -2.7712e-02,\n",
      "         -7.1030e-01,  1.2168e-01, -7.0959e-03, -1.2800e-01, -4.3040e-01,\n",
      "          3.4722e-01, -2.0709e-01,  5.2204e-01,  2.1059e-02,  1.4547e-02,\n",
      "         -1.8491e-03,  1.5136e-01, -2.9985e-02,  1.0453e-01, -1.2229e-01,\n",
      "          9.8492e-02,  5.6953e-02,  3.2556e-01, -2.7250e-01, -3.6636e-01,\n",
      "         -1.3168e-01, -3.1696e-01, -1.2214e-01,  4.2055e-01,  3.1109e-01,\n",
      "         -1.8936e-01,  3.4513e-02,  1.8237e-01, -2.5690e-01, -4.9100e-01,\n",
      "          2.2789e-01,  3.1449e-01,  4.9845e-01,  1.6710e-01, -8.8311e-02,\n",
      "         -2.4486e-01, -9.7387e-02, -3.6075e-02,  2.5385e-02, -3.4237e-02,\n",
      "          2.3571e-01, -6.4897e-01, -5.8821e-02,  4.6691e-01,  2.6334e-01,\n",
      "         -1.0631e-02,  2.5932e-01,  2.4721e-01, -4.7527e-02, -5.8735e-01,\n",
      "         -1.6249e-01, -9.7099e-02,  5.5772e-01, -1.7083e-01,  8.2172e-02,\n",
      "         -2.9257e-01, -2.2635e-01, -1.8426e-02, -4.7600e-01,  1.9858e-01,\n",
      "         -1.2255e-02, -1.1850e-02, -3.6382e-01, -4.0028e-01,  5.6957e-01,\n",
      "          5.6909e-02,  3.8771e-01,  3.9580e-01,  2.6461e-01, -1.8803e-01,\n",
      "         -2.7398e-01,  9.2914e-03,  1.2928e-02, -5.7506e-02, -4.0268e-01,\n",
      "         -3.2094e-01, -1.9461e-01,  1.0615e-01,  2.5435e-02,  6.5510e-02,\n",
      "         -2.6890e-01,  5.4664e-02,  1.6555e-01, -3.0275e-01,  3.3668e-01,\n",
      "         -2.4441e-01, -4.1912e-01, -1.0422e-01,  2.1316e-01, -1.5028e-01,\n",
      "         -3.7142e-02,  3.1214e-01, -3.7191e-01, -8.8791e-02,  2.9545e-02,\n",
      "          8.7458e-02, -3.1215e-01, -1.9502e-01, -5.7345e-02, -5.6811e-01,\n",
      "          2.6910e-02,  1.5015e-01,  3.4454e-01,  7.0702e-02, -1.3135e-01,\n",
      "         -1.5799e-01,  4.2296e-02,  2.2849e-01,  4.4586e-02,  1.8561e-01,\n",
      "         -3.5246e-02, -2.8307e-02,  6.3520e-01, -1.7013e-01,  1.3388e-01,\n",
      "          1.1656e-01, -1.3299e-01,  3.3781e-02,  3.2472e-01, -4.6458e-02,\n",
      "          1.5445e-02, -1.2494e-01, -4.0535e-01, -7.3318e-01, -4.8907e-01,\n",
      "         -3.0756e-01,  2.0591e-01,  5.3685e-01, -1.6214e-01, -9.9238e-02,\n",
      "          6.1783e-01, -6.8456e-03, -1.5140e-02,  2.2276e-01, -2.0430e-01,\n",
      "          1.4602e-01,  1.8621e-01, -2.0711e-02, -2.9061e-01, -2.6737e-02,\n",
      "          5.9720e-02, -8.5168e-02,  8.4718e-02,  3.6018e-01,  1.2022e-01,\n",
      "         -5.1211e-01,  1.3106e-01, -2.2709e-01,  8.2485e-02,  2.1025e-01,\n",
      "         -2.1505e-01, -3.5406e-01,  1.5109e-01,  5.1492e-02,  1.1570e-01,\n",
      "          4.8452e-01, -2.0576e-01,  1.4314e-01,  1.7179e-01,  3.3240e-01,\n",
      "         -6.5417e-02, -1.8692e-01, -2.2133e-02,  2.6531e-02,  3.0847e-02,\n",
      "          1.3263e-01,  9.1992e-02,  3.0206e-01, -4.6050e-02,  1.9233e-01,\n",
      "          3.0588e-01,  3.3717e-01,  1.1579e-01,  5.4915e-02,  2.2199e-01,\n",
      "         -3.3781e-01,  1.7708e-01,  2.3830e-01, -2.7000e-01, -9.1616e-02,\n",
      "          5.9389e-01,  1.9364e-01, -5.8865e-01, -1.2360e-01,  1.9018e-01,\n",
      "         -3.1762e-01, -1.5381e-01,  1.3642e-01,  1.9426e-01, -9.6689e-02,\n",
      "          2.3514e-01, -9.3035e-02,  5.5487e-01,  1.1872e-01, -2.6865e-01,\n",
      "          1.4172e-01, -9.4270e-02,  1.8383e-01,  1.0268e-01,  3.0728e-01,\n",
      "         -3.4120e-01,  2.4705e-01, -1.1441e-02,  2.1914e-01,  3.2962e-01,\n",
      "         -8.4201e-02,  4.4686e-01,  1.7329e-01,  7.2741e-02,  1.8374e-01,\n",
      "          2.3510e-01,  6.1907e-01,  3.9078e-01,  1.0786e-01,  1.5462e-01,\n",
      "         -1.1145e-01,  2.8677e-01,  4.4064e-01, -2.1100e-01, -6.9931e-02,\n",
      "          4.8421e-02, -2.1941e-01,  1.1968e-01,  2.7063e-04,  1.0088e-01,\n",
      "          7.0073e-01, -6.2560e-02,  1.6007e-02, -6.5475e-02, -1.8861e-01,\n",
      "         -3.7553e-01, -4.3919e-01,  1.6053e-01,  2.7526e-01,  3.4781e-01,\n",
      "         -9.8630e-02, -2.2263e-01,  1.7498e-01, -5.7630e-02,  6.4393e-01,\n",
      "         -5.0936e-02,  2.5514e-01, -3.4097e-01, -1.0368e-02,  3.1631e-02,\n",
      "         -2.0385e-01, -6.7290e-02,  7.2369e-02,  3.6733e-01, -1.6985e-02,\n",
      "         -5.3749e-01, -5.0304e-02, -4.5032e-02, -7.7387e-01, -1.3456e-01,\n",
      "          2.9633e-02,  2.9280e-01, -4.7840e-01, -2.0826e-01, -3.2982e-01,\n",
      "         -2.3758e-01, -1.4597e-01,  2.6237e-01, -2.9285e-01,  4.1047e-01,\n",
      "          5.0382e-02,  7.3138e-01, -1.6738e-01,  2.6758e-01, -6.6686e-03,\n",
      "         -2.7488e-01,  6.0605e-01, -2.8343e-01,  1.6081e-01,  1.2525e-01,\n",
      "         -2.5702e-01, -2.3860e-01, -4.8894e-01, -1.5411e-01,  2.1817e-01,\n",
      "         -1.2295e+00, -4.9806e-02, -1.5064e-01,  3.6190e-02,  8.1694e-02,\n",
      "          4.8238e-02,  4.9800e-03,  3.3502e-01, -1.7446e-01,  1.4936e-01,\n",
      "         -1.6278e-01,  7.0909e-01, -2.8296e-01,  5.3929e-02, -8.3085e-02,\n",
      "          2.3521e-01, -7.8294e-02, -2.5785e-01,  1.2735e-01,  2.7473e-01,\n",
      "          5.0380e-02,  6.7221e-02,  1.1332e-01, -1.3469e-01, -1.2202e-01,\n",
      "         -1.5166e-01, -5.2235e-01, -3.7218e-01,  1.9249e-01,  2.3382e-01,\n",
      "          2.1439e-01, -5.7153e-01, -3.4968e-01,  8.6040e-02,  4.4227e-02,\n",
      "         -3.9285e-01, -2.3101e-01,  1.7365e-02, -2.2698e-01, -2.1373e-01,\n",
      "         -4.7227e-03, -3.9531e-01,  4.0144e-02, -3.6824e-01, -1.1921e-01,\n",
      "         -4.5315e-01,  2.4105e-02,  3.4838e-01]])\n",
      "[[0.46978545]]\n"
     ]
    }
   ],
   "source": [
    "resume_mean_pooled_word_embeddings = [torch.mean(sentence, dim=0) for sentence in resume_embeddings]\n",
    "resume_mean_pooled_sentence_embeddings = torch.mean(torch.stack(resume_mean_pooled_word_embeddings), dim=0)\n",
    "\n",
    "job_desc_mean_pooled_word_embeddings = [torch.mean(sentence, dim=0) for sentence in job_desc_embeddings]\n",
    "job_desc_mean_pooled_sentence_embeddings = torch.mean(torch.stack(job_desc_mean_pooled_word_embeddings), dim=0)\n",
    "\n",
    "print(resume_mean_pooled_sentence_embeddings.reshape(1, -1))\n",
    "print(job_desc_mean_pooled_sentence_embeddings.reshape(1, -1))\n",
    "\n",
    "print(cosine_similarity(resume_mean_pooled_sentence_embeddings.reshape(1, -1), job_desc_mean_pooled_sentence_embeddings.reshape(1, -1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
