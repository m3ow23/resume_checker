{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from transformer_encoder import MLMTransformerEncoder\n",
    "from mlm_dataset.mlm_dataset_generator import MLMDatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[MASK]', 'administrator', 'marketing', 'associate'] ['hr']\n"
     ]
    }
   ],
   "source": [
    "# MLM dataset for training\n",
    "mlm_dataset_generator = MLMDatasetGenerator('../../dataset/resume_dataset.csv')\n",
    "inputs, labels = mlm_dataset_generator.generateMLMDataset(1)[0]\n",
    "inputs = inputs[0]\n",
    "labels = labels[0]\n",
    "print(inputs, labels)\n",
    "\n",
    "# Initialize a Tokenizer and fit on text data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='[OOV]')\n",
    "tokenizer.fit_on_texts(mlm_dataset_generator.getVocubulary())\n",
    "\n",
    "# check how many words are in the dataset (currently: 37032)\n",
    "# print(list(tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example with original Transformer hyperparameters\n",
    "num_layers = 1\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "input_vocab_size = 40000\n",
    "maximum_position_encoding = 10000\n",
    "rate = 0.1\n",
    "\n",
    "model = MLMTransformerEncoder(num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate)\n",
    "# dummy_input = [tf.keras.Input(shape=(None, None, 512)), tf.keras.Input(shape=(None, None, 512))]\n",
    "# model(dummy_input)\n",
    "\n",
    "model_trainable_variables = []\n",
    "gradients_test = []\n",
    "\n",
    "# model_trainable_variables.append(model.trainable_variables)\n",
    "\n",
    "# Define an optimizer (e.g., Adam)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define a loss function (e.g., categorical cross-entropy for classification)\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs, labels):\n",
    "    # create one-hot encoded mask and get the indices\n",
    "    mask =[[]]\n",
    "    token_indices = []\n",
    "    for index, token in enumerate(inputs):\n",
    "        if token == '[MASK]':\n",
    "            mask[0].append(0)\n",
    "            token_indices.append(index)\n",
    "        else: \n",
    "            mask[0].append(1)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    # tokenize inputs\n",
    "    input_ids = tf.cast(tokenizer.texts_to_sequences([inputs]), tf.float32)\n",
    "    # tokenize labels\n",
    "    token_ids = tokenizer.texts_to_sequences(labels)\n",
    "    # create array of zeroes with dimension [sequence_length, input_vocab_size]\n",
    "    tokenized_labels = np.zeros((len(inputs), input_vocab_size))\n",
    "    # change the [masked_token_index, token_id] to ones\n",
    "    for index, token_index in enumerate(token_indices):\n",
    "        tokenized_labels[token_index, token_ids[index]] = 1\n",
    "    tokenized_labels = tf.constant(tokenized_labels, dtype=tf.float32)\n",
    "\n",
    "    # print('\\n> INPUTS')\n",
    "    # print(input_ids)\n",
    "    # print(mask)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        predictions = model([input_ids, mask], training=False)[0]\n",
    "\n",
    "        # predictions = tf.nn.softmax(predictions)\n",
    "\n",
    "        # print('\\n> LABELS')\n",
    "        # print(tokenized_labels)\n",
    "        # print('\\n> PREDICTIONS')\n",
    "        # print(predictions)\n",
    "\n",
    "        loss = loss_function(tokenized_labels, predictions)\n",
    "\n",
    "        # print('\\n> LOSS')\n",
    "        # print(loss)\n",
    "\n",
    "    predicted_token = None\n",
    "    for row in predictions.numpy():\n",
    "        if (row[0] > 0):\n",
    "            predicted_token = np.argmax(row)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # print('GRADIENTS')\n",
    "    # print(gradients)\n",
    "\n",
    "    gradients_test.append(gradients)\n",
    "    model_trainable_variables.append(model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, predicted_token, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'mlm_transformer_encoder/transformer_encoder/embedding/embeddings:0' shape=(40000, 512) dtype=float32, numpy=\n",
      "array([[ 0.04116045,  0.03449507, -0.02031866, ...,  0.0213267 ,\n",
      "        -0.0170299 , -0.03837688],\n",
      "       [ 0.00687126, -0.0370633 ,  0.02279961, ...,  0.02008654,\n",
      "        -0.04162905, -0.04138961],\n",
      "       [ 0.00798477,  0.00461094,  0.04755919, ..., -0.04037492,\n",
      "        -0.02516056, -0.03373513],\n",
      "       ...,\n",
      "       [ 0.02167935,  0.02031581,  0.0395397 , ..., -0.03466809,\n",
      "         0.00383444,  0.01453886],\n",
      "       [-0.03064232, -0.00536763, -0.04551834, ..., -0.01846008,\n",
      "        -0.04464027,  0.03317425],\n",
      "       [ 0.0486683 ,  0.00153913,  0.01354399, ..., -0.01716299,\n",
      "         0.01344727,  0.00891051]], dtype=float32)>, <tf.Variable 'mlm_transformer_encoder/transformer_encoder/encoder/multi_head_attention/query/kernel:0' shape=(512, 8, 512) dtype=float32, numpy=\n",
      "array([[[ 2.9262018e-03,  4.1775559e-03, -4.8418256e-04, ...,\n",
      "         -3.1049266e-03,  4.0266183e-03, -8.3916768e-04],\n",
      "        [-1.3879469e-03, -1.9805601e-03,  1.3838222e-03, ...,\n",
      "          1.9653563e-03, -1.2710375e-03,  4.1855155e-03],\n",
      "        [ 3.3682324e-03,  2.8548527e-03,  3.8344064e-03, ...,\n",
      "         -1.7220197e-03, -9.5115678e-04,  8.5299811e-04],\n",
      "        ...,\n",
      "        [ 1.1549337e-03,  3.1391417e-03, -4.1424613e-03, ...,\n",
      "          2.4008718e-03,  1.4768729e-03,  3.1925670e-03],\n",
      "        [-6.1426060e-03,  2.6030338e-03, -3.7475056e-03, ...,\n",
      "         -3.2275636e-03, -1.6498195e-03, -1.1745236e-03],\n",
      "        [-5.5903639e-03,  1.9802288e-03, -4.9123871e-03, ...,\n",
      "          4.0885922e-03,  3.5033773e-03,  6.9446070e-04]],\n",
      "\n",
      "       [[ 3.4894592e-03,  3.3156702e-03, -3.4847218e-04, ...,\n",
      "          1.5025199e-03, -1.3393457e-03,  3.4987940e-03],\n",
      "        [-3.7735325e-04,  4.9035759e-03,  2.9477149e-03, ...,\n",
      "         -2.8554518e-03, -3.1928716e-03, -4.6093710e-04],\n",
      "        [ 5.3747944e-03,  4.7242790e-04, -3.2914634e-04, ...,\n",
      "          5.6478707e-03, -1.5061641e-03,  3.5079489e-03],\n",
      "        ...,\n",
      "        [ 3.7475540e-03,  2.5868616e-03,  3.0935786e-03, ...,\n",
      "         -4.1491473e-03,  3.6597983e-03, -5.9214327e-03],\n",
      "        [ 1.3530198e-03,  2.8090766e-03, -3.2151118e-04, ...,\n",
      "          2.3064637e-03, -2.4050311e-03,  1.4085581e-03],\n",
      "        [-3.9319284e-03,  2.5293909e-04,  3.5303480e-03, ...,\n",
      "          6.2904316e-03, -6.2541035e-04,  2.2256661e-03]],\n",
      "\n",
      "       [[ 3.1974425e-03, -3.5099874e-03,  9.9333969e-04, ...,\n",
      "          6.9409882e-04, -3.5449106e-03,  2.8310670e-03],\n",
      "        [ 1.3881094e-03, -3.8502391e-03,  2.8871081e-03, ...,\n",
      "         -3.6548609e-03,  1.9798621e-03, -1.5672845e-03],\n",
      "        [ 6.5113383e-04,  4.3783099e-03,  2.9122932e-03, ...,\n",
      "         -4.0916670e-03, -3.9109742e-04,  2.2186802e-04],\n",
      "        ...,\n",
      "        [ 3.8781054e-03,  4.1632927e-03, -4.9162852e-03, ...,\n",
      "         -4.0482308e-04,  5.2010320e-04, -2.3078674e-03],\n",
      "        [ 5.3911963e-03,  3.7821381e-05, -2.2633518e-03, ...,\n",
      "         -1.1652153e-03, -4.6699410e-03, -3.3077239e-03],\n",
      "        [-1.1620511e-04,  4.4156360e-03,  2.0756896e-03, ...,\n",
      "         -4.7955676e-03,  2.9744080e-03, -2.2425693e-03]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 1.8235926e-03,  2.7999210e-03,  2.9884712e-03, ...,\n",
      "          1.3579703e-03,  2.9970873e-03,  5.8369795e-03],\n",
      "        [-5.5183552e-04,  1.2512028e-04,  2.2797205e-03, ...,\n",
      "          8.8326959e-04,  2.6945570e-03, -1.1564879e-03],\n",
      "        [ 5.2388767e-03, -4.3248064e-03,  1.3735553e-03, ...,\n",
      "          1.6435384e-04, -1.6347533e-03,  3.6600995e-04],\n",
      "        ...,\n",
      "        [ 1.8079716e-03, -3.9211260e-03,  2.4784445e-03, ...,\n",
      "         -3.5732335e-03,  1.3733206e-03, -2.5968403e-03],\n",
      "        [-2.3098649e-03,  3.8942576e-03, -1.6573836e-03, ...,\n",
      "          3.8204263e-03,  1.2000336e-03,  2.7056879e-03],\n",
      "        [-4.5649796e-03,  3.9926241e-03, -4.2846436e-03, ...,\n",
      "          4.6108053e-03,  3.3640775e-03, -1.2037583e-03]],\n",
      "\n",
      "       [[ 2.0299631e-03,  1.6377219e-03, -2.2605772e-03, ...,\n",
      "         -1.5988378e-03,  6.6873693e-04,  1.0204628e-03],\n",
      "        [ 4.0141051e-03, -1.4378825e-03, -4.2933668e-03, ...,\n",
      "         -3.3526730e-03,  1.1234581e-03, -4.5934529e-03],\n",
      "        [ 3.5924669e-03, -3.0324897e-03, -2.3316490e-03, ...,\n",
      "          4.1381815e-03, -2.5690740e-03, -3.1518910e-04],\n",
      "        ...,\n",
      "        [-1.2033818e-03,  3.4436143e-03,  4.2723170e-03, ...,\n",
      "          3.5408437e-03, -5.1034116e-03, -9.8977867e-04],\n",
      "        [ 2.6986524e-03,  2.0161815e-04, -2.0555451e-03, ...,\n",
      "          2.3640057e-03, -9.7472023e-04, -2.0925624e-03],\n",
      "        [-3.6574793e-03,  1.8281100e-03, -2.8528592e-03, ...,\n",
      "         -1.2453161e-03,  3.3938722e-05, -8.8415062e-04]],\n",
      "\n",
      "       [[ 4.3541947e-03,  2.2312296e-03,  3.8477466e-03, ...,\n",
      "         -1.4453919e-03,  2.7169271e-03, -2.8761223e-04],\n",
      "        [ 1.2950599e-03, -5.1822065e-04, -1.1014545e-03, ...,\n",
      "          1.4234226e-03, -3.6607201e-03,  3.3112722e-03],\n",
      "        [-3.9702412e-03, -9.9475263e-04, -1.5777472e-04, ...,\n",
      "          1.5760544e-03,  1.2550293e-03, -3.0140129e-03],\n",
      "        ...,\n",
      "        [ 5.9147016e-04,  3.6487645e-03,  2.7907392e-04, ...,\n",
      "          1.9229496e-03, -9.4964204e-04,  3.6721674e-03],\n",
      "        [-3.9621666e-03, -2.4492489e-03, -6.3658331e-04, ...,\n",
      "          2.4111168e-03, -6.8195770e-04, -2.1757113e-03],\n",
      "        [-2.9798571e-03, -3.9448710e-03, -3.8474849e-03, ...,\n",
      "          1.9725789e-03,  2.7695287e-03,  1.3766856e-03]]], dtype=float32)>, <tf.Variable 'mlm_transformer_encoder/transformer_encoder/encoder/multi_head_attention/query/bias:0' shape=(8, 512) dtype=float32, numpy=\n",
      "array([[ 6.0377592e-05, -4.1465664e-05,  1.3921852e-04, ...,\n",
      "        -4.5290100e-04,  2.1598200e-04,  1.1725381e-03],\n",
      "       [ 8.3465705e-04,  6.7987578e-04, -8.2859478e-05, ...,\n",
      "        -8.0247340e-04, -3.8976909e-04, -1.9644349e-04],\n",
      "       [ 1.2804335e-03, -7.6650502e-04, -9.4352895e-04, ...,\n",
      "         1.4004562e-03,  1.1247379e-03, -5.0678989e-04],\n",
      "       ...,\n",
      "       [ 7.5388110e-05, -3.8551138e-04,  5.8494014e-04, ...,\n",
      "        -2.3881995e-04,  1.2817137e-03, -1.1180992e-03],\n",
      "       [-2.0667135e-03,  6.4732990e-04,  2.6456709e-04, ...,\n",
      "        -4.0995583e-04,  1.2291477e-03, -9.3329250e-04],\n",
      "       [-1.2393092e-03,  3.4123354e-04, -5.3321163e-04, ...,\n",
      "         1.2147524e-03,  3.8910490e-05, -1.0457942e-03]], dtype=float32)>, <tf.Variable 'mlm_transformer_encoder/transformer_encoder/encoder/multi_head_attention/key/kernel:0' shape=(512, 8, 512) dtype=float32, numpy=\n",
      "array([[[-2.3172379e-03, -4.7268788e-03, -3.4416080e-05, ...,\n",
      "         -8.8045071e-04, -2.0704884e-03, -6.3312019e-04],\n",
      "        [ 1.8314373e-03, -5.3400057e-03, -3.1641144e-03, ...,\n",
      "          1.9268156e-04,  1.4824623e-03,  1.7988194e-03],\n",
      "        [ 6.5997586e-04,  4.7549265e-03,  5.2862181e-03, ...,\n",
      "         -4.0412564e-03, -2.5734128e-03,  3.3749887e-03],\n",
      "        ...,\n",
      "        [ 4.7333958e-03,  3.7103731e-03,  1.3872141e-03, ...,\n",
      "         -3.8878422e-03,  7.9062878e-04,  2.5984957e-03],\n",
      "        [ 6.0161599e-03, -6.0892077e-03, -6.2960526e-03, ...,\n",
      "          8.6369866e-04, -5.8694109e-03,  1.5285794e-03],\n",
      "        [ 7.3635741e-03,  3.5409124e-03,  1.7215868e-03, ...,\n",
      "          1.4062794e-03, -1.0613781e-04, -1.4793323e-04]],\n",
      "\n",
      "       [[ 4.9818987e-03, -3.1096828e-03, -1.2090966e-03, ...,\n",
      "         -3.2318821e-03,  2.9044780e-03, -6.1444589e-04],\n",
      "        [-1.6869290e-03, -5.4837009e-03,  8.0080359e-04, ...,\n",
      "         -2.8137323e-03, -2.1658267e-03, -4.6341270e-03],\n",
      "        [-3.9698477e-03, -1.9105080e-03,  3.5240702e-04, ...,\n",
      "          4.1312166e-03, -2.4231591e-03,  1.1114809e-03],\n",
      "        ...,\n",
      "        [ 2.3521353e-03, -1.8457379e-03, -2.4402647e-03, ...,\n",
      "          2.7327281e-03, -5.8766361e-03,  2.0354528e-03],\n",
      "        [ 5.5642319e-03,  4.1319637e-04,  7.5769419e-04, ...,\n",
      "         -1.8868643e-03,  2.9549308e-03,  6.2426589e-03],\n",
      "        [ 2.4757781e-03,  4.1769915e-03, -9.9948572e-04, ...,\n",
      "         -4.0092473e-03, -1.9971363e-03, -3.2179194e-04]],\n",
      "\n",
      "       [[-3.4540999e-04,  3.8452088e-03,  4.3294649e-03, ...,\n",
      "         -9.5130329e-04, -4.7599422e-03, -4.4552889e-03],\n",
      "        [-4.5261937e-03, -5.4988469e-04, -1.3671598e-03, ...,\n",
      "         -3.0345097e-03,  4.0081493e-03, -5.5642729e-04],\n",
      "        [ 8.0679014e-04,  8.0440065e-04, -2.2380895e-03, ...,\n",
      "         -5.9762509e-03, -3.2441479e-03,  5.3299200e-03],\n",
      "        ...,\n",
      "        [ 3.2746620e-03,  5.1668804e-04,  1.2777619e-04, ...,\n",
      "         -4.9263943e-04, -1.9710097e-03,  3.4433824e-03],\n",
      "        [ 1.8667884e-04, -2.2687695e-03, -2.5088065e-03, ...,\n",
      "         -2.6538274e-03, -3.0900836e-03,  3.9289813e-03],\n",
      "        [ 2.6253157e-03, -3.2679064e-03,  6.9058762e-04, ...,\n",
      "          2.9403801e-04, -6.8193628e-04, -3.0204046e-03]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 2.9081847e-03, -4.6012313e-03, -3.6415854e-03, ...,\n",
      "          2.3054816e-04, -5.0465134e-03, -1.9882389e-03],\n",
      "        [-3.4395218e-04,  5.1530455e-03,  1.3626730e-03, ...,\n",
      "         -1.5874831e-03, -1.8775250e-03,  1.8192603e-03],\n",
      "        [ 8.9923659e-04, -1.5698568e-03, -3.7338082e-03, ...,\n",
      "          2.6506917e-03,  1.5101167e-03,  7.3556061e-05],\n",
      "        ...,\n",
      "        [-3.6999399e-03,  3.0665724e-03, -1.4704580e-03, ...,\n",
      "          8.9799659e-04,  5.9579214e-04, -1.8194764e-03],\n",
      "        [-2.6830880e-03,  2.3762400e-03, -1.4112100e-03, ...,\n",
      "         -1.1592869e-03,  3.0985153e-03, -4.9982583e-03],\n",
      "        [ 1.3071913e-03, -5.7013438e-04, -5.7796552e-04, ...,\n",
      "          3.3262672e-03,  2.1876106e-03,  8.3606405e-04]],\n",
      "\n",
      "       [[-1.1130568e-03,  7.0215977e-04,  2.2542141e-03, ...,\n",
      "         -4.3598157e-03,  3.4642830e-03,  1.4936576e-04],\n",
      "        [ 1.6962124e-04, -1.3067769e-03, -1.2362440e-03, ...,\n",
      "          3.6464501e-04, -2.3516372e-03, -5.0697103e-04],\n",
      "        [ 2.5939329e-03, -1.2250808e-03, -3.0769242e-04, ...,\n",
      "         -1.8101202e-03,  5.8966910e-04, -8.2455546e-04],\n",
      "        ...,\n",
      "        [ 4.4485247e-03,  3.4791976e-03,  1.7856434e-03, ...,\n",
      "         -3.1566042e-03,  6.1300932e-04, -5.4640844e-03],\n",
      "        [-8.7478128e-04,  2.2753472e-03, -1.6175614e-03, ...,\n",
      "         -1.1686664e-03, -5.8516576e-03, -1.6461846e-03],\n",
      "        [-2.0987829e-03, -3.2556357e-03,  3.1496752e-03, ...,\n",
      "         -2.9676459e-03,  6.8804197e-04,  1.9632129e-03]],\n",
      "\n",
      "       [[-1.1916733e-03, -1.1261811e-03, -2.2375248e-03, ...,\n",
      "         -1.2701694e-03,  2.2870435e-03,  3.1322276e-04],\n",
      "        [ 1.6718719e-03,  1.8328630e-03,  2.1418871e-03, ...,\n",
      "          1.0071493e-03,  4.5743864e-03,  4.8804557e-04],\n",
      "        [ 3.3499566e-03,  2.2841948e-03,  4.8704566e-03, ...,\n",
      "         -4.6443474e-03, -6.3758570e-04, -1.5952441e-04],\n",
      "        ...,\n",
      "        [-1.5037736e-03,  3.3603448e-03,  1.9842594e-04, ...,\n",
      "          3.2992633e-03,  6.8730977e-04,  2.8726018e-03],\n",
      "        [-3.0622794e-03, -1.2148710e-03,  3.6041902e-03, ...,\n",
      "          2.9075181e-03, -1.2867961e-03, -2.7793727e-03],\n",
      "        [-7.2815982e-03, -2.2438194e-03, -6.6851913e-03, ...,\n",
      "          5.1712559e-04,  3.7023122e-04, -2.4804641e-03]]], dtype=float32)>, <tf.Variable 'mlm_transformer_encoder/transformer_encoder/encoder/multi_head_attention/key/bias:0' shape=(8, 512) dtype=float32, numpy=\n",
      "array([[ 1.0803839e-09,  5.2881088e-10, -1.5586855e-09, ...,\n",
      "         4.8659887e-10, -1.1328956e-09, -4.5200821e-09],\n",
      "       [ 5.3730331e-10,  3.6470587e-09, -6.1026291e-09, ...,\n",
      "        -3.2406722e-09,  1.1556899e-09,  1.0603205e-08],\n",
      "       [ 2.1158540e-08, -8.3264542e-09, -2.7529348e-09, ...,\n",
      "         3.0209997e-09,  4.8120392e-09, -1.0779091e-09],\n",
      "       ...,\n",
      "       [-5.6868388e-10, -1.1006147e-09,  2.7911770e-10, ...,\n",
      "        -9.5444086e-10,  2.3045073e-09, -1.0386568e-09],\n",
      "       [ 5.8053623e-10,  4.3574500e-10, -7.6821314e-09, ...,\n",
      "        -4.4704458e-09,  1.8845931e-09,  8.1422149e-09],\n",
      "       [-2.7716953e-09, -3.5380510e-10, -2.2513438e-09, ...,\n",
      "         2.7831144e-09, -2.7980155e-11, -2.0352582e-09]], dtype=float32)>, <tf.Variable 'mlm_transformer_encoder/transformer_encoder/encoder/multi_head_attention/value/kernel:0' shape=(512, 8, 512) dtype=float32, numpy=\n",
      "array([[[-0.00091169,  0.00106341, -0.00806588, ...,  0.00763457,\n",
      "          0.00845433,  0.00046875],\n",
      "        [ 0.00692944, -0.0035368 , -0.00289374, ..., -0.0073333 ,\n",
      "         -0.0053083 ,  0.00840981],\n",
      "        [-0.00758987,  0.00351308, -0.00338924, ...,  0.0001106 ,\n",
      "         -0.00647596,  0.00417286],\n",
      "        ...,\n",
      "        [-0.00811623,  0.00153187, -0.00755254, ...,  0.00517441,\n",
      "         -0.00060071, -0.01149512],\n",
      "        [-0.00520592,  0.00288109,  0.00296018, ...,  0.00957056,\n",
      "          0.0017589 ,  0.00062122],\n",
      "        [-0.00449244,  0.00347746,  0.00327423, ..., -0.00839564,\n",
      "          0.0012417 ,  0.00405012]],\n",
      "\n",
      "       [[-0.00015469,  0.01005084, -0.00129525, ...,  0.00204422,\n",
      "          0.01005267,  0.00373306],\n",
      "        [ 0.00808685, -0.00876732, -0.00214277, ..., -0.00368658,\n",
      "         -0.00092089,  0.00779493],\n",
      "        [-0.00666803,  0.0083793 , -0.00297437, ...,  0.00260156,\n",
      "         -0.00471084,  0.00927773],\n",
      "        ...,\n",
      "        [-0.00667397,  0.00551261, -0.00889957, ...,  0.00693709,\n",
      "          0.00436513, -0.01169571],\n",
      "        [-0.00988584,  0.00624077,  0.0046982 , ...,  0.00910622,\n",
      "          0.00558446,  0.00227836],\n",
      "        [-0.00100359,  0.00434254,  0.00388718, ..., -0.00250284,\n",
      "          0.00092109,  0.00660192]],\n",
      "\n",
      "       [[ 0.00017285,  0.00513835, -0.00590481, ...,  0.00671816,\n",
      "          0.00275054,  0.00357306],\n",
      "        [ 0.00205823, -0.00455558,  0.00266233, ..., -0.00979958,\n",
      "         -0.00508056,  0.00346599],\n",
      "        [-0.00986494,  0.00335114, -0.00760766, ...,  0.00022962,\n",
      "         -0.00043546,  0.00472395],\n",
      "        ...,\n",
      "        [-0.00904428,  0.00569419, -0.00116739, ...,  0.0015389 ,\n",
      "          0.00176635, -0.00714244],\n",
      "        [-0.00401244,  0.00166106,  0.00813543, ...,  0.00174997,\n",
      "          0.00137903,  0.00555903],\n",
      "        [-0.00397146,  0.0040449 ,  0.00334911, ..., -0.00102983,\n",
      "         -0.00215919, -0.00234761]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.00465117,  0.00238875, -0.00339651, ...,  0.00622678,\n",
      "          0.00288294,  0.0065822 ],\n",
      "        [ 0.00701983, -0.006921  , -0.00170038, ..., -0.00728711,\n",
      "         -0.00624717,  0.00139236],\n",
      "        [-0.00754835,  0.00577656, -0.0097841 , ..., -0.00177733,\n",
      "         -0.00260196,  0.00205088],\n",
      "        ...,\n",
      "        [-0.00227867,  0.00694254, -0.00303579, ...,  0.00938213,\n",
      "          0.00520822, -0.003903  ],\n",
      "        [-0.00285888,  0.00698082,  0.00042316, ...,  0.00936589,\n",
      "          0.00827601,  0.00635372],\n",
      "        [-0.00857739,  0.00210791,  0.00441449, ..., -0.00925169,\n",
      "          0.00519442,  0.00318587]],\n",
      "\n",
      "       [[ 0.00046401,  0.00591646, -0.00988766, ...,  0.00242541,\n",
      "          0.0022335 ,  0.00249212],\n",
      "        [ 0.00138637, -0.00868296, -0.00020065, ..., -0.00668735,\n",
      "         -0.00462895,  0.00271651],\n",
      "        [-0.0104799 ,  0.00109701, -0.00192531, ..., -0.0028602 ,\n",
      "         -0.00291963,  0.00542516],\n",
      "        ...,\n",
      "        [-0.00560414,  0.00466486, -0.00294941, ...,  0.00278129,\n",
      "          0.00506669, -0.0026455 ],\n",
      "        [-0.00910196,  0.00436996,  0.00164295, ...,  0.00692796,\n",
      "          0.00073955,  0.00700531],\n",
      "        [-0.00659501,  0.00498645, -0.00097945, ..., -0.00133846,\n",
      "          0.00113131,  0.00690601]],\n",
      "\n",
      "       [[-0.00076396,  0.00917499, -0.00506605, ...,  0.00139557,\n",
      "          0.00430973,  0.00399927],\n",
      "        [ 0.00447398, -0.00500479,  0.00286811, ..., -0.00391124,\n",
      "         -0.00794687,  0.00691566],\n",
      "        [-0.00453409,  0.0041738 , -0.00999201, ...,  0.0043873 ,\n",
      "         -0.00605751,  0.00297927],\n",
      "        ...,\n",
      "        [-0.01077981,  0.00864188, -0.00248025, ...,  0.00512648,\n",
      "          0.00512343, -0.00818464],\n",
      "        [-0.00443519,  0.00541524,  0.00158378, ...,  0.0027194 ,\n",
      "          0.00629758,  0.00702635],\n",
      "        [-0.00347998,  0.00826959,  0.00403896, ..., -0.00588093,\n",
      "         -0.0034976 ,  0.00607763]]], dtype=float32)>, <tf.Variable 'mlm_transformer_encoder/transformer_encoder/encoder/multi_head_attention/value/bias:0' shape=(8, 512) dtype=float32, numpy=\n",
      "array([[-0.00379738,  0.00542637, -0.00546412, ...,  0.00587357,\n",
      "         0.0063823 ,  0.00546219],\n",
      "       [ 0.00480335, -0.00441193, -0.00117607, ..., -0.00748936,\n",
      "        -0.00492389,  0.00545303],\n",
      "       [-0.00705285,  0.0046393 , -0.00583449, ...,  0.00210708,\n",
      "        -0.0047956 ,  0.00510931],\n",
      "       ...,\n",
      "       [-0.00660492,  0.0052624 , -0.00432463, ...,  0.00500128,\n",
      "         0.00297859, -0.00779042],\n",
      "       [-0.00582814,  0.00516417,  0.00471953, ...,  0.00613055,\n",
      "         0.00496303,  0.00446028],\n",
      "       [-0.00519339,  0.00495214,  0.00152588, ..., -0.00488258,\n",
      "         0.00082287,  0.00239225]], dtype=float32)>, <tf.Variable 'mlm_transformer_encoder/transformer_encoder/encoder/multi_head_attention/attention_output/kernel:0' shape=(8, 512, 512) dtype=float32, numpy=\n",
      "array([[[ 7.89143704e-03, -1.93826258e-02, -8.19052290e-03, ...,\n",
      "         -3.20809409e-02,  8.02599732e-03, -2.42203102e-02],\n",
      "        [ 1.76129956e-02,  1.46197509e-02, -2.17354503e-02, ...,\n",
      "          2.05942802e-03,  2.83146091e-02, -2.50749849e-02],\n",
      "        [ 2.14418042e-02, -3.32349017e-02, -2.34094523e-02, ...,\n",
      "          3.10412748e-03,  1.12123890e-02,  1.79812964e-02],\n",
      "        ...,\n",
      "        [ 2.01918930e-02, -9.92351305e-03,  1.69659294e-02, ...,\n",
      "         -1.10520730e-02,  5.74672548e-03, -1.19136283e-02],\n",
      "        [-2.28546956e-03,  9.53010842e-03, -1.81174707e-02, ...,\n",
      "         -2.86437408e-03,  1.78746618e-02,  1.32422773e-02],\n",
      "        [-2.32023597e-02,  2.56212242e-02, -1.09404884e-02, ...,\n",
      "          2.24763434e-02,  1.23035321e-02,  1.28199840e-02]],\n",
      "\n",
      "       [[-2.39760298e-02, -1.70752890e-02, -2.81297881e-02, ...,\n",
      "          4.62153275e-03, -2.73749903e-02, -6.33920776e-03],\n",
      "        [ 9.66635160e-03, -2.71159988e-02,  4.78360103e-04, ...,\n",
      "          1.19137177e-02, -2.61040451e-03, -6.17191009e-03],\n",
      "        [ 1.98349189e-02,  1.51240611e-02,  4.68676118e-03, ...,\n",
      "         -1.91511717e-02,  1.32134615e-03, -1.09481625e-02],\n",
      "        ...,\n",
      "        [-4.96051274e-03,  2.94090156e-03, -1.12886112e-02, ...,\n",
      "          6.27971999e-03,  1.23975556e-02, -4.03052662e-03],\n",
      "        [-2.58580819e-02,  1.21351508e-02,  2.29356326e-02, ...,\n",
      "          2.33168621e-02,  3.16343606e-02,  6.13152795e-03],\n",
      "        [ 2.16302127e-02,  1.80021729e-02,  1.64304264e-02, ...,\n",
      "          2.29694434e-02, -2.73437053e-02, -9.10828169e-03]],\n",
      "\n",
      "       [[ 5.44360548e-04, -2.30233707e-02, -1.73837058e-02, ...,\n",
      "         -3.10270279e-03, -8.29437340e-07,  2.60457546e-02],\n",
      "        [ 3.53250909e-03,  1.32435802e-02,  7.84210023e-03, ...,\n",
      "         -4.49504191e-03,  1.53716253e-02, -1.74445380e-02],\n",
      "        [-1.09130910e-04, -1.75261237e-02,  7.66954944e-03, ...,\n",
      "          9.19481087e-03,  1.28181148e-02, -1.32179437e-02],\n",
      "        ...,\n",
      "        [-1.51226874e-02,  1.75026525e-03, -1.88121814e-02, ...,\n",
      "         -1.00608310e-02,  2.61699893e-02, -2.92444769e-02],\n",
      "        [ 1.75172146e-02, -2.21946388e-02,  1.88674945e-02, ...,\n",
      "          2.81053805e-03,  1.14337336e-02, -2.54877340e-02],\n",
      "        [-3.76470550e-03,  1.87418535e-02, -1.87078323e-02, ...,\n",
      "         -2.00473517e-02,  1.11250710e-02,  1.81744471e-02]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-1.64119285e-02, -2.07486674e-02, -3.43186734e-03, ...,\n",
      "          1.46350600e-02, -6.24622870e-03, -1.65026207e-02],\n",
      "        [-6.68814592e-03, -1.66056510e-02,  1.84217561e-02, ...,\n",
      "         -1.26117212e-03,  1.00494893e-02, -2.09262092e-02],\n",
      "        [-1.96880475e-02, -3.42116952e-02, -9.41159483e-03, ...,\n",
      "          2.00752504e-02,  2.05165651e-02,  9.72860144e-04],\n",
      "        ...,\n",
      "        [-1.94253437e-02,  1.38531374e-02, -1.66507419e-02, ...,\n",
      "          2.84490921e-02,  1.09086023e-03, -1.24611724e-02],\n",
      "        [-7.26761995e-03,  9.75269452e-03,  6.31873822e-03, ...,\n",
      "          2.59270929e-02,  1.44278712e-03, -1.73725802e-02],\n",
      "        [-2.33014929e-03,  2.56611471e-04,  1.52736614e-02, ...,\n",
      "         -1.47086836e-03,  2.19896045e-02, -7.30241835e-03]],\n",
      "\n",
      "       [[-1.42181967e-03,  2.50439090e-03, -1.04289735e-02, ...,\n",
      "         -2.20828876e-02, -2.00549904e-02,  1.30844803e-03],\n",
      "        [-2.24735290e-02,  2.07578167e-02,  1.23535972e-02, ...,\n",
      "         -1.69183146e-02, -4.40070173e-03, -3.09071131e-03],\n",
      "        [ 1.60000026e-02,  2.19392218e-02,  5.32808620e-03, ...,\n",
      "          2.69084163e-02,  1.61229540e-02,  1.23355389e-02],\n",
      "        ...,\n",
      "        [ 5.46106463e-03, -6.49725785e-04,  3.30097182e-03, ...,\n",
      "         -1.87332928e-02, -7.65955728e-03, -4.99484304e-04],\n",
      "        [ 3.52264335e-03,  7.33585423e-03,  8.99075903e-03, ...,\n",
      "         -1.11733368e-02, -1.21606048e-02, -1.12887006e-02],\n",
      "        [-1.69473942e-02, -7.81167904e-03, -3.05369981e-02, ...,\n",
      "          3.67820053e-03,  1.12134069e-02,  1.27155297e-02]],\n",
      "\n",
      "       [[ 7.10283965e-03, -3.36700790e-02,  1.15971928e-02, ...,\n",
      "         -3.98252206e-03,  1.46695217e-02,  2.52511799e-02],\n",
      "        [ 2.66170464e-02,  3.11688147e-02, -3.27546783e-02, ...,\n",
      "         -7.54658738e-03,  1.39409294e-02,  1.83655825e-02],\n",
      "        [-2.52021998e-02,  1.75954811e-02, -1.32020665e-02, ...,\n",
      "         -1.02962246e-02, -2.20709518e-02,  3.95841058e-03],\n",
      "        ...,\n",
      "        [ 6.63271826e-03,  3.17614689e-03,  1.45810219e-02, ...,\n",
      "         -2.06804238e-02, -1.69977248e-02,  2.33242810e-02],\n",
      "        [ 1.75648890e-02,  2.36288104e-02, -1.06977932e-02, ...,\n",
      "         -5.93184773e-03,  1.55820837e-02,  1.01580406e-02],\n",
      "        [-1.72925498e-02,  1.52886529e-02, -1.85503494e-02, ...,\n",
      "          1.56440097e-03,  1.47061783e-03, -2.87418021e-04]]],\n",
      "      dtype=float32)>, <tf.Variable 'mlm_transformer_encoder/transformer_encoder/encoder/multi_head_attention/attention_output/bias:0' shape=(512,) dtype=float32, numpy=\n",
      "array([ 3.5953533e-03, -2.4315289e-03, -6.1836718e-03, -3.2072659e-03,\n",
      "       -7.2525647e-03, -5.2009667e-03,  3.1616059e-03, -1.5047957e-03,\n",
      "       -6.7646578e-03, -5.1333397e-03,  6.0045291e-03, -6.8460172e-03,\n",
      "        5.6993347e-03, -6.1781923e-03, -5.9885276e-03, -3.7807794e-03,\n",
      "        5.8327685e-03,  6.0402467e-03, -2.0588846e-04,  6.6510406e-03,\n",
      "       -6.1667915e-03, -6.6223368e-03, -5.7265987e-03, -5.5074934e-03,\n",
      "       -5.8362498e-03, -5.0476887e-03,  7.9337828e-04, -4.2098686e-03,\n",
      "       -6.1481227e-03, -5.5144844e-03,  5.4604849e-03, -1.7848779e-03,\n",
      "        6.0396376e-03,  6.7079221e-03,  1.8423237e-03,  5.3659980e-03,\n",
      "        3.2797267e-03,  5.0685783e-03, -6.0691056e-03,  3.4257201e-03,\n",
      "        5.4917759e-03, -5.9617450e-03, -4.7319108e-03,  5.9025316e-03,\n",
      "       -5.6045968e-03,  5.4760450e-03, -8.3819795e-03,  6.3237902e-03,\n",
      "        6.3187056e-03,  8.6863823e-03, -5.9155226e-03, -5.9318095e-03,\n",
      "        4.3864846e-03,  6.3468036e-03,  3.9427876e-03, -4.3166308e-03,\n",
      "       -3.4704637e-03, -6.5376181e-03, -3.9500347e-03,  6.8890583e-03,\n",
      "        6.4266799e-03,  7.0409812e-03,  5.6757163e-03, -5.0357124e-03,\n",
      "        4.4807130e-03, -5.0813262e-03,  5.4672854e-03,  6.1470061e-03,\n",
      "       -6.0227802e-03,  5.7234466e-03, -6.2844157e-03,  5.4646800e-03,\n",
      "        1.4706269e-03, -5.8861375e-03,  6.3837296e-03,  5.2800435e-03,\n",
      "       -6.1587272e-03, -7.4618990e-03, -1.6059969e-03, -7.0784837e-03,\n",
      "        6.2391115e-03, -5.5744336e-03, -6.1536334e-03,  6.8348902e-03,\n",
      "        5.6860241e-04,  5.4685520e-03, -1.6404891e-03,  6.8885344e-03,\n",
      "        3.7036950e-03, -5.7584285e-03, -6.0652755e-03, -5.6136400e-03,\n",
      "        1.2348026e-04, -2.5855927e-04,  1.8120107e-03,  2.7712507e-03,\n",
      "        5.3404453e-03,  8.6219581e-03,  5.3566550e-03,  6.8113557e-03,\n",
      "       -5.1741437e-03, -5.0837509e-03, -5.2020731e-03, -5.3624292e-03,\n",
      "        5.9076636e-03,  6.5577473e-03, -8.5238284e-03, -5.8668922e-03,\n",
      "       -7.0033227e-03, -6.2091365e-03,  5.5249245e-03,  6.9893869e-03,\n",
      "        5.5000894e-03,  5.7899691e-03, -5.7576457e-03,  5.7758270e-03,\n",
      "        5.2637751e-03, -4.9089175e-03, -5.5129821e-03, -5.2051265e-03,\n",
      "       -5.4698489e-03, -5.8896816e-03,  5.1863939e-03,  3.7095388e-03,\n",
      "        6.2268483e-03,  2.3587253e-03,  4.0470930e-03, -2.7359601e-03,\n",
      "       -2.0229272e-03, -5.4814168e-03, -5.5910782e-03, -6.1657461e-03,\n",
      "        5.1430520e-03,  5.6443862e-03, -5.2455780e-03,  6.3149272e-03,\n",
      "       -6.7143799e-03, -5.7903295e-03, -5.9374194e-03, -6.7321989e-03,\n",
      "        5.4067564e-03, -6.0685449e-03,  2.5804122e-03,  5.4753749e-03,\n",
      "       -6.5978188e-03,  5.5165607e-03, -7.0230747e-03,  6.4172498e-03,\n",
      "       -6.1140950e-03,  6.1427900e-03,  5.9962012e-03, -4.4260272e-03,\n",
      "       -6.6253743e-03,  2.0848075e-03, -3.2544183e-03, -6.1567943e-03,\n",
      "        5.5940798e-03,  5.8522481e-03, -5.2277022e-03, -5.0405646e-03,\n",
      "       -5.5127209e-03, -5.3572580e-03, -8.8708177e-03,  6.4250319e-03,\n",
      "       -5.6007048e-03, -2.6083402e-03, -5.2996604e-03, -2.5327105e-04,\n",
      "        5.4823011e-03,  6.2643974e-03, -5.8404105e-03,  5.8230939e-03,\n",
      "        6.8400661e-03,  6.1960095e-03,  5.6537720e-03,  6.7688758e-03,\n",
      "        5.4982733e-03, -5.4911328e-03, -1.6050820e-03,  6.9677834e-03,\n",
      "        4.7092792e-03, -4.6598469e-03, -7.0345141e-03,  6.2375902e-03,\n",
      "        5.0459960e-03,  8.7780046e-04, -5.5789812e-03, -5.7005039e-03,\n",
      "       -5.2609495e-03,  6.9080675e-03,  7.1338303e-03,  4.0483144e-03,\n",
      "       -6.8406076e-03, -4.5994278e-03, -8.1234463e-03,  6.0385996e-03,\n",
      "        3.6122703e-03, -6.1432882e-03,  4.0114849e-04, -4.5494176e-03,\n",
      "        4.3502348e-03,  6.5990249e-03,  3.3045586e-03, -5.3971843e-03,\n",
      "       -5.4861326e-03,  2.7799401e-03, -1.2355930e-03, -5.4057087e-03,\n",
      "       -6.5718954e-03,  6.0028704e-03,  5.6206067e-03,  1.4213705e-03,\n",
      "       -6.9244127e-03,  3.1131546e-03, -6.4083668e-03, -2.9621811e-03,\n",
      "       -3.0416918e-03,  6.7621535e-03,  6.5076887e-03, -5.9514148e-03,\n",
      "       -6.7461748e-04,  6.0266112e-03, -6.2379176e-03, -7.8728376e-03,\n",
      "        6.2669469e-03, -5.0913682e-03,  1.7938622e-03,  6.3743014e-03,\n",
      "       -6.2411809e-03,  7.4953060e-03, -5.7117823e-03, -2.6937672e-03,\n",
      "       -4.6134870e-03,  6.0212361e-03,  5.2642259e-03, -5.1095621e-03,\n",
      "       -5.7106246e-03,  5.9022950e-03,  3.5118102e-03, -6.0651037e-03,\n",
      "       -7.6871924e-04, -5.9127314e-03,  1.7904810e-03,  6.2054629e-03,\n",
      "        5.3215059e-03, -4.1286405e-03,  5.0359457e-03,  5.8172015e-03,\n",
      "        5.7918783e-03,  5.6477506e-03, -6.7645870e-03, -1.1742889e-03,\n",
      "       -5.7843346e-03,  5.6463634e-03, -5.5906735e-03,  5.9569781e-03,\n",
      "        4.6387208e-03,  5.2156551e-03, -6.3839010e-03,  5.3454372e-03,\n",
      "        5.5131740e-03, -3.7996580e-03,  9.8442531e-04,  4.3639733e-04,\n",
      "       -6.2919115e-03, -5.8849170e-03,  5.7389140e-03, -5.3316001e-03,\n",
      "       -5.1384759e-03, -5.0844750e-03,  5.1723439e-03,  7.5554200e-03,\n",
      "        2.9201433e-03, -5.7058958e-03, -5.7141404e-03,  4.7863987e-03,\n",
      "        3.2328309e-03,  6.1039296e-03, -8.1631803e-04,  6.2548826e-03,\n",
      "       -6.8930932e-03,  2.8548369e-03, -5.9481738e-03,  4.8245867e-03,\n",
      "       -5.0173434e-03, -5.7669682e-03,  5.4288167e-03,  7.1609584e-03,\n",
      "       -1.6724357e-03, -4.0102628e-04, -6.2750038e-03, -5.2870065e-03,\n",
      "        5.3595658e-03,  5.5458220e-03, -3.9830082e-04,  5.8608414e-03,\n",
      "        5.9005050e-03,  5.5636973e-03,  4.1184425e-03, -5.4600658e-03,\n",
      "        5.7033892e-03,  4.3495870e-03,  4.7153500e-03, -6.9281114e-03,\n",
      "        8.0689434e-03, -5.6493999e-03,  3.6434741e-03, -5.5027017e-03,\n",
      "       -5.9123402e-03, -5.8722203e-03,  4.7049280e-03, -5.7385191e-03,\n",
      "        5.9758076e-03, -5.8215950e-03, -6.2502385e-03,  6.4190510e-03,\n",
      "        6.2778289e-03,  5.1536201e-03, -5.8868560e-03,  5.5033579e-03,\n",
      "        5.9706839e-03,  3.2166932e-03,  2.0785313e-03,  5.9814616e-03,\n",
      "        5.2504195e-03, -5.1447568e-03, -6.0152519e-03,  7.4709398e-03,\n",
      "        7.2163069e-03, -5.6061936e-03,  6.2842909e-03, -6.2472573e-03,\n",
      "        5.7420051e-03,  6.7203352e-03, -7.2399490e-03, -2.6218409e-03,\n",
      "       -5.9931576e-03,  4.9364851e-03,  6.1615775e-03,  6.5655862e-03,\n",
      "       -4.5769154e-03, -5.7707392e-03,  4.3222369e-03,  7.0181317e-03,\n",
      "        7.2412137e-03,  5.5472064e-03,  7.2356951e-03, -7.9080006e-03,\n",
      "        3.0722069e-03,  5.1539247e-03,  6.3426448e-03,  6.6121873e-03,\n",
      "        6.2861955e-03, -4.8998081e-05, -2.8253382e-03,  5.2787922e-03,\n",
      "        2.7139948e-03,  4.2592050e-03,  4.4690459e-03, -5.7409280e-03,\n",
      "       -6.0494635e-03, -5.7566785e-03,  6.7278440e-03, -5.4450608e-03,\n",
      "       -1.4277238e-03,  5.8998792e-03, -2.5354146e-03, -6.4381715e-03,\n",
      "        1.9868708e-03, -5.6495531e-03, -5.7319035e-03, -6.4906455e-03,\n",
      "       -5.3978032e-03,  9.1481693e-03, -7.1773231e-03,  6.9366177e-03,\n",
      "        5.3763138e-03,  7.1301418e-03, -5.4804990e-03,  5.5477335e-03,\n",
      "       -5.7330737e-03, -5.0319452e-03,  5.6857737e-03, -4.5095719e-03,\n",
      "       -5.9948140e-03, -4.9286694e-03,  1.9974620e-03, -4.9038730e-03,\n",
      "       -6.1424952e-03, -5.5205440e-03, -6.0600583e-03, -3.7268843e-03,\n",
      "       -5.8550020e-03, -5.3504682e-03,  5.2010827e-03, -4.5998082e-03,\n",
      "        6.4605731e-03, -5.9481487e-03,  4.4447039e-03,  6.0658730e-03,\n",
      "        8.9077883e-05, -4.9498449e-03, -6.0626632e-03,  4.0018056e-03,\n",
      "        5.6522647e-03, -5.7645240e-03,  5.9534055e-03, -4.5059361e-03,\n",
      "       -6.5981764e-03,  5.3227018e-03, -5.2545313e-03, -6.0287486e-03,\n",
      "       -5.7986155e-03,  6.0341931e-03, -6.5057711e-03,  5.5116503e-03,\n",
      "        3.6764063e-03, -6.4891647e-03,  4.6475856e-03,  7.0253707e-04,\n",
      "       -6.5332055e-03, -6.3361898e-03, -5.9461850e-03, -6.1261002e-03,\n",
      "        6.0398621e-03,  3.9318250e-03,  1.0561261e-03,  3.0241124e-03,\n",
      "        6.2662954e-03,  7.1362918e-03,  5.8201323e-03, -4.3304698e-03,\n",
      "       -5.0746193e-03,  5.5638016e-03,  6.0698809e-03, -4.5207809e-03,\n",
      "       -3.5345235e-03, -5.2965591e-03,  5.1465952e-03,  6.5851966e-03,\n",
      "       -5.5446746e-03,  3.7218905e-03,  4.6222643e-03, -2.8429481e-03,\n",
      "        5.7033449e-03, -7.4058590e-03,  5.3039659e-03, -5.2599353e-03,\n",
      "        6.4041303e-03,  5.3843195e-03, -9.2406766e-03, -6.7103719e-03,\n",
      "        2.1893521e-04,  6.3760728e-03, -2.7584138e-03,  6.8848841e-03,\n",
      "        6.1872341e-03, -7.5663864e-03,  5.9804586e-03,  5.9928400e-03,\n",
      "        6.0242997e-03, -5.9033590e-03, -7.0407321e-03,  4.8674154e-03,\n",
      "        5.0090221e-03,  6.5338458e-03,  1.6757327e-03,  6.5272190e-03,\n",
      "       -6.2312605e-03,  6.0238563e-03,  6.3157263e-03, -4.0078820e-03,\n",
      "        2.3451382e-03, -6.5060938e-03, -7.9088956e-03, -7.6685762e-03,\n",
      "        3.4740704e-04,  5.8035515e-03,  6.2322235e-03, -5.5552884e-03,\n",
      "       -5.9817666e-03,  6.1134081e-03,  5.9396662e-03,  4.1302727e-03,\n",
      "        4.2594098e-03,  6.5435530e-03, -1.5315486e-03, -5.4057282e-03,\n",
      "        6.0452926e-03,  5.4765590e-03,  5.8268956e-03,  2.8871389e-03,\n",
      "       -5.6811911e-03,  6.3008843e-03,  4.0853349e-03,  6.2848050e-03,\n",
      "        5.5396780e-03, -3.9820825e-03, -5.8267126e-03, -3.0040371e-03,\n",
      "        5.7510082e-03, -6.4738961e-03,  5.6089316e-03, -4.6736160e-03,\n",
      "       -3.4487124e-03,  6.2876260e-03,  5.6044920e-03,  3.1793432e-03,\n",
      "       -5.3907554e-03, -5.3012106e-03, -5.7117627e-03,  2.0401219e-03],\n",
      "      dtype=float32)>, <tf.Variable 'dense/kernel:0' shape=(512, 2048) dtype=float32, numpy=\n",
      "array([[ 0.01917462, -0.02420222,  0.00655124, ...,  0.011299  ,\n",
      "        -0.05075932,  0.01945783],\n",
      "       [-0.03329679, -0.02283483,  0.02380111, ..., -0.01816903,\n",
      "         0.02961505, -0.01621686],\n",
      "       [ 0.048807  ,  0.02743931, -0.03800359, ..., -0.03209066,\n",
      "         0.03020477,  0.01229221],\n",
      "       ...,\n",
      "       [-0.00045311, -0.00440641,  0.01957208, ..., -0.03588355,\n",
      "        -0.00457558,  0.02839496],\n",
      "       [ 0.04530419,  0.02967578, -0.03419957, ...,  0.04940258,\n",
      "        -0.02203717,  0.03840968],\n",
      "       [-0.00856716,  0.02936651,  0.02497915, ...,  0.03637055,\n",
      "        -0.00730737,  0.04158644]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(2048,) dtype=float32, numpy=\n",
      "array([-0.00551034,  0.        ,  0.00669995, ..., -0.00154074,\n",
      "       -0.00498932, -0.00499497], dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(2048, 512) dtype=float32, numpy=\n",
      "array([[-0.03008178,  0.02236566,  0.00856611, ..., -0.01333221,\n",
      "         0.04037883,  0.0207771 ],\n",
      "       [-0.00948597, -0.02272829,  0.03104936, ..., -0.00916557,\n",
      "         0.03307357,  0.04549661],\n",
      "       [-0.04434541,  0.01841066,  0.007526  , ..., -0.03419996,\n",
      "        -0.04461775, -0.03519849],\n",
      "       ...,\n",
      "       [-0.0507703 , -0.02986244,  0.02189373, ..., -0.00858706,\n",
      "         0.02497053,  0.02582317],\n",
      "       [ 0.02160561,  0.01080151,  0.01038546, ..., -0.02726142,\n",
      "        -0.00972358, -0.0477    ],\n",
      "       [ 0.0185297 ,  0.02998228,  0.01339726, ..., -0.03926681,\n",
      "         0.00404131, -0.02744539]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(512,) dtype=float32, numpy=\n",
      "array([-6.5850890e-03, -2.2732469e-03, -6.6462508e-03, -4.7942661e-03,\n",
      "       -7.1070502e-03, -6.1589428e-03, -2.8695650e-03, -2.6978455e-03,\n",
      "       -6.9052172e-03, -2.6972103e-03,  6.1719455e-03, -6.7214728e-03,\n",
      "        6.7833699e-03, -5.6491117e-03, -7.6752407e-03, -1.2841170e-03,\n",
      "        4.9362625e-03,  6.4034848e-03,  4.2201504e-03,  5.9022759e-03,\n",
      "       -7.1106530e-03, -3.3457936e-03, -6.3408660e-03, -6.6822711e-03,\n",
      "        5.0824462e-03, -4.6515591e-03,  6.1691431e-03,  5.7717250e-03,\n",
      "       -5.5103637e-03, -6.8394360e-03, -6.4424672e-03,  7.7377879e-03,\n",
      "        6.9773369e-03,  7.0675975e-03,  1.9086053e-03,  6.3146930e-03,\n",
      "        3.1472920e-03,  5.9856717e-03, -7.3077832e-03, -2.0654162e-03,\n",
      "        6.4481720e-03, -6.4888340e-03, -5.7554767e-03,  7.5055142e-03,\n",
      "       -5.9057823e-03,  7.6451404e-03, -7.9938993e-03,  6.4353268e-03,\n",
      "        6.5720784e-03,  7.9520969e-03, -6.5033683e-03, -8.2440367e-03,\n",
      "        8.0036493e-03,  8.0779381e-03, -6.5256995e-03, -2.1382205e-03,\n",
      "       -7.5987224e-03, -7.3688529e-03, -1.9409598e-03,  8.1252232e-03,\n",
      "        5.8178944e-03,  7.0198081e-03,  5.9113717e-03, -6.5598213e-03,\n",
      "       -5.6040464e-03, -6.5287072e-03,  7.9028374e-03,  6.9837742e-03,\n",
      "       -7.9277465e-03,  6.9324072e-03, -7.0129898e-03,  7.3072948e-03,\n",
      "       -6.8185944e-03, -8.8266330e-03,  6.0330960e-03,  6.8245349e-03,\n",
      "       -7.3615755e-03, -2.7915679e-03,  3.4120616e-03, -7.5008697e-03,\n",
      "        6.4321919e-03, -6.6636428e-03, -7.7499431e-03,  7.1739699e-03,\n",
      "       -6.8114451e-03,  6.9480990e-03, -3.5502238e-03,  6.2318938e-03,\n",
      "        2.8593112e-03, -6.1782710e-03, -6.3023572e-03, -6.1793854e-03,\n",
      "        5.0397366e-03, -5.0659264e-03, -4.9226247e-03,  5.5304337e-03,\n",
      "        6.6539939e-03,  4.1473564e-03,  7.3881857e-03,  6.5870523e-03,\n",
      "       -7.2400626e-03, -5.9783626e-03, -5.7471674e-03, -6.2942915e-03,\n",
      "        6.9872648e-03,  7.7813067e-03, -7.3107025e-03, -6.1855814e-03,\n",
      "       -7.3194318e-03, -3.2123751e-03,  7.2859139e-03,  8.4192669e-03,\n",
      "        4.7886302e-03,  7.2929943e-03, -5.7215160e-03,  6.8832240e-03,\n",
      "        1.9075900e-03, -2.5596146e-03, -5.7437634e-03, -6.4087147e-03,\n",
      "       -6.3093514e-03, -5.7227216e-03,  3.4380243e-03,  6.9782551e-04,\n",
      "        6.2560686e-03,  1.4823139e-05,  4.5442767e-03, -4.8210560e-03,\n",
      "       -6.4301817e-03, -6.7574480e-03, -7.7151181e-03, -6.9614467e-03,\n",
      "        6.1023631e-03,  6.6332882e-03, -2.9053104e-03,  7.4206912e-03,\n",
      "       -7.4805790e-03, -7.0288228e-03, -6.8373815e-03, -7.7708210e-03,\n",
      "       -9.7956974e-04, -6.6527892e-03, -6.6802814e-04,  3.3765538e-03,\n",
      "       -7.3891799e-03,  6.1036269e-03, -1.0086461e-03,  6.9025047e-03,\n",
      "       -6.6516139e-03,  7.1650296e-03,  5.7660840e-03, -5.3634034e-03,\n",
      "       -6.5272986e-03, -3.5117986e-03, -6.4257556e-03, -6.3166460e-03,\n",
      "        6.1141793e-03,  6.6228253e-03, -3.5810971e-03, -5.0366614e-03,\n",
      "       -5.5303867e-03, -5.5913604e-03, -6.0512300e-04,  6.3751913e-03,\n",
      "       -1.7358002e-03, -6.0967444e-03, -5.3394702e-03,  6.4061061e-03,\n",
      "        5.9382133e-03,  7.4779023e-03, -7.0832875e-03,  2.7565989e-03,\n",
      "        6.9885436e-03,  6.6375630e-03,  7.2231716e-03,  7.0916926e-03,\n",
      "        5.5003539e-03,  1.9871103e-04, -6.0197059e-03,  8.5838130e-03,\n",
      "        6.4173653e-03, -4.3232040e-03, -8.2096932e-03,  7.0627239e-03,\n",
      "        4.8531927e-03,  3.1503979e-03, -7.0695234e-03, -6.4407149e-03,\n",
      "       -5.3142970e-03,  2.7422903e-03,  7.4771759e-03,  7.3705870e-03,\n",
      "       -7.8396462e-03, -2.7276631e-03, -7.6216073e-03,  7.3823268e-03,\n",
      "       -3.0591793e-03, -7.2618336e-03, -4.5039994e-03,  5.4119714e-03,\n",
      "        4.6452736e-03,  6.2129996e-03,  5.2805673e-03, -5.4995236e-03,\n",
      "       -5.9058587e-03, -9.0758493e-03,  4.2425515e-04, -6.8061803e-03,\n",
      "       -8.3000707e-03,  6.6910265e-03,  5.5556102e-03, -1.9314433e-03,\n",
      "       -7.5674574e-03,  1.4896882e-03, -6.7202253e-03, -6.6945888e-03,\n",
      "       -3.3360315e-03,  8.1588356e-03,  5.9957779e-03, -6.2063080e-03,\n",
      "        7.2083450e-03,  7.3417774e-03, -7.9916837e-03, -7.9297768e-03,\n",
      "        6.3432325e-03, -6.8134530e-03, -2.4738431e-03,  8.3507420e-03,\n",
      "       -7.2492613e-03,  7.3957779e-03, -7.0508928e-03,  6.5491023e-03,\n",
      "       -6.0855458e-04,  6.8407357e-03,  6.5316176e-03, -3.3841908e-03,\n",
      "       -7.5556384e-03,  6.7201266e-03, -2.7279817e-03, -5.6350501e-03,\n",
      "        3.2291100e-03, -5.9283879e-03,  5.5404138e-03,  6.1127199e-03,\n",
      "        3.2856970e-03, -4.9379999e-03, -2.4997778e-03,  5.8914041e-03,\n",
      "       -6.6732294e-03, -1.2972978e-03, -7.6591913e-03,  7.9961205e-03,\n",
      "       -6.3375710e-03,  6.6006901e-03, -6.8376362e-03, -6.5991664e-03,\n",
      "        6.9666789e-03,  6.1126524e-03, -1.4194463e-03,  7.3334570e-03,\n",
      "        5.9821140e-03, -7.3556248e-03,  9.7493385e-04, -7.4170833e-04,\n",
      "       -7.2790394e-03, -7.3336731e-03,  6.5745125e-03, -6.5572746e-03,\n",
      "       -6.8627284e-03, -6.0931314e-03,  3.3779889e-03,  6.4032129e-03,\n",
      "        4.9402760e-03, -5.9544835e-03, -7.1239718e-03,  5.9410906e-03,\n",
      "        3.0041698e-03,  5.6678597e-03,  6.4384486e-03,  6.8433969e-03,\n",
      "       -7.4838139e-03, -3.1746323e-03, -4.9220691e-03,  5.2451333e-03,\n",
      "       -5.7945205e-03, -5.3610872e-03,  6.3846493e-03,  6.7481506e-03,\n",
      "        1.5824983e-03, -7.9665827e-03, -7.5553106e-03, -5.9818607e-03,\n",
      "        7.6418213e-04,  6.6449344e-03,  2.2004442e-03,  7.4710641e-03,\n",
      "        7.0856526e-03,  5.2457438e-03,  7.9052045e-04, -5.3183488e-03,\n",
      "        6.4523784e-03,  4.4587208e-03,  3.2969702e-03, -7.2742198e-03,\n",
      "       -4.7282288e-03, -6.9648824e-03, -1.4203644e-03, -4.7326307e-03,\n",
      "       -7.1679098e-03, -5.5164681e-03, -5.7322783e-03, -6.0687154e-03,\n",
      "        7.5967368e-03, -7.2295298e-03, -6.1558629e-03,  7.3386598e-03,\n",
      "        4.9223411e-03,  6.0238610e-03, -6.5101078e-03,  7.0613124e-03,\n",
      "        5.9892577e-03,  3.5120845e-03,  8.2716969e-04,  6.4261048e-03,\n",
      "       -6.6904007e-03, -1.3118078e-03, -6.9667641e-03,  7.6364395e-03,\n",
      "        6.8596900e-03, -5.3091445e-03,  7.4073425e-03, -7.1423170e-03,\n",
      "        6.9763553e-03,  3.5055168e-03, -6.6962056e-03, -2.0767457e-03,\n",
      "       -6.5537598e-03,  7.9140710e-03,  6.5708519e-03,  6.6519845e-03,\n",
      "       -1.9531177e-03, -6.6240150e-03, -6.0474686e-03,  7.5131198e-03,\n",
      "        5.9495647e-03,  4.8403568e-03,  6.0591740e-03, -4.2227167e-03,\n",
      "       -6.9106240e-03,  1.4689264e-03,  6.8726195e-03,  6.9398805e-03,\n",
      "        5.9855054e-03,  1.0033088e-03,  6.2200301e-03,  6.6670184e-03,\n",
      "        4.1070380e-03, -5.4839603e-03,  5.4564266e-03, -4.9079936e-03,\n",
      "       -7.1445643e-03, -6.2812665e-03,  6.8882350e-03, -2.8399732e-03,\n",
      "       -5.7513022e-04,  6.1597512e-03, -6.0526440e-03, -7.0873252e-03,\n",
      "       -1.4137343e-03, -7.2013577e-03, -5.8308672e-03, -7.6465178e-03,\n",
      "       -6.7242179e-03,  7.2564743e-03, -7.3634251e-03,  9.3088867e-03,\n",
      "        6.6291885e-03,  8.2968865e-03, -6.4745299e-03,  7.0729679e-03,\n",
      "       -6.3312952e-03, -4.0637078e-03,  5.8239093e-03, -4.5236023e-03,\n",
      "       -7.2352327e-03, -5.0724680e-03, -7.3636421e-03, -6.1016171e-03,\n",
      "       -6.5070773e-03, -4.7979653e-03, -7.0520728e-03, -1.4282308e-03,\n",
      "        2.4673159e-03, -6.4752339e-03,  5.6073056e-03,  1.3672230e-04,\n",
      "        5.5112261e-03, -6.9316109e-03,  5.7917028e-03, -1.9751629e-03,\n",
      "        8.8317000e-04, -5.7011810e-03, -6.5277531e-03,  5.7928185e-03,\n",
      "        4.5297081e-03, -5.5004642e-03,  6.8792547e-03, -4.2726980e-03,\n",
      "       -7.0372825e-03,  5.5345409e-03, -7.6589626e-03, -6.3282070e-03,\n",
      "       -8.0328286e-03,  6.3798041e-03, -7.5535029e-03,  7.7181133e-03,\n",
      "        5.6540421e-03, -6.1013279e-03,  2.9091674e-03,  2.5007674e-03,\n",
      "       -3.9117127e-03, -5.9864866e-03, -5.1925932e-03, -6.5321978e-03,\n",
      "        7.4358964e-03,  4.7508613e-03, -2.4211220e-03,  4.6463180e-03,\n",
      "        6.3348264e-03,  7.1145399e-03,  5.7026953e-03,  8.0373362e-03,\n",
      "       -6.6831256e-03,  5.9304419e-03,  5.6661684e-03, -5.5013290e-03,\n",
      "        4.1122902e-03,  5.3424556e-03,  6.3218852e-03,  6.7420178e-03,\n",
      "       -6.9851605e-03,  5.5548111e-03,  2.9664033e-03, -5.0404477e-03,\n",
      "        4.9219150e-03, -6.8001198e-03,  5.3464212e-03, -5.2738185e-03,\n",
      "        6.9984132e-03,  7.1399324e-03, -4.9094483e-03, -6.8312930e-03,\n",
      "        2.0147823e-03,  6.8616304e-03,  2.2225671e-03,  7.0227394e-03,\n",
      "        5.9981216e-03, -8.2519716e-03,  4.8883152e-03,  5.8928565e-03,\n",
      "        5.9547909e-03, -6.7087249e-03, -8.4884493e-03,  5.9816539e-03,\n",
      "        4.7087539e-03,  6.7281951e-03,  2.7154342e-03,  6.7185480e-03,\n",
      "       -6.5660523e-03,  5.6098117e-03,  7.0303869e-03,  1.1633354e-03,\n",
      "        4.6832236e-03, -7.2961189e-03, -7.4291551e-03, -7.4580023e-03,\n",
      "        3.0842815e-03,  7.2035771e-03,  6.6399793e-03, -5.7127248e-03,\n",
      "       -6.6885827e-03,  5.5692415e-03,  6.1933319e-03,  6.5236636e-03,\n",
      "       -7.1502770e-03,  6.2979772e-03,  2.4260173e-03, -6.0950695e-03,\n",
      "        7.3236632e-03,  6.3249660e-03,  7.2721075e-03,  5.9775640e-03,\n",
      "       -7.0462273e-03,  6.7705777e-03,  6.5654274e-03,  7.9183495e-03,\n",
      "        5.6697330e-03, -7.6301169e-04, -6.2020193e-03,  1.6221200e-03,\n",
      "        6.8006143e-03, -6.5233586e-03,  4.8986641e-03, -3.9604288e-03,\n",
      "        3.6840972e-03,  7.0900568e-03,  6.4982926e-03,  1.7037749e-03,\n",
      "       -4.6946052e-03, -3.2740459e-03, -6.1181593e-03,  6.7277616e-03],\n",
      "      dtype=float32)>, <tf.Variable 'mlm_transformer_encoder/transformer_encoder/encoder/layer_normalization/gamma:0' shape=(512,) dtype=float32, numpy=\n",
      "array([0.9981758 , 0.9991286 , 1.0076401 , 0.99151266, 1.0070144 ,\n",
      "       0.9919226 , 0.9939944 , 0.99214906, 1.0088893 , 0.99198174,\n",
      "       1.0052549 , 1.0096217 , 0.9931693 , 0.9908963 , 1.0057769 ,\n",
      "       0.9905855 , 0.993254  , 1.0062506 , 0.99104995, 1.0072931 ,\n",
      "       1.0080036 , 1.002992  , 0.99485254, 0.9942006 , 0.9899762 ,\n",
      "       0.99323916, 0.99839914, 0.9922312 , 1.0085632 , 0.9931015 ,\n",
      "       1.000061  , 0.99683934, 0.99729574, 1.0088935 , 0.99175084,\n",
      "       0.99389994, 0.994069  , 0.99293584, 1.0077589 , 0.99524593,\n",
      "       0.99884415, 0.9941662 , 0.9934639 , 1.0064641 , 0.992225  ,\n",
      "       1.0036284 , 1.0083321 , 1.0065085 , 1.0070161 , 1.006946  ,\n",
      "       1.0061361 , 1.0028907 , 0.997839  , 1.0001118 , 0.9954119 ,\n",
      "       0.99486107, 0.99451977, 0.9924266 , 1.0015644 , 1.0061069 ,\n",
      "       0.9906812 , 1.0075204 , 1.0049853 , 0.99313796, 0.9926305 ,\n",
      "       0.99448216, 1.002423  , 1.0027773 , 1.0059277 , 1.0048566 ,\n",
      "       1.0085589 , 1.0044193 , 0.9959978 , 1.0026132 , 0.9916325 ,\n",
      "       0.990254  , 1.005881  , 1.0115441 , 1.0050875 , 1.0072745 ,\n",
      "       0.9927231 , 0.9935488 , 1.0073557 , 1.0077947 , 0.99830025,\n",
      "       0.995307  , 0.9978323 , 1.0069607 , 0.991755  , 1.008517  ,\n",
      "       1.0043732 , 0.9926954 , 0.99139327, 1.0030239 , 0.9971461 ,\n",
      "       0.9987622 , 0.99374896, 1.0114986 , 0.9933754 , 1.0081888 ,\n",
      "       1.0043408 , 0.99298066, 0.9928682 , 0.98759544, 0.99554133,\n",
      "       1.009025  , 1.009603  , 1.0051706 , 1.008188  , 0.9944304 ,\n",
      "       1.0047622 , 1.0078499 , 0.99061817, 0.98731434, 0.99400157,\n",
      "       1.0042683 , 0.9927653 , 0.993898  , 1.0038049 , 0.9941601 ,\n",
      "       0.9908893 , 0.99366736, 0.9899001 , 0.99816024, 0.994074  ,\n",
      "       0.9963936 , 0.9939944 , 0.99171394, 0.99789804, 0.99343294,\n",
      "       0.99210274, 0.9968264 , 0.99296683, 1.0058913 , 0.9915344 ,\n",
      "       1.0071118 , 1.0079888 , 0.9940665 , 1.0068653 , 1.0042562 ,\n",
      "       0.99212474, 0.9947337 , 0.99668664, 1.0053911 , 1.006537  ,\n",
      "       1.0031132 , 1.011331  , 1.0074543 , 1.0059005 , 1.0071034 ,\n",
      "       0.9907334 , 0.99317884, 0.99623096, 0.9936802 , 0.9989971 ,\n",
      "       0.99541086, 0.994198  , 0.99689716, 1.0044239 , 0.9935029 ,\n",
      "       0.9938862 , 0.9942456 , 1.0136158 , 1.0053406 , 1.0005231 ,\n",
      "       0.9902912 , 1.0040932 , 0.9920148 , 0.99438256, 1.0085331 ,\n",
      "       1.006425  , 1.011317  , 1.0055425 , 1.007331  , 0.99495393,\n",
      "       1.0086014 , 0.9934245 , 0.9937249 , 0.9915809 , 1.0088897 ,\n",
      "       0.99111396, 0.99027896, 1.0067854 , 1.0059346 , 0.9918187 ,\n",
      "       0.9962501 , 0.99510205, 0.99641126, 1.0037615 , 1.0086527 ,\n",
      "       1.0050106 , 1.0021487 , 1.0068245 , 1.0052189 , 1.009573  ,\n",
      "       1.0072706 , 0.9968611 , 0.9953933 , 0.99298894, 0.9952737 ,\n",
      "       0.9924446 , 1.0023516 , 0.99954534, 0.9941057 , 1.0026299 ,\n",
      "       0.9925272 , 0.99047005, 0.9971054 , 1.0081334 , 0.9943721 ,\n",
      "       1.003634  , 0.9853917 , 1.0066111 , 1.0017442 , 1.0072882 ,\n",
      "       0.99981385, 0.9986073 , 1.0069906 , 1.0032963 , 0.99179846,\n",
      "       0.9957583 , 0.9954911 , 1.0079358 , 1.0057381 , 1.0004768 ,\n",
      "       0.9932259 , 0.9954683 , 1.0068475 , 1.0083247 , 1.008447  ,\n",
      "       1.0062376 , 0.99551046, 0.9965228 , 0.9953419 , 0.9934286 ,\n",
      "       0.99358135, 1.0039477 , 1.0062604 , 0.99431986, 0.99275804,\n",
      "       0.9960306 , 0.992766  , 0.99437857, 1.0045009 , 0.9915153 ,\n",
      "       0.99391794, 0.99424005, 1.0060469 , 0.99192154, 0.99223644,\n",
      "       1.0092635 , 0.9898436 , 0.9947373 , 1.0058846 , 0.99397475,\n",
      "       0.9956598 , 0.99246764, 0.9930362 , 1.0003741 , 1.0047272 ,\n",
      "       0.99335605, 0.9885873 , 0.99598396, 0.99318624, 1.0015286 ,\n",
      "       1.0081733 , 0.9935012 , 0.99385685, 0.9955727 , 0.99455833,\n",
      "       0.9917872 , 1.0109338 , 0.99228406, 0.99407077, 1.0067652 ,\n",
      "       0.9936985 , 0.9967158 , 0.99998003, 0.9987746 , 1.0067576 ,\n",
      "       1.0079632 , 0.9908356 , 0.9901323 , 1.0010715 , 1.0036565 ,\n",
      "       0.992566  , 0.9942674 , 1.0073507 , 0.9945921 , 0.99762124,\n",
      "       1.0067164 , 0.99429685, 0.9922069 , 1.0035139 , 1.0020541 ,\n",
      "       0.99525315, 1.0035824 , 1.0005631 , 0.9970947 , 0.99439174,\n",
      "       0.994334  , 0.99242216, 0.99421513, 1.0018404 , 1.0075201 ,\n",
      "       1.0035444 , 0.995127  , 0.9998687 , 1.0072712 , 1.0088283 ,\n",
      "       0.9912161 , 1.002896  , 0.99276483, 1.0056229 , 1.0066911 ,\n",
      "       1.0080223 , 0.99291277, 0.9930493 , 1.0055906 , 0.99932414,\n",
      "       1.0048097 , 0.9902279 , 0.99558276, 1.0077686 , 0.98848134,\n",
      "       0.9930932 , 0.9922736 , 1.0085726 , 0.99614817, 0.993034  ,\n",
      "       1.0083023 , 1.0043274 , 1.0067334 , 1.0087266 , 1.0090746 ,\n",
      "       0.9902497 , 1.0046321 , 0.9924443 , 1.0075984 , 1.0071394 ,\n",
      "       1.0023493 , 1.0043079 , 0.99352914, 1.0089645 , 0.98840034,\n",
      "       1.0056379 , 0.98787844, 1.0097015 , 0.997736  , 1.0010875 ,\n",
      "       1.0082365 , 1.0088135 , 1.0066743 , 0.9937081 , 0.9948857 ,\n",
      "       1.002672  , 0.9954463 , 0.9998495 , 0.9903193 , 1.0059325 ,\n",
      "       1.0043576 , 0.9930689 , 1.0076858 , 0.9933683 , 0.99822783,\n",
      "       0.99684805, 0.990321  , 1.0075443 , 1.0042789 , 1.0045414 ,\n",
      "       1.0108743 , 1.0106106 , 0.99320495, 1.0086291 , 1.0078934 ,\n",
      "       1.0048184 , 0.99422586, 1.0092862 , 1.0044245 , 1.0043384 ,\n",
      "       0.99478   , 0.9935934 , 0.9979594 , 0.99260026, 1.0039302 ,\n",
      "       1.0023824 , 0.9986523 , 0.9935149 , 1.0028574 , 0.9921577 ,\n",
      "       1.0067439 , 0.993852  , 1.0025696 , 0.9943337 , 0.9937029 ,\n",
      "       0.9948727 , 0.9923996 , 0.99296963, 0.9944613 , 1.0051335 ,\n",
      "       0.998202  , 0.99440706, 1.0069805 , 1.0022562 , 1.0002456 ,\n",
      "       0.9942081 , 0.995061  , 0.99119395, 1.0044352 , 0.99422866,\n",
      "       0.9930909 , 1.0022235 , 1.0060085 , 1.0066621 , 1.0082462 ,\n",
      "       1.0055865 , 0.99475193, 0.9925335 , 0.99259216, 0.9953778 ,\n",
      "       1.0057462 , 0.9908055 , 0.9906966 , 0.995836  , 0.9930983 ,\n",
      "       0.9941411 , 0.9909397 , 0.9927221 , 1.0046772 , 1.0048383 ,\n",
      "       0.9894293 , 0.99483925, 0.99291444, 1.0051541 , 1.0069506 ,\n",
      "       0.99310994, 0.9908967 , 0.9923246 , 0.99205136, 1.0093828 ,\n",
      "       0.98873806, 0.999157  , 0.99278736, 0.9938584 , 0.992827  ,\n",
      "       0.99376094, 0.9935028 , 0.9926182 , 0.995615  , 1.003483  ,\n",
      "       1.0138078 , 0.9924911 , 0.9941471 , 1.0066569 , 1.0040905 ,\n",
      "       1.0065742 , 0.9971442 , 1.0083393 , 0.9905477 , 1.0027727 ,\n",
      "       0.993398  , 0.9982309 , 1.002951  , 0.99367326, 0.9926111 ,\n",
      "       1.0083387 , 0.9905125 , 1.007692  , 1.0067099 , 1.0008636 ,\n",
      "       1.0047604 , 0.99646896, 0.9903176 , 0.99337685, 1.0070095 ,\n",
      "       1.0056758 , 0.9960009 , 1.0064831 , 1.0014949 , 0.993117  ,\n",
      "       0.9916447 , 1.0038515 , 0.99336857, 0.9950343 , 0.9951328 ,\n",
      "       1.0042343 , 0.9879508 , 0.99344814, 1.0081671 , 0.99214476,\n",
      "       0.9995738 , 0.99676675, 0.99398494, 1.0054872 , 0.9936799 ,\n",
      "       1.0053961 , 0.9934033 , 0.9928819 , 1.0047688 , 0.9912107 ,\n",
      "       0.99990046, 1.0069419 , 0.9937698 , 1.0012134 , 0.9990178 ,\n",
      "       1.0086627 , 0.9942739 , 0.9946221 , 1.0021589 , 0.9943601 ,\n",
      "       1.0055726 , 0.9919477 ], dtype=float32)>, <tf.Variable 'mlm_transformer_encoder/transformer_encoder/encoder/layer_normalization/beta:0' shape=(512,) dtype=float32, numpy=\n",
      "array([ 1.05528370e-03,  1.70462835e-03, -7.29485042e-03, -3.14332894e-03,\n",
      "       -7.66467769e-03, -6.53463742e-03,  1.03171531e-03, -2.02354230e-03,\n",
      "       -8.43294710e-03, -2.31157849e-03,  5.97618660e-03, -8.39848630e-03,\n",
      "        4.37697954e-03, -3.52374907e-03, -7.52029801e-03,  2.04771815e-04,\n",
      "        4.38295165e-03,  7.83183239e-03,  2.58403685e-04,  7.67458323e-03,\n",
      "       -7.25071738e-03, -6.18044613e-03, -5.50241163e-03, -5.78650925e-03,\n",
      "       -2.74963211e-03, -3.61803570e-03, -2.63474463e-03, -2.78008822e-03,\n",
      "       -8.44326057e-03, -4.00269497e-03,  4.61014640e-03,  6.06430578e-04,\n",
      "        7.02177221e-03,  7.46751437e-03, -8.83979257e-04,  5.62455272e-03,\n",
      "        6.41276082e-03,  3.10262945e-03, -7.35132117e-03,  4.51972336e-03,\n",
      "        5.40948240e-03, -5.66652557e-03, -5.54596120e-03,  6.22905977e-03,\n",
      "       -4.67467401e-03,  7.38278311e-03, -8.55939649e-03,  6.65248046e-03,\n",
      "        6.53470773e-03,  8.20716470e-03, -5.99941751e-03, -6.51842169e-03,\n",
      "        8.38650158e-04,  7.22890720e-03,  3.24061769e-03, -2.02526245e-03,\n",
      "       -2.52255122e-03, -5.07925032e-03, -6.51123701e-03,  6.24103937e-03,\n",
      "        3.08181369e-03,  7.79165933e-03,  6.33261865e-03, -5.86129213e-03,\n",
      "        3.69153963e-03, -4.24311496e-03,  7.01755472e-03,  5.15135704e-03,\n",
      "       -7.26061780e-03,  6.67161774e-03, -7.90336449e-03,  6.65232260e-03,\n",
      "       -1.19341724e-03, -6.31764159e-03,  4.97056963e-03,  6.19467441e-03,\n",
      "       -6.66372478e-03, -8.29291064e-03, -4.86669689e-03, -7.29637034e-03,\n",
      "        4.90358658e-03, -4.35817381e-03, -7.50445155e-03,  8.21665023e-03,\n",
      "       -2.86016351e-04,  4.97505814e-03, -3.95367248e-03,  7.32937781e-03,\n",
      "        3.35604418e-05, -6.52399240e-03, -6.74542179e-03, -3.80914123e-03,\n",
      "       -1.32841713e-04, -4.48932732e-03,  7.62693526e-04,  6.63990667e-03,\n",
      "        5.20928064e-03,  1.14586595e-02,  3.79252806e-03,  7.65637448e-03,\n",
      "       -6.47116872e-03, -4.29512514e-03, -3.55925760e-03, -1.34701375e-03,\n",
      "        5.48649067e-03,  8.07661097e-03, -9.49879736e-03, -6.58052275e-03,\n",
      "       -8.60201381e-03, -6.26300555e-03,  6.10246696e-03,  7.84771703e-03,\n",
      "        2.57471413e-03,  5.75291039e-03, -5.52322669e-03,  6.47013402e-03,\n",
      "        3.10551538e-03, -3.88507429e-03, -5.80675853e-03, -4.25061118e-03,\n",
      "       -4.99733584e-03, -5.29897818e-03,  1.36999390e-03,  3.29233310e-03,\n",
      "        6.12074556e-03,  1.79890241e-03,  2.38125795e-03, -2.14571250e-03,\n",
      "       -8.64786329e-04, -5.41347498e-03, -4.50036395e-03, -6.30898261e-03,\n",
      "        5.89961093e-03,  5.77370729e-03, -5.05980477e-03,  7.12031638e-03,\n",
      "       -7.32553005e-03, -5.30386344e-03, -7.36565096e-03, -7.13508669e-03,\n",
      "        2.56837625e-03, -6.14360906e-03,  1.56725547e-03,  6.03256142e-03,\n",
      "       -6.64395280e-03,  5.44534065e-03, -8.39354843e-03,  7.22677819e-03,\n",
      "       -7.33113987e-03,  6.74306368e-03,  3.01154610e-03, -4.63948585e-03,\n",
      "       -6.47842372e-03,  3.65965953e-03, -4.21399763e-03, -5.44299604e-03,\n",
      "        5.15360944e-03,  5.29200258e-03, -7.16212066e-03, -4.82417084e-03,\n",
      "       -4.92366543e-03, -5.61355054e-03, -1.28733395e-02,  6.17308589e-03,\n",
      "       -6.10281760e-03, -8.09843536e-04, -7.49339536e-03,  2.11446523e-03,\n",
      "        5.39131882e-03,  7.16496771e-03, -6.72355713e-03,  9.34984256e-03,\n",
      "        6.72273431e-03,  7.46046938e-03,  5.69952419e-03,  7.99350534e-03,\n",
      "        4.38818708e-03, -4.96483222e-03, -1.81216968e-03,  8.62019137e-03,\n",
      "        5.51151950e-03, -4.03961109e-04, -7.41840992e-03,  6.35679532e-03,\n",
      "        1.98853109e-03,  7.29249849e-04, -5.21632656e-03, -5.05866995e-03,\n",
      "       -6.25881134e-03,  9.62249469e-03,  6.61015091e-03,  4.42807656e-03,\n",
      "       -6.97742309e-03, -6.05189661e-03, -9.46830306e-03,  6.27723569e-03,\n",
      "        1.44692126e-03, -4.75501735e-03, -1.59795175e-03, -3.11810826e-03,\n",
      "        4.58149752e-03,  7.10424874e-03,  3.77288670e-03, -4.66549024e-03,\n",
      "       -5.90200350e-03,  2.37645023e-03, -2.05607526e-03, -5.02460403e-03,\n",
      "       -7.18177063e-03,  5.36321336e-03,  6.48384495e-03,  3.78988869e-03,\n",
      "       -8.09319690e-03,  3.97276692e-03, -8.04822706e-03, -4.76854667e-03,\n",
      "       -5.40055195e-03,  7.26367300e-03,  6.21593976e-03, -5.68092708e-03,\n",
      "        8.54710874e-04,  5.91498008e-03, -7.48986332e-03, -6.94688177e-03,\n",
      "        4.81545832e-03, -4.88022203e-03, -1.28970016e-03,  8.19743704e-03,\n",
      "       -7.36625213e-03,  8.50786362e-03, -6.42941147e-03, -1.10546092e-03,\n",
      "       -5.01955952e-03,  5.12121711e-03,  4.55382233e-03, -3.92811000e-03,\n",
      "       -6.16849540e-03,  6.62950333e-03,  1.40260789e-03, -4.17540595e-03,\n",
      "       -3.67474626e-03, -3.93265858e-03,  4.97403881e-03,  5.99079393e-03,\n",
      "        5.56617370e-03, -2.82851327e-03,  3.88268614e-03,  6.42796233e-03,\n",
      "        4.06302046e-03,  4.67090122e-03, -8.14394467e-03,  1.56857364e-03,\n",
      "       -5.67546394e-03,  6.37785718e-03, -5.28921979e-03,  3.20325559e-03,\n",
      "        4.50984854e-03,  5.18220523e-03, -4.04782733e-03,  7.58677442e-03,\n",
      "        4.72550793e-03, -3.68244923e-03,  4.31456044e-03,  2.73553003e-03,\n",
      "       -5.87708503e-03, -7.11642113e-03,  5.01032081e-03, -5.36026387e-03,\n",
      "       -5.60194766e-03, -4.39830963e-03,  2.44926359e-03,  1.07753566e-02,\n",
      "        4.65247873e-03, -5.54951234e-03, -6.37434190e-03,  3.91108263e-03,\n",
      "        7.98581983e-04,  5.82538918e-03,  2.23017647e-03,  6.59768144e-03,\n",
      "       -7.75405206e-03,  2.62416131e-03, -4.21678042e-03,  5.31103555e-03,\n",
      "       -6.62453240e-03, -4.56759892e-03,  5.22703584e-03,  7.47543899e-03,\n",
      "       -1.72300602e-03,  7.06203165e-04, -7.50145316e-03, -4.60098078e-03,\n",
      "        5.15173608e-03,  6.50593964e-03, -4.50233277e-03,  5.61709655e-03,\n",
      "        5.91988442e-03,  5.98146254e-03,  1.22717151e-03, -5.59535902e-03,\n",
      "        5.71938837e-03,  5.22376457e-03,  3.99567373e-03, -5.89694455e-03,\n",
      "        9.60079301e-03, -5.36079379e-03,  8.19172070e-04, -4.59906179e-03,\n",
      "       -6.36980589e-03, -6.29242882e-03,  1.79436442e-03, -5.43138245e-03,\n",
      "        5.05003892e-03, -6.16076821e-03, -7.11970171e-03,  8.16196669e-03,\n",
      "        5.84215624e-03,  5.08854073e-03, -6.58493722e-03,  5.33493049e-03,\n",
      "        5.85153187e-03,  4.09862166e-03,  4.21541464e-03,  6.27511088e-03,\n",
      "        8.97604041e-04, -3.02354060e-03, -4.24488448e-03,  8.87426920e-03,\n",
      "        3.23149469e-03, -3.97682190e-03,  6.62603602e-03, -5.88716473e-03,\n",
      "        6.32149819e-03,  9.92903393e-03, -8.53941683e-03, -5.98324230e-03,\n",
      "       -6.68053981e-03,  3.29204206e-03,  6.58100704e-03,  7.15417555e-03,\n",
      "       -4.42051329e-03, -5.42473281e-03,  3.51737835e-03,  8.82129837e-03,\n",
      "        4.68714163e-03,  6.74623810e-03,  4.34019556e-03, -1.07281664e-02,\n",
      "        3.28931841e-04,  7.08021829e-03,  7.26437103e-03,  8.11839290e-03,\n",
      "        7.07719661e-03, -2.61435169e-03, -7.88090867e-04,  6.17979700e-03,\n",
      "        3.36865569e-03,  3.46469111e-03,  2.23853090e-03, -7.77054951e-03,\n",
      "       -6.14779582e-03, -3.14547005e-03,  7.21642189e-03, -3.99029022e-03,\n",
      "       -3.79530364e-03,  6.19450025e-03, -5.11664606e-04, -6.94261724e-03,\n",
      "       -3.94286914e-03, -5.41438116e-03, -6.19446393e-03, -7.58718560e-03,\n",
      "       -5.08911861e-03,  8.65266193e-03, -8.10914394e-03,  7.31220888e-03,\n",
      "        4.78500733e-03,  8.58401600e-03, -5.43455780e-03,  6.49522198e-03,\n",
      "       -5.65540791e-03, -4.51450190e-03,  5.83225163e-03, -4.45340527e-03,\n",
      "       -6.36782963e-03, -6.09275466e-03,  2.23343982e-03, -5.03826141e-03,\n",
      "       -5.94165968e-03, -3.81818134e-03, -6.74904650e-03, -3.18378862e-03,\n",
      "       -5.44552878e-03, -5.23715839e-03,  2.95799342e-03, -2.40935781e-03,\n",
      "        4.75554634e-03, -4.78688255e-03,  2.50804937e-03,  7.00581167e-03,\n",
      "       -2.44281907e-03, -3.85026797e-03, -7.47159636e-03,  6.17921259e-03,\n",
      "        5.99373039e-03, -6.07258128e-03,  5.21834427e-03, -1.32550893e-03,\n",
      "       -6.39416324e-03,  4.91112424e-03, -4.79015987e-03, -5.29185496e-03,\n",
      "       -6.22033421e-03,  7.40512181e-03, -7.79065024e-03,  6.84530521e-03,\n",
      "        2.99725123e-03, -5.27297566e-03,  2.52130115e-03,  5.98392356e-03,\n",
      "       -8.39504786e-03, -3.84245673e-03, -5.10054640e-03, -5.26408013e-03,\n",
      "        3.82707850e-03,  4.55508428e-03, -3.21042433e-04, -5.43778937e-04,\n",
      "        6.54325169e-03,  6.63840864e-03,  5.47219953e-03, -3.22178286e-03,\n",
      "       -4.52387752e-03,  6.70490786e-03,  6.51750294e-03, -3.84219876e-03,\n",
      "        5.90289535e-04, -3.07388301e-03,  4.81199613e-03,  7.86198024e-03,\n",
      "       -4.57422528e-03,  4.92205005e-03,  1.48084480e-03, -4.01698693e-04,\n",
      "        4.26724413e-03, -6.56181946e-03,  3.59409209e-03, -4.09004977e-03,\n",
      "        6.21194532e-03,  6.31844392e-03, -1.29542090e-02, -6.60042278e-03,\n",
      "        2.05703382e-03,  7.67551828e-03, -4.55048820e-03,  7.66678993e-03,\n",
      "        5.82985254e-03, -8.71305540e-03,  1.69548590e-03,  5.40294684e-03,\n",
      "        5.25524886e-03, -5.32137509e-03, -6.65040128e-03,  4.56703641e-03,\n",
      "        3.10635148e-03,  8.04952160e-03,  2.30678730e-03,  7.48899346e-03,\n",
      "       -6.77781412e-03,  5.50374296e-03,  5.88737661e-03, -1.88111770e-03,\n",
      "        3.12036439e-03, -6.01198850e-03, -7.59723876e-03, -7.03570293e-03,\n",
      "        4.32466948e-03,  6.44481694e-03,  6.46511698e-03, -3.82401678e-03,\n",
      "       -6.56190189e-03,  6.58239564e-03,  4.54984931e-03,  2.99497554e-03,\n",
      "        1.93627481e-03,  6.38935482e-03, -1.12213613e-03, -4.80001234e-03,\n",
      "        7.14734988e-03,  3.80192860e-03,  6.44407747e-03,  4.74951509e-03,\n",
      "       -5.38283912e-03,  6.25924626e-03,  2.00777291e-03,  5.89740882e-03,\n",
      "        4.39585187e-03, -2.16654246e-03, -6.47292193e-03,  1.12299609e-03,\n",
      "        6.01539575e-03, -6.80276519e-03,  4.77494625e-03, -5.45425480e-03,\n",
      "       -4.69443854e-03,  7.11835548e-03,  5.30415680e-03,  2.92682485e-03,\n",
      "       -5.99710084e-03, -5.05020982e-03, -6.12336257e-03,  2.43048789e-03],\n",
      "      dtype=float32)>, <tf.Variable 'mlm_transformer_encoder/transformer_encoder/encoder/layer_normalization_1/gamma:0' shape=(512,) dtype=float32, numpy=\n",
      "array([1.0186533 , 0.9957942 , 1.0194257 , 1.0161619 , 1.0145017 ,\n",
      "       1.0155493 , 1.0154272 , 1.0097792 , 1.0200082 , 0.9974215 ,\n",
      "       1.0197196 , 1.0192974 , 1.0052862 , 1.0046371 , 1.0193663 ,\n",
      "       0.99597794, 1.0053642 , 1.0194696 , 1.0085065 , 1.0196879 ,\n",
      "       1.0197393 , 1.0080656 , 1.0135515 , 1.0172509 , 1.0194863 ,\n",
      "       1.010544  , 1.0013556 , 1.0140322 , 1.0168073 , 1.0161376 ,\n",
      "       1.0193784 , 1.0196975 , 1.0191622 , 1.0192226 , 0.9994688 ,\n",
      "       1.0132041 , 1.0159229 , 1.0167594 , 1.0198478 , 0.99442184,\n",
      "       1.0194309 , 1.0147799 , 1.0149255 , 1.0196055 , 1.0127318 ,\n",
      "       1.0194666 , 1.0186414 , 1.0194894 , 1.0194817 , 1.0184917 ,\n",
      "       1.0189986 , 1.0173504 , 1.020027  , 1.0190761 , 1.019214  ,\n",
      "       0.99446404, 1.0183566 , 1.0189947 , 1.0179586 , 1.0195984 ,\n",
      "       1.0042818 , 1.0196731 , 1.0196141 , 1.0168837 , 1.0191706 ,\n",
      "       1.0116417 , 1.0189582 , 1.0195599 , 1.0195167 , 1.0194899 ,\n",
      "       1.019728  , 1.0196334 , 1.0188861 , 1.0194314 , 1.0122788 ,\n",
      "       1.0172526 , 1.0194154 , 1.0168599 , 0.9999601 , 1.0181363 ,\n",
      "       1.0073204 , 1.0107305 , 1.019799  , 1.0199035 , 1.0196817 ,\n",
      "       1.0194737 , 1.0182604 , 1.0196376 , 0.9947607 , 1.0160322 ,\n",
      "       1.0194162 , 1.0170901 , 1.0135005 , 1.0194925 , 1.014751  ,\n",
      "       1.0197921 , 1.0124815 , 1.0135919 , 1.0193652 , 1.0197816 ,\n",
      "       1.0194508 , 0.99998176, 1.0193201 , 1.0188204 , 1.0194422 ,\n",
      "       1.0198178 , 1.0194595 , 1.0142585 , 1.019234  , 1.0145335 ,\n",
      "       1.0195353 , 1.0199238 , 1.0073837 , 1.0188615 , 1.0154299 ,\n",
      "       1.0190231 , 0.9945124 , 0.99490696, 1.0193278 , 1.0158197 ,\n",
      "       1.0160054 , 1.0131828 , 0.9938309 , 0.99524134, 1.014793  ,\n",
      "       1.0067239 , 1.0033777 , 1.0060523 , 1.0194287 , 1.0163242 ,\n",
      "       1.0190874 , 1.0160052 , 1.0161048 , 1.0165577 , 1.0115502 ,\n",
      "       1.0197635 , 1.0195074 , 1.0191171 , 1.0197763 , 1.018992  ,\n",
      "       1.002285  , 1.0115956 , 1.0005049 , 1.0198138 , 1.0196083 ,\n",
      "       1.0162009 , 1.0134572 , 1.019511  , 1.0199373 , 1.0169343 ,\n",
      "       1.0017494 , 1.0167695 , 1.0077673 , 0.9941483 , 1.0191177 ,\n",
      "       1.0159749 , 1.0126437 , 1.0161608 , 1.0197237 , 1.0140362 ,\n",
      "       1.0014673 , 1.0167568 , 1.0147896 , 1.0199271 , 1.0084018 ,\n",
      "       1.0107327 , 1.0194092 , 1.0188887 , 1.0185611 , 1.0195624 ,\n",
      "       1.0196767 , 1.0124527 , 1.0156922 , 1.0198916 , 1.0177588 ,\n",
      "       1.0200428 , 1.0135831 , 0.9981018 , 1.0158746 , 1.0197448 ,\n",
      "       1.0155644 , 1.0024989 , 1.0195454 , 1.0195752 , 1.006397  ,\n",
      "       1.0036191 , 1.0137278 , 1.0180115 , 1.0185542 , 1.015214  ,\n",
      "       1.0141084 , 1.0191852 , 1.0193326 , 1.0167361 , 1.0194381 ,\n",
      "       1.0193926 , 1.0144571 , 1.0145894 , 1.0194082 , 1.0130152 ,\n",
      "       1.0167035 , 1.0168775 , 1.0155386 , 1.0072334 , 1.016902  ,\n",
      "       1.0197285 , 0.99303126, 1.0196234 , 1.0193872 , 1.0136536 ,\n",
      "       1.0196317 , 1.0000827 , 1.0195401 , 1.0124133 , 1.0196444 ,\n",
      "       1.0199109 , 1.0160807 , 1.019432  , 1.0195162 , 1.0037204 ,\n",
      "       1.019434  , 1.0192207 , 1.0199126 , 1.0192417 , 1.0196445 ,\n",
      "       1.019256  , 1.0113742 , 1.0196247 , 1.0196123 , 1.0194725 ,\n",
      "       1.0149752 , 1.0154572 , 0.99003446, 1.0197338 , 1.0101969 ,\n",
      "       0.9987447 , 1.0201222 , 1.0172142 , 1.0111309 , 1.0020188 ,\n",
      "       0.9946035 , 0.99641156, 1.0196358 , 1.0166355 , 1.0128026 ,\n",
      "       1.0046349 , 1.0130849 , 1.0196686 , 1.0194292 , 1.0043906 ,\n",
      "       1.01976   , 1.019405  , 1.0137988 , 1.0193537 , 1.0145818 ,\n",
      "       1.0183253 , 1.0187695 , 1.0135247 , 1.0080917 , 1.019737  ,\n",
      "       1.014582  , 1.018628  , 1.0065272 , 1.0005316 , 1.0192577 ,\n",
      "       1.0197395 , 1.0122019 , 1.0192019 , 1.0196483 , 1.0157202 ,\n",
      "       1.0109936 , 1.0199282 , 1.0187104 , 1.006397  , 1.0168154 ,\n",
      "       1.0124938 , 0.9969894 , 1.0060133 , 1.0197593 , 1.0165284 ,\n",
      "       1.0198054 , 1.0151094 , 1.0133555 , 1.017191  , 1.0161866 ,\n",
      "       1.0135959 , 1.0138245 , 1.0194522 , 0.9944977 , 1.0171558 ,\n",
      "       1.0196204 , 1.0129919 , 1.0086704 , 1.0199529 , 0.9996347 ,\n",
      "       1.0190446 , 1.0192761 , 1.0134945 , 1.0067906 , 1.0155103 ,\n",
      "       1.0106254 , 1.017475  , 0.9961892 , 1.0194808 , 0.99687034,\n",
      "       1.0200536 , 0.9966309 , 1.0123746 , 1.0199252 , 1.0194789 ,\n",
      "       1.019717  , 1.0195351 , 1.0190122 , 1.0193211 , 1.0199729 ,\n",
      "       1.0199137 , 1.0129691 , 1.0136058 , 1.019256  , 1.0192665 ,\n",
      "       1.0192163 , 1.0115104 , 1.0067269 , 1.0191295 , 1.0201544 ,\n",
      "       0.99537396, 1.0143089 , 1.0195087 , 1.0186198 , 0.99978834,\n",
      "       1.0193646 , 1.0191365 , 1.0198809 , 1.0189764 , 1.019633  ,\n",
      "       1.01436   , 1.0201579 , 1.0190068 , 1.0197643 , 1.0195173 ,\n",
      "       1.0070674 , 1.0194857 , 1.0178493 , 1.0197093 , 1.0093883 ,\n",
      "       1.0193746 , 1.0056596 , 1.0193903 , 1.0191404 , 1.0087373 ,\n",
      "       1.019733  , 1.0200632 , 1.0157503 , 0.99573845, 1.0196722 ,\n",
      "       1.0193999 , 1.0124096 , 1.0139877 , 1.0079055 , 1.0190431 ,\n",
      "       1.0193217 , 1.0067381 , 1.0193598 , 0.9961996 , 1.018913  ,\n",
      "       1.0150595 , 1.014698  , 1.0195802 , 1.0173591 , 1.0196097 ,\n",
      "       1.016733  , 1.0192806 , 1.0171502 , 1.0199887 , 1.0194204 ,\n",
      "       1.0194687 , 1.014714  , 1.0197293 , 1.0196989 , 1.0194452 ,\n",
      "       1.0161113 , 1.0112875 , 1.019022  , 1.0163009 , 1.019762  ,\n",
      "       1.0195241 , 1.0195589 , 1.0161625 , 1.0192748 , 1.0041789 ,\n",
      "       1.0196189 , 1.0016701 , 0.9969736 , 1.0158113 , 1.0091603 ,\n",
      "       1.0044761 , 1.0025834 , 1.0190777 , 1.0049683 , 1.0010397 ,\n",
      "       1.0104755 , 1.0147504 , 1.0198305 , 1.0198628 , 1.0117834 ,\n",
      "       1.0116034 , 1.0191615 , 1.0070121 , 1.0198272 , 1.0156492 ,\n",
      "       1.0195655 , 1.0193546 , 1.0194516 , 1.0195956 , 1.0195999 ,\n",
      "       1.0195816 , 1.0123625 , 1.0055064 , 0.9949058 , 1.0157081 ,\n",
      "       1.0171778 , 1.0071021 , 1.0109935 , 1.0191553 , 1.0191938 ,\n",
      "       1.0175494 , 1.0128168 , 0.99485606, 1.0198283 , 1.0188265 ,\n",
      "       1.0190533 , 1.019139  , 1.0133947 , 1.0194718 , 1.0195829 ,\n",
      "       1.0142093 , 1.0196928 , 1.0190552 , 1.0151129 , 1.019556  ,\n",
      "       1.0189035 , 1.0143679 , 0.9966893 , 1.0040073 , 1.0026858 ,\n",
      "       1.01947   , 1.0121334 , 1.0142256 , 1.0139018 , 1.0195222 ,\n",
      "       1.0203012 , 1.0195342 , 1.0178981 , 1.0196587 , 0.99673164,\n",
      "       1.019451  , 1.0169103 , 1.0193496 , 1.0062511 , 1.0196507 ,\n",
      "       1.0110284 , 1.019014  , 1.0191661 , 1.0148486 , 1.0020826 ,\n",
      "       1.0199885 , 1.019746  , 1.0197668 , 1.0194447 , 1.0167804 ,\n",
      "       1.0192593 , 1.0069485 , 1.0167557 , 1.0188828 , 1.019213  ,\n",
      "       1.0191162 , 1.0191602 , 1.019549  , 1.0172659 , 1.0107918 ,\n",
      "       1.0182811 , 1.0143158 , 1.0088247 , 1.0149795 , 1.0199144 ,\n",
      "       1.0192536 , 1.0026302 , 1.0192077 , 1.0172861 , 1.0152826 ,\n",
      "       1.0190866 , 1.0197077 , 1.0160284 , 1.0194077 , 1.0108142 ,\n",
      "       1.0193019 , 1.0021185 , 0.99587643, 1.0141059 , 1.0121486 ,\n",
      "       1.019654  , 1.0192528 , 1.0101961 , 1.0195694 , 0.9936925 ,\n",
      "       1.016304  , 1.0094205 , 1.0073358 , 1.0181365 , 1.0110228 ,\n",
      "       1.0195987 , 1.0193044 ], dtype=float32)>, <tf.Variable 'mlm_transformer_encoder/transformer_encoder/encoder/layer_normalization_1/beta:0' shape=(512,) dtype=float32, numpy=\n",
      "array([-0.01999658,  0.00502408, -0.01978791, -0.02008572, -0.01995096,\n",
      "       -0.0202027 , -0.01897663, -0.02006614, -0.0199299 , -0.01778167,\n",
      "        0.01996912, -0.01988494,  0.0192065 , -0.01916217, -0.01991621,\n",
      "       -0.01653178,  0.01940988,  0.01993624,  0.01993   ,  0.01985894,\n",
      "       -0.01978691, -0.01953794, -0.02007104, -0.01999941,  0.01979718,\n",
      "       -0.01962462,  0.01858439,  0.01967101, -0.01999487, -0.01994795,\n",
      "       -0.01994474,  0.01994636,  0.01999158,  0.01981054,  0.01859521,\n",
      "        0.01980825,  0.01714632,  0.01984982, -0.01987665,  0.00216061,\n",
      "        0.01975208, -0.01973831, -0.02001902,  0.01979888, -0.01977338,\n",
      "        0.01991729, -0.01981524,  0.01981701,  0.01982951,  0.01986486,\n",
      "       -0.01978248, -0.02001328,  0.01984714,  0.01999869, -0.02002862,\n",
      "       -0.00726261, -0.01998611, -0.0198187 , -0.01926152,  0.01988651,\n",
      "        0.01904521,  0.01982503,  0.01997507, -0.02008044, -0.0195783 ,\n",
      "       -0.01971075,  0.02003988,  0.02003674, -0.01995381,  0.01988685,\n",
      "       -0.01990563,  0.01992852, -0.01910095, -0.01997076,  0.01967265,\n",
      "        0.02003916, -0.01982708, -0.01735735, -0.00760487, -0.01978854,\n",
      "        0.01931389, -0.01959107, -0.01989655,  0.01991555, -0.0198735 ,\n",
      "        0.02000532, -0.01954698,  0.0198156 ,  0.01276703, -0.01990925,\n",
      "       -0.01994127, -0.02003353,  0.02015955, -0.0200154 , -0.02007946,\n",
      "        0.01981512,  0.01993981,  0.01685235,  0.01992333,  0.01989232,\n",
      "       -0.0199271 , -0.01848141, -0.01990333, -0.01985362,  0.01990768,\n",
      "        0.01997245, -0.01994463, -0.01990786, -0.01975992, -0.01635302,\n",
      "        0.0199861 ,  0.01997009,  0.01958031,  0.01982415, -0.01987343,\n",
      "        0.02007551,  0.00737162, -0.00388911, -0.01979975, -0.01991943,\n",
      "       -0.01990904, -0.01967996,  0.00202267,  0.01356807,  0.01976432,\n",
      "       -0.01640913,  0.01934988, -0.01943881, -0.01987631, -0.01992878,\n",
      "       -0.02007073, -0.0198376 ,  0.02012668,  0.01989256, -0.0195592 ,\n",
      "        0.01993153, -0.01978551, -0.01991351, -0.01988783, -0.01984074,\n",
      "       -0.01404698, -0.01958168, -0.0116191 ,  0.01979266, -0.01979033,\n",
      "        0.02012436, -0.01533095,  0.01983285, -0.0198804 ,  0.0198476 ,\n",
      "        0.01871731, -0.02007181, -0.01932829, -0.01109752, -0.01994998,\n",
      "       -0.01985361,  0.01977403,  0.02003971, -0.01977198, -0.02020921,\n",
      "       -0.01883174, -0.01978944, -0.01631644,  0.01993671, -0.01599086,\n",
      "       -0.01996808, -0.01986201,  0.01992314,  0.01976338,  0.019908  ,\n",
      "       -0.01978636,  0.01543856,  0.01980718,  0.01987207,  0.02009303,\n",
      "        0.01995248,  0.01972163,  0.01172627, -0.02022526,  0.01977687,\n",
      "        0.02013085, -0.01892596, -0.01988876,  0.0198894 ,  0.01951477,\n",
      "        0.01872391, -0.01987414, -0.01980625, -0.01999688,  0.01665085,\n",
      "        0.01979879,  0.0200767 , -0.01981804, -0.01730413, -0.01981176,\n",
      "        0.01981899, -0.01749282, -0.01986392, -0.01993847,  0.01688304,\n",
      "        0.0201117 ,  0.02004157,  0.02008926, -0.01954039, -0.01989621,\n",
      "       -0.01983757,  0.00537501, -0.01988327, -0.01980563,  0.01970271,\n",
      "        0.01997267,  0.00958708, -0.01993783,  0.01441063, -0.01996996,\n",
      "       -0.01990302, -0.01913119,  0.01978606,  0.01997681, -0.01922949,\n",
      "        0.01995696,  0.02000653, -0.01996387, -0.01998323,  0.01999878,\n",
      "       -0.01981734, -0.016147  ,  0.01993762, -0.01988948,  0.01976217,\n",
      "       -0.02005842,  0.02016042,  0.00176436,  0.01999417,  0.01961823,\n",
      "       -0.01815979, -0.01994522,  0.01994774, -0.01548265, -0.01876759,\n",
      "        0.01539493, -0.01629332,  0.01972359,  0.01988113,  0.01692067,\n",
      "       -0.01934856, -0.01627116,  0.01989432, -0.01993717, -0.01737238,\n",
      "       -0.01986578,  0.01998249, -0.01978131,  0.01990969, -0.01981817,\n",
      "       -0.01967997,  0.01995039,  0.02007466,  0.01323684,  0.01990232,\n",
      "        0.01982885, -0.02002566,  0.01403426,  0.01034466, -0.01994877,\n",
      "       -0.01990057,  0.01963818, -0.0200474 , -0.01997234, -0.01985949,\n",
      "        0.01995368,  0.01994841,  0.01983998, -0.01937557, -0.01986664,\n",
      "        0.020128  ,  0.01632486,  0.0194236 ,  0.01964215,  0.02013941,\n",
      "       -0.01991073, -0.01995654, -0.01988745,  0.01991146, -0.02015078,\n",
      "       -0.01988971,  0.01984362,  0.01985375,  0.01687182, -0.02003499,\n",
      "       -0.0199009 , -0.01972418, -0.01255338,  0.01994955, -0.01074829,\n",
      "        0.01990203,  0.0199768 ,  0.01992736, -0.01325489, -0.02011463,\n",
      "        0.01963995,  0.01987915,  0.01355011, -0.01988301, -0.01689765,\n",
      "       -0.01988274, -0.01173759, -0.02009355, -0.01987946, -0.01983412,\n",
      "       -0.01985745, -0.02000624,  0.0198025 , -0.01988062, -0.01995722,\n",
      "        0.01985656,  0.01998693,  0.02011806, -0.01986003,  0.02003546,\n",
      "        0.01979523,  0.02015494,  0.01508081,  0.01987006, -0.01989432,\n",
      "        0.0007077 , -0.01972753,  0.01989216,  0.01984795, -0.01837462,\n",
      "        0.01975993, -0.01983383,  0.0199022 ,  0.01961774, -0.0199258 ,\n",
      "       -0.01702468, -0.01995269,  0.01997066,  0.01990644,  0.0198648 ,\n",
      "       -0.01564473, -0.01984482, -0.02002237,  0.01998516,  0.01952162,\n",
      "        0.01971803,  0.01909329, -0.01974459, -0.02000363,  0.01538826,\n",
      "        0.01990188,  0.01991963,  0.02000361, -0.00481063,  0.01998692,\n",
      "        0.01990022,  0.01945754, -0.02021977,  0.01948223, -0.01999055,\n",
      "       -0.01978541, -0.01933137,  0.01978877, -0.01631302, -0.0193845 ,\n",
      "        0.01987388, -0.01997657, -0.01984131, -0.0174258 , -0.01994854,\n",
      "       -0.01981781, -0.01980272, -0.01991   ,  0.01992123, -0.01980593,\n",
      "        0.0199081 ,  0.01999572,  0.01974503, -0.01995547,  0.01992473,\n",
      "       -0.01980646, -0.01989299,  0.01984261, -0.01997193, -0.01997398,\n",
      "       -0.01986426, -0.02000867, -0.02000145, -0.01975483, -0.01927539,\n",
      "       -0.01977022,  0.00974551,  0.0175259 , -0.02000985,  0.01965392,\n",
      "        0.01312752,  0.01884436, -0.01994853,  0.01924893,  0.00902283,\n",
      "       -0.0139601 , -0.02010668, -0.0199267 ,  0.01988767,  0.02023064,\n",
      "       -0.01979483,  0.01990478, -0.01948673, -0.01995104,  0.01999024,\n",
      "       -0.02000777, -0.0198378 , -0.02000839,  0.01989098, -0.01999719,\n",
      "        0.01997332,  0.02010488, -0.01913914, -0.00292496,  0.01713911,\n",
      "       -0.01781486, -0.01935205, -0.0196749 , -0.01999346,  0.01999585,\n",
      "        0.01958287, -0.01741069,  0.01424911,  0.02001725,  0.020016  ,\n",
      "        0.01997414,  0.01985435, -0.0201083 ,  0.0199084 ,  0.01988699,\n",
      "       -0.02028541,  0.01986584,  0.01991178,  0.01991559,  0.02001254,\n",
      "       -0.02007397,  0.02023578,  0.01688368, -0.01935512,  0.01904985,\n",
      "       -0.01996135,  0.0197288 , -0.02008116,  0.01980128,  0.01989825,\n",
      "       -0.01989396, -0.01998073,  0.01924124,  0.01996114,  0.01207149,\n",
      "        0.01982887,  0.02008748, -0.01990738,  0.01924393,  0.02000955,\n",
      "        0.01969615, -0.01984275, -0.02008759,  0.02022696,  0.01907317,\n",
      "        0.01993527,  0.01992363,  0.01991509, -0.01985692,  0.02004557,\n",
      "        0.01977478,  0.01305451,  0.02011611, -0.01975749, -0.01981855,\n",
      "       -0.01979037,  0.01951288,  0.01976914,  0.01989487, -0.01958068,\n",
      "       -0.02005083,  0.02007107,  0.01941076,  0.02000204, -0.01995709,\n",
      "        0.01977747,  0.01901528, -0.02000045,  0.01988338,  0.02014451,\n",
      "        0.01976267,  0.01980374, -0.0201267 ,  0.01976703,  0.0199694 ,\n",
      "        0.02000431,  0.01885982,  0.00638376, -0.0198015 ,  0.0155612 ,\n",
      "        0.01992129, -0.0197518 ,  0.01977505, -0.01996583,  0.00400301,\n",
      "        0.01994607,  0.0197188 , -0.01211878, -0.01982391, -0.02008692,\n",
      "       -0.01999751,  0.01996943], dtype=float32)>, <tf.Variable 'kernel:0' shape=(512, 40000) dtype=float32, numpy=\n",
      "array([[ 0.02085119,  0.0088875 ,  0.02037507, ...,  0.02165172,\n",
      "         0.0091726 ,  0.00418121],\n",
      "       [-0.01453865, -0.01430875, -0.00180248, ..., -0.00560504,\n",
      "         0.00661631, -0.01011966],\n",
      "       [ 0.01089106,  0.01521688,  0.01875724, ...,  0.00534432,\n",
      "         0.02054154,  0.02614534],\n",
      "       ...,\n",
      "       [ 0.00529003,  0.00764977,  0.01379395, ...,  0.00891231,\n",
      "         0.01513236,  0.01526806],\n",
      "       [ 0.01090059,  0.00513094,  0.02296159, ...,  0.01077338,\n",
      "         0.0226437 ,  0.00532803],\n",
      "       [-0.00973539, -0.01168333, -0.02341943, ..., -0.02167157,\n",
      "        -0.00881811, -0.00894706]], dtype=float32)>, <tf.Variable 'bias:0' shape=(40000,) dtype=float32, numpy=\n",
      "array([-0.01505769, -0.01524097, -0.01505816, ..., -0.01482015,\n",
      "       -0.01522224, -0.01518394], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(model_trainable_variables[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gradients_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = tf.Tensor(2.6558914, shape=(), dtype=float32), Predicted Token = 34672, True Token = [[183]]\n",
      "Epoch 1, Loss = tf.Tensor(1.8565924, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 2, Loss = tf.Tensor(1.6944456, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 3, Loss = tf.Tensor(1.4963288, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 4, Loss = tf.Tensor(1.3135195, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 5, Loss = tf.Tensor(1.1246877, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 6, Loss = tf.Tensor(0.9316613, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 7, Loss = tf.Tensor(0.73952705, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 8, Loss = tf.Tensor(0.55323195, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 9, Loss = tf.Tensor(0.3811002, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 10, Loss = tf.Tensor(0.23640299, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 11, Loss = tf.Tensor(0.13174462, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 12, Loss = tf.Tensor(0.06811293, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 13, Loss = tf.Tensor(0.034353714, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 14, Loss = tf.Tensor(0.017595932, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 15, Loss = tf.Tensor(0.009350679, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 16, Loss = tf.Tensor(0.0051987995, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 17, Loss = tf.Tensor(0.0030285132, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 18, Loss = tf.Tensor(0.001845434, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 19, Loss = tf.Tensor(0.0011733546, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 20, Loss = tf.Tensor(0.00077501737, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 21, Loss = tf.Tensor(0.0005318364, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n",
      "Epoch 22, Loss = tf.Tensor(0.0003772839, shape=(), dtype=float32), Predicted Token = 183, True Token = [[183]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\test_encoders\\transformer_encoder\\convergence_test.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/transformer_encoder/convergence_test.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/transformer_encoder/convergence_test.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/transformer_encoder/convergence_test.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     loss, predicted_token, token_ids \u001b[39m=\u001b[39m train_step(inputs, labels)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/transformer_encoder/convergence_test.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Log or print the loss for monitoring\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/transformer_encoder/convergence_test.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(epoch) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, Loss = \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(loss) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, Predicted Token = \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(predicted_token) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, True Token = \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(token_ids))\n",
      "\u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\test_encoders\\transformer_encoder\\convergence_test.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/transformer_encoder/convergence_test.ipynb#W5sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m gradients_test\u001b[39m.\u001b[39mappend(gradients)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/transformer_encoder/convergence_test.ipynb#W5sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m model_trainable_variables\u001b[39m.\u001b[39mappend(model\u001b[39m.\u001b[39mtrainable_variables)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/transformer_encoder/convergence_test.ipynb#W5sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mapply_gradients(\u001b[39mzip\u001b[39;49m(gradients, model\u001b[39m.\u001b[39;49mtrainable_variables))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/transformer_encoder/convergence_test.ipynb#W5sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss, predicted_token, token_ids\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1223\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_gradients_aggregation \u001b[39mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m   1222\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[1;32m-> 1223\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:652\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[0;32m    651\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, trainable_variables))\n\u001b[1;32m--> 652\u001b[0m iteration \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_apply_gradients(grads_and_vars)\n\u001b[0;32m    654\u001b[0m \u001b[39m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[39mfor\u001b[39;00m variable \u001b[39min\u001b[39;00m trainable_variables:\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1253\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mesh \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_with_dtensor:\n\u001b[0;32m   1250\u001b[0m     \u001b[39m# Skip any usage of strategy logic for DTensor\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[1;32m-> 1253\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49m__internal__\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49minterim\u001b[39m.\u001b[39;49mmaybe_merge_call(\n\u001b[0;32m   1254\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distributed_apply_gradients_fn,\n\u001b[0;32m   1255\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distribution_strategy,\n\u001b[0;32m   1256\u001b[0m     grads_and_vars,\n\u001b[0;32m   1257\u001b[0m )\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[1;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[39mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39m  The return value of the `fn` call.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[1;32m---> 51\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(strategy, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   \u001b[39mreturn\u001b[39;00m distribute_lib\u001b[39m.\u001b[39mget_replica_context()\u001b[39m.\u001b[39mmerge_call(\n\u001b[0;32m     54\u001b[0m       fn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1345\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[1;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[0;32m   1342\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step(grad, var)\n\u001b[0;32m   1344\u001b[0m \u001b[39mfor\u001b[39;00m grad, var \u001b[39min\u001b[39;00m grads_and_vars:\n\u001b[1;32m-> 1345\u001b[0m     distribution\u001b[39m.\u001b[39;49mextended\u001b[39m.\u001b[39;49mupdate(\n\u001b[0;32m   1346\u001b[0m         var, apply_grad_to_update_var, args\u001b[39m=\u001b[39;49m(grad,), group\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1349\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_ema:\n\u001b[0;32m   1350\u001b[0m     _, var_list \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3011\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   3008\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[0;32m   3009\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   3010\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[1;32m-> 3011\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update(var, fn, args, kwargs, group)\n\u001b[0;32m   3012\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replica_ctx_update(\n\u001b[0;32m   3014\u001b[0m       var, fn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs, group\u001b[39m=\u001b[39mgroup)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4081\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   4078\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update\u001b[39m(\u001b[39mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[0;32m   4079\u001b[0m   \u001b[39m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[0;32m   4080\u001b[0m   \u001b[39m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[1;32m-> 4081\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_non_slot(var, fn, (var,) \u001b[39m+\u001b[39;49m \u001b[39mtuple\u001b[39;49m(args), kwargs, group)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4087\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   4083\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_non_slot\u001b[39m(\u001b[39mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[0;32m   4084\u001b[0m   \u001b[39m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[0;32m   4085\u001b[0m   \u001b[39m# once that value is used for something.\u001b[39;00m\n\u001b[0;32m   4086\u001b[0m   \u001b[39mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[1;32m-> 4087\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   4088\u001b[0m     \u001b[39mif\u001b[39;00m should_group:\n\u001b[0;32m   4089\u001b[0m       \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    595\u001b[0m   \u001b[39mwith\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mControlStatusCtx(status\u001b[39m=\u001b[39mag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mUNSPECIFIED):\n\u001b[1;32m--> 596\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1342\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m   1340\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step_xla(grad, var, \u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_key(var)))\n\u001b[0;32m   1341\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1342\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_step(grad, var)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:241\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_key(variable) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_dict:\n\u001b[0;32m    233\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    234\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe optimizer cannot recognize variable \u001b[39m\u001b[39m{\u001b[39;00mvariable\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`tf.keras.optimizers.legacy.\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[1;32m--> 241\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_step(gradient, variable)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\adam.py:199\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[39m# Dense gradients.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     m\u001b[39m.\u001b[39massign_add((gradient \u001b[39m-\u001b[39m m) \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_1))\n\u001b[1;32m--> 199\u001b[0m     v\u001b[39m.\u001b[39massign_add((tf\u001b[39m.\u001b[39;49msquare(gradient) \u001b[39m-\u001b[39;49m v) \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_2))\n\u001b[0;32m    200\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mamsgrad:\n\u001b[0;32m    201\u001b[0m         v_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_velocity_hats[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_dict[var_key]]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example of usage in the training loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    loss, predicted_token, token_ids = train_step(inputs, labels)\n",
    "    # Log or print the loss for monitoring\n",
    "    print('Epoch ' + str(epoch) + ', Loss = ' + str(loss) + ', Predicted Token = ' + str(predicted_token) + ', True Token = ' + str(token_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
