{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from transformer_encoder import MLMTransformerEncoder\n",
    "from mlm_dataset.mlm_dataset_generator import MLMDatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example with original Transformer hyperparameters\n",
    "num_layers = 1\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "input_vocab_size = 40000\n",
    "maximum_position_encoding = 10000\n",
    "\n",
    "model = MLMTransformerEncoder(num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding)\n",
    "dummy_input = [tf.keras.Input(shape=(None,)), tf.keras.Input(shape=(None,)), tf.keras.Input(shape=(None,))]\n",
    "model(dummy_input, build=True)\n",
    "\n",
    "# Define an optimizer (e.g., Adam)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Define a loss function (e.g., categorical cross-entropy for classification)\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM dataset for training\n",
    "mlm_dataset_generator = MLMDatasetGenerator('../../dataset/resume_dataset.csv')\n",
    "\n",
    "oov_token = '[oov]'\n",
    "\n",
    "# Initialize a Tokenizer and fit on text data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token, filters='')\n",
    "# Traing tokenizer on dataset vocab\n",
    "tokenizer.fit_on_texts(mlm_dataset_generator.getVocubulary())\n",
    "\n",
    "# padding function\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "batch_size = 64\n",
    "# default is None\n",
    "sample_limit = None\n",
    "\n",
    "mlm_dataset = [mlm_dataset_generator.generateMLMDataset(batch_size, sample_limit=sample_limit)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many words are in the dataset (currently: 37032)\n",
    "# print(tokenizer.word_index['[mask]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM dataset checker\n",
    "# inputs, mask, labels = mlm_dataset[0]\n",
    "\n",
    "# print(inputs)\n",
    "# print(mask)\n",
    "\n",
    "# for sequence_index, sequence in enumerate(labels):\n",
    "#     for token_index, token in enumerate(sequence):\n",
    "#         for value in token:\n",
    "#             if (value > 0):\n",
    "#                 print(sequence_index, token_index, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch: tuple, input_vocab_size) -> ((tf.constant([], tf.float32), tf.constant([], tf.float32)), tf.constant([], tf.float32)):\n",
    "    tokens_batch, labels_batch = batch\n",
    "\n",
    "    # tokenize the tokens_batch and labels_batch, string to token_id conversion\n",
    "    tokenized_tokens_batch = tokenizer.texts_to_sequences(tokens_batch)\n",
    "    tokenized_labels_batch = np.array(tokenizer.texts_to_sequences(labels_batch)).flatten().tolist()\n",
    "\n",
    "    # apply padding on tokens\n",
    "    # we used -1 as padding to allow for reversing of the attention mask\n",
    "    padded_tokenized_tokens_batch = pad_sequences(tokenized_tokens_batch, padding='post', value=-1)\n",
    "\n",
    "    # create the attention mask\n",
    "    attention_mask = np.array(padded_tokenized_tokens_batch)\n",
    "    # change mask tokens to 0\n",
    "    attention_mask[attention_mask == tokenizer.word_index['[mask]']] = 0\n",
    "    # change non-masked tokens to 1\n",
    "    attention_mask[attention_mask > 0] = 1\n",
    "\n",
    "    # create reversed attention mask\n",
    "    mlm_mask = attention_mask.copy()\n",
    "    mlm_mask[mlm_mask == 0] = 1\n",
    "    mlm_mask[np.logical_or(mlm_mask == 1, mlm_mask == -1)] = 0\n",
    "    mlm_mask = np.expand_dims(mlm_mask, axis=-1)\n",
    "\n",
    "    # create labels\n",
    "    labels = attention_mask.copy()\n",
    "    labels[labels == -1] = 1\n",
    "    labels = labels.tolist()\n",
    "    masked_token_index = 0\n",
    "    for sequence in labels:\n",
    "        for token_index, token in enumerate(sequence):\n",
    "            if (token == 0):\n",
    "                token_label = [0] * input_vocab_size\n",
    "                token_label[tokenized_labels_batch[masked_token_index]] = 1\n",
    "                sequence[token_index] = token_label\n",
    "                masked_token_index += 1\n",
    "            elif(token == 1):\n",
    "                sequence[token_index] = [0] * input_vocab_size\n",
    "\n",
    "    # change padding tokens to 0\n",
    "    attention_mask[attention_mask == -1] = 0\n",
    "\n",
    "    input = (tf.constant(padded_tokenized_tokens_batch, tf.float32), tf.constant(attention_mask, tf.float32), tf.constant(mlm_mask, tf.float32))\n",
    "\n",
    "    return (input, tf.constant(labels, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# model_trainable_variables = []\n",
    "# model_gradients = []\n",
    "\n",
    "# Define a training loop\n",
    "def train_step(batch, input_vocab_size):\n",
    "    start_time = time.time()\n",
    "\n",
    "    inputs, labels = tokenize(batch, input_vocab_size)\n",
    "\n",
    "    # counter = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=False)\n",
    "\n",
    "        loss = loss_function(labels, predictions)\n",
    "        # print('\\n> LOSS')\n",
    "        # print(loss)\n",
    "\n",
    "    # get the predicted token(s) ID(s)\n",
    "    # indices = []\n",
    "    # predicted_token = []\n",
    "    # for index, row in enumerate(mask[0]):\n",
    "    #     if (row == 0):\n",
    "    #         predicted_token.append(np.argmax(predictions[index]))\n",
    "            # indices.append(index)\n",
    "\n",
    "    # if (counter == 9):\n",
    "    #     print(inputs, labels, mask, token_indices, token_ids)\n",
    "    #     # print('\\n> LABELS')\n",
    "    #     # print(tokenized_labels)\n",
    "    #     print('\\n> PREDICTIONS')\n",
    "    #     print(predictions)\n",
    "\n",
    "    #     # display the token index and element index of values > 0\n",
    "    #     for index, row in enumerate(tokenized_labels):\n",
    "    #         for element_index, element in enumerate(row):\n",
    "    #             if (element > 0):\n",
    "    #                 print(index, element, element_index)\n",
    "\n",
    "    # Manual Loss calculation\n",
    "    # total_loss_test = 0\n",
    "    # for tokenized_label, prediction in zip(tokenized_labels, predictions):\n",
    "    #     total_loss_test += np.sum(tokenized_label * -np.log(prediction))\n",
    "    # print(\"manual:\", total_loss_test / len(predictions))\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # print('GRADIENTS')\n",
    "    # print(gradients)\n",
    "\n",
    "    # model_gradients.append(gradients)\n",
    "\n",
    "    # gradients_accumulator = [grad_accum + grad for grad_accum, grad in zip(gradients_accumulator, gradients)]\n",
    "    # total_loss += loss\n",
    "\n",
    "    # print('Seq ' + str(counter) + ', Loss = ' + str(loss.numpy()) + ', Predicted Token = ' + str(predicted_token) + ', True Token = ' + str(token_ids))\n",
    "    # counter += 1\n",
    "\n",
    "    # gradients_avg = [grad / len(inputs_batch) for grad in gradients_accumulator]\n",
    "    \n",
    "    # optimizer.minimize(total_loss / len(inputs_batch), model.trainable_variables, tape=tape)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, str(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\test_encoders\\batching_transformer_encoder\\run_test.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_index, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(mlm_dataset):  \u001b[39m# Provide training data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         loss, elapsed_time \u001b[39m=\u001b[39m train_step(batch, input_vocab_size)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m# Log or print the loss for monitoring\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(epoch) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, Batch \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(batch_index) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, Loss = \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(loss\u001b[39m.\u001b[39mnumpy()) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, Elapsed Time: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m elapsed_time \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example of usage in the training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_index, batch in enumerate(mlm_dataset):  # Provide training data\n",
    "        loss, elapsed_time = train_step(batch, input_vocab_size)\n",
    "        # Log or print the loss for monitoring\n",
    "        print('Epoch ' + str(epoch) + ', Batch ' + str(batch_index) + ', Loss = ' + str(loss.numpy()) + ', Elapsed Time: ' + elapsed_time + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_trainable_variables[0])\n",
    "# print(model_gradients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_trainable_variables[8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
