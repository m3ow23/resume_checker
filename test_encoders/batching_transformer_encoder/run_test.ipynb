{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from transformer_encoder import MLMTransformerEncoder\n",
    "from mlm_dataset.mlm_dataset_generator import MLMDatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example with original Transformer hyperparameters\n",
    "num_layers = 1\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "input_vocab_size = 40000\n",
    "maximum_position_encoding = 10000\n",
    "\n",
    "model = MLMTransformerEncoder(num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding)\n",
    "model([tf.keras.Input(shape=(None,)), tf.keras.Input(shape=(None,)), tf.keras.Input(shape=(None,))])\n",
    "\n",
    "# Define an optimizer (e.g., Adam)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Define a loss function (e.g., categorical cross-entropy for classification)\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM dataset for training\n",
    "mlm_dataset_generator = MLMDatasetGenerator('../../dataset/resume_dataset.csv')\n",
    "\n",
    "oov_token = '[oov]'\n",
    "\n",
    "# Initialize a Tokenizer and fit on text data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token, filters='')\n",
    "# Traing tokenizer on dataset vocab\n",
    "tokenizer.fit_on_texts(mlm_dataset_generator.getVocubulary())\n",
    "\n",
    "# padding function\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "batch_size = 20\n",
    "# default is None\n",
    "sample_limit = 1000\n",
    "\n",
    "mlm_dataset = mlm_dataset_generator.generateMLMDataset(batch_size, sample_limit=sample_limit)\n",
    "\n",
    "# to free memory\n",
    "mlm_dataset_generator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many words are in the dataset (currently: 37032)\n",
    "# print(tokenizer.word_index['[mask]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM dataset checker\n",
    "# inputs, mask, labels = mlm_dataset[0]\n",
    "\n",
    "# print(inputs)\n",
    "# print(mask)\n",
    "\n",
    "# for sequence_index, sequence in enumerate(labels):\n",
    "#     for token_index, token in enumerate(sequence):\n",
    "#         for value in token:\n",
    "#             if (value > 0):\n",
    "#                 print(sequence_index, token_index, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch: tuple, input_vocab_size) -> ((tf.constant([], tf.float32), tf.constant([], tf.float32)), tf.constant([], tf.float32)):\n",
    "    tokens_batch, labels_batch = batch\n",
    "\n",
    "    # tokenize the tokens_batch and labels_batch, string to token_id conversion\n",
    "    tokenized_tokens_batch = tokenizer.texts_to_sequences(tokens_batch)\n",
    "    tokenized_labels_batch = np.array(tokenizer.texts_to_sequences(labels_batch)).flatten().tolist()\n",
    "\n",
    "    # apply padding on tokens\n",
    "    # we used -1 as padding to allow for reversing of the attention mask\n",
    "    padded_tokenized_tokens_batch = pad_sequences(tokenized_tokens_batch, padding='post', value=-1)\n",
    "\n",
    "    # create the attention mask\n",
    "    attention_mask = np.array(padded_tokenized_tokens_batch)\n",
    "    # change mask tokens to 0\n",
    "    attention_mask[attention_mask == tokenizer.word_index['[mask]']] = 0\n",
    "    # change non-masked tokens to 1\n",
    "    attention_mask[attention_mask > 0] = 1\n",
    "\n",
    "    # create reversed attention mask\n",
    "    mlm_mask = attention_mask.copy()\n",
    "    mlm_mask[mlm_mask == 1] = -1\n",
    "    mlm_mask[mlm_mask == 0] = 1\n",
    "    mlm_mask[mlm_mask == -1] = 0\n",
    "    mlm_mask = np.expand_dims(mlm_mask, axis=-1)\n",
    "\n",
    "    # create labels\n",
    "    labels = attention_mask.copy()\n",
    "    labels[labels == -1] = 1\n",
    "    labels = labels.tolist()\n",
    "    masked_token_index = 0\n",
    "    for sequence in labels:\n",
    "        for token_index, token in enumerate(sequence):\n",
    "            if (token == 0):\n",
    "                token_label = [0] * input_vocab_size\n",
    "                token_label[tokenized_labels_batch[masked_token_index]] = 1\n",
    "                sequence[token_index] = token_label\n",
    "                masked_token_index += 1\n",
    "            elif(token == 1):\n",
    "                sequence[token_index] = [0] * input_vocab_size\n",
    "\n",
    "    # change padding tokens to 0\n",
    "    attention_mask[attention_mask == -1] = 0\n",
    "\n",
    "    return (tf.constant(padded_tokenized_tokens_batch, tf.float32), \n",
    "            tf.constant(attention_mask, tf.float32), \n",
    "            tf.constant(mlm_mask, tf.float32), \n",
    "            tf.constant(labels, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# model_trainable_variables = []\n",
    "# model_gradients = []\n",
    "\n",
    "# Define a training loop\n",
    "def train_step(batch, input_vocab_size):\n",
    "    # start_time = time.time()\n",
    "\n",
    "    padded_tokenized_tokens_batch, attention_mask, mlm_mask, labels = tokenize(batch, input_vocab_size)\n",
    "\n",
    "    # counter = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([padded_tokenized_tokens_batch, attention_mask, mlm_mask], training=False)\n",
    "\n",
    "        loss = loss_function(labels, predictions)\n",
    "        # print('\\n> LOSS')\n",
    "        # print(loss)\n",
    "\n",
    "    # get the predicted token(s) ID(s)\n",
    "    # indices = []\n",
    "    # predicted_token = []\n",
    "    # for index, row in enumerate(mask[0]):\n",
    "    #     if (row == 0):\n",
    "    #         predicted_token.append(np.argmax(predictions[index]))\n",
    "            # indices.append(index)\n",
    "\n",
    "    # if (counter == 9):\n",
    "    #     print(inputs, labels, mask, token_indices, token_ids)\n",
    "    #     # print('\\n> LABELS')\n",
    "    #     # print(tokenized_labels)\n",
    "    #     print('\\n> PREDICTIONS')\n",
    "    #     print(predictions)\n",
    "\n",
    "    #     # display the token index and element index of values > 0\n",
    "    #     for index, row in enumerate(tokenized_labels):\n",
    "    #         for element_index, element in enumerate(row):\n",
    "    #             if (element > 0):\n",
    "    #                 print(index, element, element_index)\n",
    "\n",
    "    # Manual Loss calculation\n",
    "    # total_loss_test = 0\n",
    "    # for tokenized_label, prediction in zip(tokenized_labels, predictions):\n",
    "    #     total_loss_test += np.sum(tokenized_label * -np.log(prediction))\n",
    "    # print(\"manual:\", total_loss_test / len(predictions))\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # print('GRADIENTS')\n",
    "    # print(gradients)\n",
    "\n",
    "    # model_gradients.append(gradients)\n",
    "\n",
    "    # gradients_accumulator = [grad_accum + grad for grad_accum, grad in zip(gradients_accumulator, gradients)]\n",
    "    # total_loss += loss\n",
    "\n",
    "    # print('Seq ' + str(counter) + ', Loss = ' + str(loss.numpy()) + ', Predicted Token = ' + str(predicted_token) + ', True Token = ' + str(token_ids))\n",
    "    # counter += 1\n",
    "\n",
    "    # gradients_avg = [grad / len(inputs_batch) for grad in gradients_accumulator]\n",
    "    \n",
    "    # optimizer.minimize(total_loss / len(inputs_batch), model.trainable_variables, tape=tape)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss #, str(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss = 0.7173047\n",
      "Epoch 0, Batch 1, Loss = 0.9461497\n",
      "Epoch 0, Batch 2, Loss = 0.39980787\n",
      "Epoch 0, Batch 3, Loss = 1.161488\n",
      "Epoch 0, Batch 4, Loss = 0.92218745\n",
      "Epoch 0, Batch 5, Loss = 0.4583708\n",
      "Epoch 0, Batch 6, Loss = 0.56126654\n",
      "Epoch 0, Batch 7, Loss = 0.53752095\n",
      "Epoch 0, Batch 8, Loss = 0.6324646\n",
      "Epoch 0, Batch 9, Loss = 0.8333324\n",
      "Epoch 0, Batch 10, Loss = 0.31450394\n",
      "Epoch 0, Batch 11, Loss = 0.41068906\n",
      "Epoch 0, Batch 12, Loss = 1.2938486\n",
      "Epoch 0, Batch 13, Loss = 0.50085026\n",
      "Epoch 0, Batch 14, Loss = 0.7120204\n",
      "Epoch 0, Batch 15, Loss = 0.9528971\n",
      "Epoch 0, Batch 16, Loss = 0.18879342\n",
      "Epoch 0, Batch 17, Loss = 1.0689213\n",
      "Epoch 0, Batch 18, Loss = 1.126787\n",
      "Epoch 0, Batch 19, Loss = 0.33892745\n",
      "Epoch 0, Batch 20, Loss = 0.6003047\n",
      "Epoch 0, Batch 21, Loss = 0.37335286\n",
      "Epoch 0, Batch 22, Loss = 0.32314193\n",
      "Epoch 0, Batch 23, Loss = 0.8925075\n",
      "Epoch 0, Batch 24, Loss = 0.96448386\n",
      "Epoch 0, Batch 25, Loss = 0.84367436\n",
      "Epoch 0, Batch 26, Loss = 0.13906586\n",
      "Epoch 0, Batch 27, Loss = 0.26643005\n",
      "Epoch 0, Batch 28, Loss = 0.5619989\n",
      "Epoch 0, Batch 29, Loss = 0.35848415\n",
      "Epoch 0, Batch 30, Loss = 0.60987175\n",
      "Epoch 0, Batch 31, Loss = 0.6048492\n",
      "Epoch 0, Batch 32, Loss = 0.31536692\n",
      "Epoch 0, Batch 33, Loss = 0.68801177\n",
      "Epoch 0, Batch 34, Loss = 0.8006128\n",
      "Epoch 0, Batch 35, Loss = 0.32212234\n",
      "Epoch 0, Batch 36, Loss = 0.67487824\n",
      "Epoch 0, Batch 37, Loss = 0.34987754\n",
      "Epoch 0, Batch 38, Loss = 0.25330046\n",
      "Epoch 0, Batch 39, Loss = 0.3693058\n",
      "Epoch 0, Batch 40, Loss = 0.39820793\n",
      "Epoch 0, Batch 41, Loss = 0.36775556\n",
      "Epoch 0, Batch 42, Loss = 0.54861\n",
      "Epoch 0, Batch 43, Loss = 0.72319293\n",
      "Epoch 0, Batch 44, Loss = 0.7426889\n",
      "Epoch 0, Batch 45, Loss = 0.5242198\n",
      "Epoch 0, Batch 46, Loss = 0.84041834\n",
      "Epoch 0, Batch 47, Loss = 0.89787006\n",
      "Epoch 0, Batch 48, Loss = 0.3600042\n",
      "Epoch 0, Batch 49, Loss = 0.7204997\n",
      "Epoch 1, Batch 0, Loss = 0.5282999\n",
      "Epoch 1, Batch 1, Loss = 0.63314307\n",
      "Epoch 1, Batch 2, Loss = 0.29944104\n",
      "Epoch 1, Batch 3, Loss = 0.7986245\n",
      "Epoch 1, Batch 4, Loss = 0.6548588\n",
      "Epoch 1, Batch 5, Loss = 0.3011123\n",
      "Epoch 1, Batch 6, Loss = 0.38412067\n",
      "Epoch 1, Batch 7, Loss = 0.34079725\n",
      "Epoch 1, Batch 8, Loss = 0.4179318\n",
      "Epoch 1, Batch 9, Loss = 0.58846337\n",
      "Epoch 1, Batch 10, Loss = 0.22479494\n",
      "Epoch 1, Batch 11, Loss = 0.29265034\n",
      "Epoch 1, Batch 12, Loss = 0.86279243\n",
      "Epoch 1, Batch 13, Loss = 0.3688251\n",
      "Epoch 1, Batch 14, Loss = 0.5343849\n",
      "Epoch 1, Batch 15, Loss = 0.71131384\n",
      "Epoch 1, Batch 16, Loss = 0.14974387\n",
      "Epoch 1, Batch 17, Loss = 0.7886137\n",
      "Epoch 1, Batch 18, Loss = 0.8451739\n",
      "Epoch 1, Batch 19, Loss = 0.26741087\n",
      "Epoch 1, Batch 20, Loss = 0.47175205\n",
      "Epoch 1, Batch 21, Loss = 0.31017905\n",
      "Epoch 1, Batch 22, Loss = 0.26927072\n",
      "Epoch 1, Batch 23, Loss = 0.75043607\n",
      "Epoch 1, Batch 24, Loss = 0.794703\n",
      "Epoch 1, Batch 25, Loss = 0.7160386\n",
      "Epoch 1, Batch 26, Loss = 0.12533635\n",
      "Epoch 1, Batch 27, Loss = 0.23305345\n",
      "Epoch 1, Batch 28, Loss = 0.48319197\n",
      "Epoch 1, Batch 29, Loss = 0.30907047\n",
      "Epoch 1, Batch 30, Loss = 0.5155372\n",
      "Epoch 1, Batch 31, Loss = 0.52053434\n",
      "Epoch 1, Batch 32, Loss = 0.27709904\n",
      "Epoch 1, Batch 33, Loss = 0.57910395\n",
      "Epoch 1, Batch 34, Loss = 0.6710982\n",
      "Epoch 1, Batch 35, Loss = 0.27723217\n",
      "Epoch 1, Batch 36, Loss = 0.59198344\n",
      "Epoch 1, Batch 37, Loss = 0.30798852\n",
      "Epoch 1, Batch 38, Loss = 0.21758685\n",
      "Epoch 1, Batch 39, Loss = 0.31359515\n",
      "Epoch 1, Batch 40, Loss = 0.33446163\n",
      "Epoch 1, Batch 41, Loss = 0.29321748\n",
      "Epoch 1, Batch 42, Loss = 0.4774029\n",
      "Epoch 1, Batch 43, Loss = 0.57354516\n",
      "Epoch 1, Batch 44, Loss = 0.6163847\n",
      "Epoch 1, Batch 45, Loss = 0.43200287\n",
      "Epoch 1, Batch 46, Loss = 0.70662403\n",
      "Epoch 1, Batch 47, Loss = 0.74802613\n",
      "Epoch 1, Batch 48, Loss = 0.30556452\n",
      "Epoch 1, Batch 49, Loss = 0.58450025\n",
      "Epoch 2, Batch 0, Loss = 0.4680432\n",
      "Epoch 2, Batch 1, Loss = 0.58917\n",
      "Epoch 2, Batch 2, Loss = 0.28467602\n",
      "Epoch 2, Batch 3, Loss = 0.77324134\n",
      "Epoch 2, Batch 4, Loss = 0.63232446\n",
      "Epoch 2, Batch 5, Loss = 0.30553976\n",
      "Epoch 2, Batch 6, Loss = 0.39177927\n",
      "Epoch 2, Batch 7, Loss = 0.35247722\n",
      "Epoch 2, Batch 8, Loss = 0.43048027\n",
      "Epoch 2, Batch 9, Loss = 0.5785792\n",
      "Epoch 2, Batch 10, Loss = 0.23036103\n",
      "Epoch 2, Batch 11, Loss = 0.30129445\n",
      "Epoch 2, Batch 12, Loss = 0.8688091\n",
      "Epoch 2, Batch 13, Loss = 0.382448\n",
      "Epoch 2, Batch 14, Loss = 0.5468371\n",
      "Epoch 2, Batch 15, Loss = 0.7067452\n",
      "Epoch 2, Batch 16, Loss = 0.15192363\n",
      "Epoch 2, Batch 17, Loss = 0.75204396\n",
      "Epoch 2, Batch 18, Loss = 0.7896581\n",
      "Epoch 2, Batch 19, Loss = 0.2645654\n",
      "Epoch 2, Batch 20, Loss = 0.45904458\n",
      "Epoch 2, Batch 21, Loss = 0.30102864\n",
      "Epoch 2, Batch 22, Loss = 0.26254174\n",
      "Epoch 2, Batch 23, Loss = 0.6952916\n",
      "Epoch 2, Batch 24, Loss = 0.7303649\n",
      "Epoch 2, Batch 25, Loss = 0.663428\n",
      "Epoch 2, Batch 26, Loss = 0.12213019\n",
      "Epoch 2, Batch 27, Loss = 0.22679779\n",
      "Epoch 2, Batch 28, Loss = 0.45442504\n",
      "Epoch 2, Batch 29, Loss = 0.29696345\n",
      "Epoch 2, Batch 30, Loss = 0.4869302\n",
      "Epoch 2, Batch 31, Loss = 0.4915905\n",
      "Epoch 2, Batch 32, Loss = 0.27110732\n",
      "Epoch 2, Batch 33, Loss = 0.56367415\n",
      "Epoch 2, Batch 34, Loss = 0.65100116\n",
      "Epoch 2, Batch 35, Loss = 0.27932084\n",
      "Epoch 2, Batch 36, Loss = 0.56648797\n",
      "Epoch 2, Batch 37, Loss = 0.30587724\n",
      "Epoch 2, Batch 38, Loss = 0.22110146\n",
      "Epoch 2, Batch 39, Loss = 0.31679073\n",
      "Epoch 2, Batch 40, Loss = 0.33469072\n",
      "Epoch 2, Batch 41, Loss = 0.29529464\n",
      "Epoch 2, Batch 42, Loss = 0.47804844\n",
      "Epoch 2, Batch 43, Loss = 0.57101333\n",
      "Epoch 2, Batch 44, Loss = 0.6141426\n",
      "Epoch 2, Batch 45, Loss = 0.42312568\n",
      "Epoch 2, Batch 46, Loss = 0.69649816\n",
      "Epoch 2, Batch 47, Loss = 0.69398254\n",
      "Epoch 2, Batch 48, Loss = 0.29754552\n",
      "Epoch 2, Batch 49, Loss = 0.55503273\n",
      "Epoch 3, Batch 0, Loss = 0.45055267\n",
      "Epoch 3, Batch 1, Loss = 0.56324023\n",
      "Epoch 3, Batch 2, Loss = 0.26414266\n",
      "Epoch 3, Batch 3, Loss = 0.715805\n",
      "Epoch 3, Batch 4, Loss = 0.5923805\n",
      "Epoch 3, Batch 5, Loss = 0.28965348\n",
      "Epoch 3, Batch 6, Loss = 0.3742758\n",
      "Epoch 3, Batch 7, Loss = 0.34219757\n",
      "Epoch 3, Batch 8, Loss = 0.41496718\n",
      "Epoch 3, Batch 9, Loss = 0.5293655\n",
      "Epoch 3, Batch 10, Loss = 0.21880451\n",
      "Epoch 3, Batch 11, Loss = 0.2894766\n",
      "Epoch 3, Batch 12, Loss = 0.821567\n",
      "Epoch 3, Batch 13, Loss = 0.3615701\n",
      "Epoch 3, Batch 14, Loss = 0.515783\n",
      "Epoch 3, Batch 15, Loss = 0.6710422\n",
      "Epoch 3, Batch 16, Loss = 0.14428152\n",
      "Epoch 3, Batch 17, Loss = 0.7264792\n",
      "Epoch 3, Batch 18, Loss = 0.75877684\n",
      "Epoch 3, Batch 19, Loss = 0.24957137\n",
      "Epoch 3, Batch 20, Loss = 0.44284326\n",
      "Epoch 3, Batch 21, Loss = 0.28745526\n",
      "Epoch 3, Batch 22, Loss = 0.2532596\n",
      "Epoch 3, Batch 23, Loss = 0.6888138\n",
      "Epoch 3, Batch 24, Loss = 0.7214471\n",
      "Epoch 3, Batch 25, Loss = 0.6618757\n",
      "Epoch 3, Batch 26, Loss = 0.12066739\n",
      "Epoch 3, Batch 27, Loss = 0.2239566\n",
      "Epoch 3, Batch 28, Loss = 0.43566358\n",
      "Epoch 3, Batch 29, Loss = 0.28560987\n",
      "Epoch 3, Batch 30, Loss = 0.45629027\n",
      "Epoch 3, Batch 31, Loss = 0.45381767\n",
      "Epoch 3, Batch 32, Loss = 0.24965517\n",
      "Epoch 3, Batch 33, Loss = 0.48811543\n",
      "Epoch 3, Batch 34, Loss = 0.57031924\n",
      "Epoch 3, Batch 35, Loss = 0.25583452\n",
      "Epoch 3, Batch 36, Loss = 0.57285464\n",
      "Epoch 3, Batch 37, Loss = 0.28646234\n",
      "Epoch 3, Batch 38, Loss = 0.20635982\n",
      "Epoch 3, Batch 39, Loss = 0.29264247\n",
      "Epoch 3, Batch 40, Loss = 0.30454183\n",
      "Epoch 3, Batch 41, Loss = 0.26192135\n",
      "Epoch 3, Batch 42, Loss = 0.43294752\n",
      "Epoch 3, Batch 43, Loss = 0.51240283\n",
      "Epoch 3, Batch 44, Loss = 0.55979747\n",
      "Epoch 3, Batch 45, Loss = 0.38929975\n",
      "Epoch 3, Batch 46, Loss = 0.6801898\n",
      "Epoch 3, Batch 47, Loss = 0.69556373\n",
      "Epoch 3, Batch 48, Loss = 0.27472168\n",
      "Epoch 3, Batch 49, Loss = 0.5441968\n",
      "Epoch 4, Batch 0, Loss = 0.4317875\n",
      "Epoch 4, Batch 1, Loss = 0.59681046\n",
      "Epoch 4, Batch 2, Loss = 0.25894433\n",
      "Epoch 4, Batch 3, Loss = 0.7172803\n",
      "Epoch 4, Batch 4, Loss = 0.5692895\n",
      "Epoch 4, Batch 5, Loss = 0.27889782\n",
      "Epoch 4, Batch 6, Loss = 0.36332002\n",
      "Epoch 4, Batch 7, Loss = 0.33759478\n",
      "Epoch 4, Batch 8, Loss = 0.41504943\n",
      "Epoch 4, Batch 9, Loss = 0.5049604\n",
      "Epoch 4, Batch 10, Loss = 0.21850586\n",
      "Epoch 4, Batch 11, Loss = 0.2929429\n",
      "Epoch 4, Batch 12, Loss = 0.73254687\n",
      "Epoch 4, Batch 13, Loss = 0.37230396\n",
      "Epoch 4, Batch 14, Loss = 0.5196982\n",
      "Epoch 4, Batch 15, Loss = 0.6580841\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[20,99,40000] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Mul] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\test_encoders\\batching_transformer_encoder\\run_test.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_index, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(mlm_dataset):  \u001b[39m# Provide training data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         loss \u001b[39m=\u001b[39m train_step(batch, input_vocab_size)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m# Log or print the loss for monitoring\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(epoch) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, Batch \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(batch_index) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, Loss = \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(loss\u001b[39m.\u001b[39mnumpy())) \u001b[39m# + ', Elapsed Time: ' + elapsed_time)\u001b[39;00m\n",
      "\u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\test_encoders\\batching_transformer_encoder\\run_test.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(labels, predictions)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# print('\\n> LOSS')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# print(loss)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m#     total_loss_test += np.sum(tokenized_label * -np.log(prediction))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# print(\"manual:\", total_loss_test / len(predictions))\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss, model\u001b[39m.\u001b[39;49mtrainable_variables)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# print('GRADIENTS')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# print(gradients)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# optimizer.minimize(total_loss / len(inputs_batch), model.trainable_variables, tape=tape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X31sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(gradients, model\u001b[39m.\u001b[39mtrainable_variables))\n",
      "File \u001b[1;32md:\\Applications\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1065\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1059\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[0;32m   1060\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1061\u001b[0m           output_gradients))\n\u001b[0;32m   1062\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1063\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1065\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[0;32m   1066\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[0;32m   1067\u001b[0m     flat_targets,\n\u001b[0;32m   1068\u001b[0m     flat_sources,\n\u001b[0;32m   1069\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[0;32m   1070\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[0;32m   1071\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[0;32m   1073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[0;32m   1074\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32md:\\Applications\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[0;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m     target,\n\u001b[0;32m     70\u001b[0m     sources,\n\u001b[0;32m     71\u001b[0m     output_gradients,\n\u001b[0;32m     72\u001b[0m     sources_raw,\n\u001b[0;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[1;32md:\\Applications\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:147\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    145\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[1;32m--> 147\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[0;32m    148\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[1;32md:\\Applications\\Python\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1378\u001b[0m, in \u001b[0;36m_MulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1374\u001b[0m x \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39minputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1375\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(grad, tensor\u001b[39m.\u001b[39mTensor) \u001b[39mand\u001b[39;00m\n\u001b[0;32m   1376\u001b[0m     _ShapesFullySpecifiedAndEqual(x, y, grad) \u001b[39mand\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m     grad\u001b[39m.\u001b[39mdtype \u001b[39min\u001b[39;00m (dtypes\u001b[39m.\u001b[39mint32, dtypes\u001b[39m.\u001b[39mfloat32)):\n\u001b[1;32m-> 1378\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39mmul(grad, y), gen_math_ops\u001b[39m.\u001b[39;49mmul(grad, x)\n\u001b[0;32m   1379\u001b[0m \u001b[39massert\u001b[39;00m x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype \u001b[39m==\u001b[39m y\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype, (x\u001b[39m.\u001b[39mdtype, \u001b[39m\"\u001b[39m\u001b[39m vs. \u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m   1381\u001b[0m (sx, rx, must_reduce_x), (sy, ry, must_reduce_y) \u001b[39m=\u001b[39m (\n\u001b[0;32m   1382\u001b[0m     SmartBroadcastGradientArgs(x, y, grad))\n",
      "File \u001b[1;32md:\\Applications\\Python\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:7502\u001b[0m, in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   7500\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   7501\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 7502\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[0;32m   7503\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[0;32m   7504\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\Python\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5888\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   5886\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NoReturn:\n\u001b[0;32m   5887\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m-> 5888\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[20,99,40000] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Mul] name: "
     ]
    }
   ],
   "source": [
    "# Example of usage in the training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_index, batch in enumerate(mlm_dataset):  # Provide training data\n",
    "        loss = train_step(batch, input_vocab_size)\n",
    "        # Log or print the loss for monitoring\n",
    "        print('Epoch ' + str(epoch) + ', Batch ' + str(batch_index) + ', Loss = ' + str(loss.numpy())) # + ', Elapsed Time: ' + elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_trainable_variables[0])\n",
    "# print(model_gradients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_trainable_variables[8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
