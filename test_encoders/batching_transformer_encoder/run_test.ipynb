{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from transformer_encoder import MLMTransformerEncoder\n",
    "from mlm_dataset.mlm_dataset_generator import MLMDatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example with original Transformer hyperparameters\n",
    "num_layers = 1\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "input_vocab_size = 40000\n",
    "maximum_position_encoding = 10000\n",
    "\n",
    "model = MLMTransformerEncoder(num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding)\n",
    "model([tf.keras.Input(shape=(None,)), tf.keras.Input(shape=(None,)), tf.keras.Input(shape=(None,))])\n",
    "\n",
    "# Define an optimizer (e.g., Adam)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Define a loss function (e.g., categorical cross-entropy for classification)\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM dataset for training\n",
    "mlm_dataset_generator = MLMDatasetGenerator('../../dataset/resume_dataset.csv')\n",
    "\n",
    "oov_token = '[oov]'\n",
    "\n",
    "# Initialize a Tokenizer and fit on text data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token, filters='')\n",
    "# Traing tokenizer on dataset vocab\n",
    "tokenizer.fit_on_texts(mlm_dataset_generator.getVocubulary())\n",
    "\n",
    "# padding function\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "batch_size = 5\n",
    "# default is None\n",
    "sample_limit = 20\n",
    "\n",
    "mlm_dataset = [mlm_dataset_generator.generateMLMDataset(batch_size, sample_limit=sample_limit)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many words are in the dataset (currently: 37032)\n",
    "# print(tokenizer.word_index['[mask]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM dataset checker\n",
    "# inputs, mask, labels = mlm_dataset[0]\n",
    "\n",
    "# print(inputs)\n",
    "# print(mask)\n",
    "\n",
    "# for sequence_index, sequence in enumerate(labels):\n",
    "#     for token_index, token in enumerate(sequence):\n",
    "#         for value in token:\n",
    "#             if (value > 0):\n",
    "#                 print(sequence_index, token_index, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch: tuple, input_vocab_size) -> ((tf.constant([], tf.float32), tf.constant([], tf.float32)), tf.constant([], tf.float32)):\n",
    "    tokens_batch, labels_batch = batch\n",
    "\n",
    "    # tokenize the tokens_batch and labels_batch, string to token_id conversion\n",
    "    tokenized_tokens_batch = tokenizer.texts_to_sequences(tokens_batch)\n",
    "    tokenized_labels_batch = np.array(tokenizer.texts_to_sequences(labels_batch)).flatten().tolist()\n",
    "\n",
    "    # apply padding on tokens\n",
    "    # we used -1 as padding to allow for reversing of the attention mask\n",
    "    padded_tokenized_tokens_batch = pad_sequences(tokenized_tokens_batch, padding='post', value=-1)\n",
    "\n",
    "    # create the attention mask\n",
    "    attention_mask = np.array(padded_tokenized_tokens_batch)\n",
    "    # change mask tokens to 0\n",
    "    attention_mask[attention_mask == tokenizer.word_index['[mask]']] = 0\n",
    "    # change non-masked tokens to 1\n",
    "    attention_mask[attention_mask > 0] = 1\n",
    "\n",
    "    # create reversed attention mask\n",
    "    mlm_mask = attention_mask.copy()\n",
    "    mlm_mask[mlm_mask == 1] = -1\n",
    "    mlm_mask[mlm_mask == 0] = 1\n",
    "    mlm_mask[mlm_mask == -1] = 0\n",
    "    mlm_mask = np.expand_dims(mlm_mask, axis=-1)\n",
    "\n",
    "    # create labels\n",
    "    labels = attention_mask.copy()\n",
    "    labels[labels == -1] = 1\n",
    "    labels = labels.tolist()\n",
    "    masked_token_index = 0\n",
    "    for sequence in labels:\n",
    "        for token_index, token in enumerate(sequence):\n",
    "            if (token == 0):\n",
    "                token_label = [0] * input_vocab_size\n",
    "                token_label[tokenized_labels_batch[masked_token_index]] = 1\n",
    "                sequence[token_index] = token_label\n",
    "                masked_token_index += 1\n",
    "            elif(token == 1):\n",
    "                sequence[token_index] = [0] * input_vocab_size\n",
    "\n",
    "    # change padding tokens to 0\n",
    "    attention_mask[attention_mask == -1] = 0\n",
    "\n",
    "    return (tf.constant(padded_tokenized_tokens_batch, tf.float32), \n",
    "            tf.constant(attention_mask, tf.float32), \n",
    "            tf.constant(mlm_mask, tf.float32), \n",
    "            tf.constant(labels, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# model_trainable_variables = []\n",
    "# model_gradients = []\n",
    "\n",
    "# Define a training loop\n",
    "def train_step(batch, input_vocab_size):\n",
    "    start_time = time.time()\n",
    "\n",
    "    padded_tokenized_tokens_batch, attention_mask, mlm_mask, labels = tokenize(batch, input_vocab_size)\n",
    "\n",
    "    # counter = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([padded_tokenized_tokens_batch, attention_mask, mlm_mask], training=False)\n",
    "\n",
    "        loss = loss_function(labels, predictions)\n",
    "        # print('\\n> LOSS')\n",
    "        # print(loss)\n",
    "\n",
    "    # get the predicted token(s) ID(s)\n",
    "    # indices = []\n",
    "    # predicted_token = []\n",
    "    # for index, row in enumerate(mask[0]):\n",
    "    #     if (row == 0):\n",
    "    #         predicted_token.append(np.argmax(predictions[index]))\n",
    "            # indices.append(index)\n",
    "\n",
    "    # if (counter == 9):\n",
    "    #     print(inputs, labels, mask, token_indices, token_ids)\n",
    "    #     # print('\\n> LABELS')\n",
    "    #     # print(tokenized_labels)\n",
    "    #     print('\\n> PREDICTIONS')\n",
    "    #     print(predictions)\n",
    "\n",
    "    #     # display the token index and element index of values > 0\n",
    "    #     for index, row in enumerate(tokenized_labels):\n",
    "    #         for element_index, element in enumerate(row):\n",
    "    #             if (element > 0):\n",
    "    #                 print(index, element, element_index)\n",
    "\n",
    "    # Manual Loss calculation\n",
    "    # total_loss_test = 0\n",
    "    # for tokenized_label, prediction in zip(tokenized_labels, predictions):\n",
    "    #     total_loss_test += np.sum(tokenized_label * -np.log(prediction))\n",
    "    # print(\"manual:\", total_loss_test / len(predictions))\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # print('GRADIENTS')\n",
    "    # print(gradients)\n",
    "\n",
    "    # model_gradients.append(gradients)\n",
    "\n",
    "    # gradients_accumulator = [grad_accum + grad for grad_accum, grad in zip(gradients_accumulator, gradients)]\n",
    "    # total_loss += loss\n",
    "\n",
    "    # print('Seq ' + str(counter) + ', Loss = ' + str(loss.numpy()) + ', Predicted Token = ' + str(predicted_token) + ', True Token = ' + str(token_ids))\n",
    "    # counter += 1\n",
    "\n",
    "    # gradients_avg = [grad / len(inputs_batch) for grad in gradients_accumulator]\n",
    "    \n",
    "    # optimizer.minimize(total_loss / len(inputs_batch), model.trainable_variables, tape=tape)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, str(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss = 0.8308428, Elapsed Time: 1.4930076599121094\n",
      "\n",
      "Epoch 1, Batch 0, Loss = 0.75420004, Elapsed Time: 1.4474689960479736\n",
      "\n",
      "Epoch 2, Batch 0, Loss = 0.68537486, Elapsed Time: 1.373466968536377\n",
      "\n",
      "Epoch 3, Batch 0, Loss = 0.61206937, Elapsed Time: 1.3865442276000977\n",
      "\n",
      "Epoch 4, Batch 0, Loss = 0.5399508, Elapsed Time: 1.3843226432800293\n",
      "\n",
      "Epoch 5, Batch 0, Loss = 0.46463642, Elapsed Time: 1.4256720542907715\n",
      "\n",
      "Epoch 6, Batch 0, Loss = 0.39152712, Elapsed Time: 1.4504659175872803\n",
      "\n",
      "Epoch 7, Batch 0, Loss = 0.32413942, Elapsed Time: 1.3794887065887451\n",
      "\n",
      "Epoch 8, Batch 0, Loss = 0.2660629, Elapsed Time: 1.4080462455749512\n",
      "\n",
      "Epoch 9, Batch 0, Loss = 0.20680784, Elapsed Time: 1.380868911743164\n",
      "\n",
      "Epoch 10, Batch 0, Loss = 0.16402, Elapsed Time: 1.3492512702941895\n",
      "\n",
      "Epoch 11, Batch 0, Loss = 0.13333476, Elapsed Time: 1.3869976997375488\n",
      "\n",
      "Epoch 12, Batch 0, Loss = 0.11300136, Elapsed Time: 1.3764543533325195\n",
      "\n",
      "Epoch 13, Batch 0, Loss = 0.092129715, Elapsed Time: 1.4674429893493652\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\test_encoders\\batching_transformer_encoder\\run_test.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_index, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(mlm_dataset):  \u001b[39m# Provide training data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         loss, elapsed_time \u001b[39m=\u001b[39m train_step(batch, input_vocab_size)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m# Log or print the loss for monitoring\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(epoch) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, Batch \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(batch_index) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, Loss = \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(loss\u001b[39m.\u001b[39mnumpy()) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, Elapsed Time: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m elapsed_time \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\test_encoders\\batching_transformer_encoder\\run_test.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X10sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, model\u001b[39m.\u001b[39mtrainable_variables)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X10sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# print('GRADIENTS')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X10sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# print(gradients)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X10sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X10sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X10sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# optimizer.minimize(total_loss / len(inputs_batch), model.trainable_variables, tape=tape)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X10sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mapply_gradients(\u001b[39mzip\u001b[39;49m(gradients, model\u001b[39m.\u001b[39;49mtrainable_variables))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_transformer_encoder/run_test.ipynb#X10sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss, \u001b[39mstr\u001b[39m(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1223\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_gradients_aggregation \u001b[39mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m   1222\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[1;32m-> 1223\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:652\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[0;32m    651\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, trainable_variables))\n\u001b[1;32m--> 652\u001b[0m iteration \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_apply_gradients(grads_and_vars)\n\u001b[0;32m    654\u001b[0m \u001b[39m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[39mfor\u001b[39;00m variable \u001b[39min\u001b[39;00m trainable_variables:\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1253\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mesh \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_with_dtensor:\n\u001b[0;32m   1250\u001b[0m     \u001b[39m# Skip any usage of strategy logic for DTensor\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[1;32m-> 1253\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49m__internal__\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49minterim\u001b[39m.\u001b[39;49mmaybe_merge_call(\n\u001b[0;32m   1254\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distributed_apply_gradients_fn,\n\u001b[0;32m   1255\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distribution_strategy,\n\u001b[0;32m   1256\u001b[0m     grads_and_vars,\n\u001b[0;32m   1257\u001b[0m )\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[1;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[39mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39m  The return value of the `fn` call.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[1;32m---> 51\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(strategy, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   \u001b[39mreturn\u001b[39;00m distribute_lib\u001b[39m.\u001b[39mget_replica_context()\u001b[39m.\u001b[39mmerge_call(\n\u001b[0;32m     54\u001b[0m       fn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1345\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[1;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[0;32m   1342\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step(grad, var)\n\u001b[0;32m   1344\u001b[0m \u001b[39mfor\u001b[39;00m grad, var \u001b[39min\u001b[39;00m grads_and_vars:\n\u001b[1;32m-> 1345\u001b[0m     distribution\u001b[39m.\u001b[39;49mextended\u001b[39m.\u001b[39;49mupdate(\n\u001b[0;32m   1346\u001b[0m         var, apply_grad_to_update_var, args\u001b[39m=\u001b[39;49m(grad,), group\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1349\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_ema:\n\u001b[0;32m   1350\u001b[0m     _, var_list \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3011\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   3008\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[0;32m   3009\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   3010\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[1;32m-> 3011\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update(var, fn, args, kwargs, group)\n\u001b[0;32m   3012\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replica_ctx_update(\n\u001b[0;32m   3014\u001b[0m       var, fn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs, group\u001b[39m=\u001b[39mgroup)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4081\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   4078\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update\u001b[39m(\u001b[39mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[0;32m   4079\u001b[0m   \u001b[39m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[0;32m   4080\u001b[0m   \u001b[39m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[1;32m-> 4081\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_non_slot(var, fn, (var,) \u001b[39m+\u001b[39;49m \u001b[39mtuple\u001b[39;49m(args), kwargs, group)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4087\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   4083\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_non_slot\u001b[39m(\u001b[39mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[0;32m   4084\u001b[0m   \u001b[39m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[0;32m   4085\u001b[0m   \u001b[39m# once that value is used for something.\u001b[39;00m\n\u001b[0;32m   4086\u001b[0m   \u001b[39mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[1;32m-> 4087\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   4088\u001b[0m     \u001b[39mif\u001b[39;00m should_group:\n\u001b[0;32m   4089\u001b[0m       \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    595\u001b[0m   \u001b[39mwith\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mControlStatusCtx(status\u001b[39m=\u001b[39mag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mUNSPECIFIED):\n\u001b[1;32m--> 596\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1342\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m   1340\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step_xla(grad, var, \u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_key(var)))\n\u001b[0;32m   1341\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1342\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_step(grad, var)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:241\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_key(variable) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_dict:\n\u001b[0;32m    233\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    234\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe optimizer cannot recognize variable \u001b[39m\u001b[39m{\u001b[39;00mvariable\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`tf.keras.optimizers.legacy.\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[1;32m--> 241\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_step(gradient, variable)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\adam.py:199\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[39m# Dense gradients.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     m\u001b[39m.\u001b[39massign_add((gradient \u001b[39m-\u001b[39m m) \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_1))\n\u001b[1;32m--> 199\u001b[0m     v\u001b[39m.\u001b[39;49massign_add((tf\u001b[39m.\u001b[39;49msquare(gradient) \u001b[39m-\u001b[39;49m v) \u001b[39m*\u001b[39;49m (\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeta_2))\n\u001b[0;32m    200\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mamsgrad:\n\u001b[0;32m    201\u001b[0m         v_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_velocity_hats[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_dict[var_key]]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example of usage in the training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_index, batch in enumerate(mlm_dataset):  # Provide training data\n",
    "        loss, elapsed_time = train_step(batch, input_vocab_size)\n",
    "        # Log or print the loss for monitoring\n",
    "        print('Epoch ' + str(epoch) + ', Batch ' + str(batch_index) + ', Loss = ' + str(loss.numpy()) + ', Elapsed Time: ' + elapsed_time + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_trainable_variables[0])\n",
    "# print(model_gradients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_trainable_variables[8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
