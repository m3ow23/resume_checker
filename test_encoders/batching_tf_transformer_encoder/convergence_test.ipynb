{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from transformer_encoder import MLMTransformerEncoder\n",
    "from mlm_dataset.mlm_dataset_generator import MLMDatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM dataset for training\n",
    "mlm_dataset_generator = MLMDatasetGenerator('../../dataset/resume_dataset.csv')\n",
    "inputs, labels = mlm_dataset_generator.generateMLMDataset(1)[0]\n",
    "inputs = inputs[0]\n",
    "labels = labels[0]\n",
    "print(inputs, labels)\n",
    "\n",
    "# Initialize a Tokenizer and fit on text data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='[OOV]')\n",
    "tokenizer.fit_on_texts(mlm_dataset_generator.getVocubulary())\n",
    "\n",
    "# check how many words are in the dataset (currently: 37032)\n",
    "# print(list(tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example with original Transformer hyperparameters\n",
    "num_layers = 1\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "input_vocab_size = 40000\n",
    "maximum_position_encoding = 10000\n",
    "rate = 0.1\n",
    "\n",
    "model = MLMTransformerEncoder(num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate)\n",
    "# dummy_input = [tf.keras.Input(shape=(None, None, 512)), tf.keras.Input(shape=(None, None, 512))]\n",
    "# model(dummy_input)\n",
    "\n",
    "# model_trainable_variables = []\n",
    "# gradients_test = []\n",
    "\n",
    "# model_trainable_variables.append(model.trainable_variables)\n",
    "\n",
    "# Define an optimizer (e.g., Adam)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define a loss function (e.g., categorical cross-entropy for classification)\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs, labels):\n",
    "    # create one-hot encoded mask and get the indices\n",
    "    mask =[[]]\n",
    "    token_indices = []\n",
    "    for index, token in enumerate(inputs):\n",
    "        if token == '[MASK]':\n",
    "            mask[0].append(0)\n",
    "            token_indices.append(index)\n",
    "        else: \n",
    "            mask[0].append(1)\n",
    "    mask = tf.constant(mask, tf.float32)\n",
    "    # tokenize inputs\n",
    "    input_ids = tf.cast(tokenizer.texts_to_sequences([inputs]), tf.float32)\n",
    "    # tokenize labels\n",
    "    token_ids = tokenizer.texts_to_sequences(labels)\n",
    "    # create array of zeroes with dimension [sequence_length, input_vocab_size]\n",
    "    tokenized_labels = np.zeros((len(inputs), input_vocab_size))\n",
    "    # change the [masked_token_index, token_id] to ones\n",
    "    for index, token_index in enumerate(token_indices):\n",
    "        tokenized_labels[token_index, token_ids[index]] = 1\n",
    "    tokenized_labels = tf.constant(tokenized_labels, dtype=tf.float32)\n",
    "\n",
    "    # print('\\n> INPUTS')\n",
    "    # print(input_ids)\n",
    "    # print(mask)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        predictions = model([input_ids, mask], training=False)[0]\n",
    "\n",
    "        # predictions = tf.nn.softmax(predictions)\n",
    "\n",
    "        # print('\\n> LABELS')\n",
    "        # print(tokenized_labels)\n",
    "        # print('\\n> PREDICTIONS')\n",
    "        # print(predictions)\n",
    "\n",
    "        loss = loss_function(tokenized_labels, predictions)\n",
    "\n",
    "        # print('\\n> LOSS')\n",
    "        # print(loss)\n",
    "\n",
    "    # indices = []\n",
    "    predicted_token = []\n",
    "    for index, row in enumerate(mask[0]):\n",
    "        if (row == 0):\n",
    "            predicted_token.append(np.argmax(predictions[index]))\n",
    "            # indices.append(index)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # print('GRADIENTS')\n",
    "    # print(gradients)\n",
    "\n",
    "    # gradients_test.append(gradients)\n",
    "    # model_trainable_variables.append(model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, predicted_token, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_trainable_variables[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gradients_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of usage in the training loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    loss, predicted_token, token_ids = train_step(inputs, labels)\n",
    "    # Log or print the loss for monitoring\n",
    "    print('Epoch ' + str(epoch) + ', Loss = ' + str(loss) + ', Predicted Token = ' + str(predicted_token) + ', True Token = ' + str(token_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
