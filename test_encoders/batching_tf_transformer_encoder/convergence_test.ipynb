{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from transformer_encoder import MLMTransformerEncoder\n",
    "from mlm_dataset.batching_mlm_dataset_generator import MLMDatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[MASK]', 'administr', 'market', 'associ'], ['hr', '[MASK]'], ['[MASK]', 'custom', '[MASK]', 'manag', 'with', '[number]', '[MASK]', 'of', 'experi', 'in', 'hospit', 'and', 'custom', 'servic', 'management'], ['respect', 'builder', '[MASK]', 'leader', 'of', '[MASK]', 'team', 'strive', 'to', 'instil', 'a', 'share', '[MASK]', 'commit', 'to', 'custom', 'service'], ['focus', '[MASK]', 'custom', 'satisfact'], ['team', '[MASK'], ['market', '[MASK]'], ['[MASK]', 'resolut', 'techniqu'], ['train', 'and', '[MASK]'], ['skill', '[MASK]'], ['client', 'relat', '[MASK]'], ['missouri', 'dot', 'supervisor', 'train', '[MASK]'], ['certifi', '[MASK]', 'ihg', 'in', 'custom', 'loyalti', 'and', 'market', '[MASK]', 'segment'], ['hilton', '[MASK]', 'general', 'manag', 'train', 'certif'], ['accomplish', 'trainer', 'for', 'cross', 'server', 'hospit', 'system', '[MASK]', '[MASK]'], ['[MASK]', 'onq'], ['opera', '[MASK'], ['reserv', '[MASK]', 'or'], ['complet', 'cours', 'and', 'seminar', 'in', 'custom', 'servic', 'sale', '[MASK]', '[MASK]', 'control', 'loss', '[MASK]', 'safeti', 'time', 'manag', 'leadership', 'and', 'perform', 'assessment'], ['hr', '[MASK]', 'market', 'associ']] [['hr'], ['administr'], ['dedic', 'servic', 'year'], ['and', 'customer-focus', 'enthusiast'], ['on'], ['manag'], ['savvi'], ['conflict'], ['develop'], ['multi-task'], ['specialist'], ['certif'], ['by', 'by'], ['worldwid'], ['such', 'as'], ['hilton'], ['pms'], ['system'], ['strategi', 'inventori', 'prevent'], ['administr']]\n"
     ]
    }
   ],
   "source": [
    "# MLM dataset for training\n",
    "mlm_dataset_generator = MLMDatasetGenerator('../../dataset/resume_dataset.csv')\n",
    "inputs, labels = mlm_dataset_generator.generateMLMDataset(1)[0]\n",
    "inputs = inputs[0]\n",
    "labels = labels[0]\n",
    "print(inputs, labels)\n",
    "\n",
    "# Initialize a Tokenizer and fit on text data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='[OOV]')\n",
    "tokenizer.fit_on_texts(mlm_dataset_generator.getVocubulary())\n",
    "\n",
    "# check how many words are in the dataset (currently: 37032)\n",
    "# print(list(tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24591\n"
     ]
    }
   ],
   "source": [
    "# check how many words are in the dataset (currently: 37032)\n",
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example with original Transformer hyperparameters\n",
    "num_layers = 1\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "input_vocab_size = 40000\n",
    "maximum_position_encoding = 10000\n",
    "rate = 0.1\n",
    "\n",
    "model = MLMTransformerEncoder(num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate)\n",
    "# dummy_input = [tf.keras.Input(shape=(None, None, 512)), tf.keras.Input(shape=(None, None, 512))]\n",
    "# model(dummy_input)\n",
    "\n",
    "# model_trainable_variables = []\n",
    "# gradients_test = []\n",
    "\n",
    "# model_trainable_variables.append(model.trainable_variables)\n",
    "\n",
    "# Define an optimizer (e.g., Adam)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define a loss function (e.g., categorical cross-entropy for classification)\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs, labels):\n",
    "    # create one-hot encoded mask and get the indices\n",
    "    mask =[[]]\n",
    "    token_indices = []\n",
    "    for index, token in enumerate(inputs):\n",
    "        if token == '[MASK]':\n",
    "            mask[0].append(0)\n",
    "            token_indices.append(index)\n",
    "        else: \n",
    "            mask[0].append(1)\n",
    "    mask = tf.constant(mask, tf.float32)\n",
    "    # tokenize inputs\n",
    "    input_ids = tf.cast(tokenizer.texts_to_sequences([inputs]), tf.float32)\n",
    "    # tokenize labels\n",
    "    token_ids = tokenizer.texts_to_sequences(labels)\n",
    "    # create array of zeroes with dimension [sequence_length, input_vocab_size]\n",
    "    tokenized_labels = np.zeros((len(inputs), input_vocab_size))\n",
    "    # change the [masked_token_index, token_id] to ones\n",
    "    for index, token_index in enumerate(token_indices):\n",
    "        tokenized_labels[token_index, token_ids[index]] = 1\n",
    "    tokenized_labels = tf.constant(tokenized_labels, dtype=tf.float32)\n",
    "\n",
    "    # print('\\n> INPUTS')\n",
    "    # print(input_ids)\n",
    "    # print(mask)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        predictions = model([input_ids, mask], training=False)[0]\n",
    "\n",
    "        # predictions = tf.nn.softmax(predictions)\n",
    "\n",
    "        # print('\\n> LABELS')\n",
    "        # print(tokenized_labels)\n",
    "        # print('\\n> PREDICTIONS')\n",
    "        # print(predictions)\n",
    "\n",
    "        loss = loss_function(tokenized_labels, predictions)\n",
    "\n",
    "        # print('\\n> LOSS')\n",
    "        # print(loss)\n",
    "\n",
    "    # indices = []\n",
    "    predicted_token = []\n",
    "    for index, row in enumerate(mask[0]):\n",
    "        if (row == 0):\n",
    "            predicted_token.append(np.argmax(predictions[index]))\n",
    "            # indices.append(index)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # print('GRADIENTS')\n",
    "    # print(gradients)\n",
    "\n",
    "    # gradients_test.append(gradients)\n",
    "    # model_trainable_variables.append(model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, predicted_token, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_trainable_variables[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gradients_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\test_encoders\\batching_tf_transformer_encoder\\convergence_test.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_tf_transformer_encoder/convergence_test.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_tf_transformer_encoder/convergence_test.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_tf_transformer_encoder/convergence_test.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     loss, predicted_token, token_ids \u001b[39m=\u001b[39m train_step(inputs, labels)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_tf_transformer_encoder/convergence_test.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Log or print the loss for monitoring\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_tf_transformer_encoder/convergence_test.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(epoch) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, Loss = \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(loss) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, Predicted Token = \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(predicted_token) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, True Token = \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(token_ids))\n",
      "\u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\test_encoders\\batching_tf_transformer_encoder\\convergence_test.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_tf_transformer_encoder/convergence_test.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m mask \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant(mask, tf\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_tf_transformer_encoder/convergence_test.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# tokenize inputs\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_tf_transformer_encoder/convergence_test.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(tokenizer\u001b[39m.\u001b[39;49mtexts_to_sequences([inputs]), tf\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_tf_transformer_encoder/convergence_test.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# tokenize labels\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/test_encoders/batching_tf_transformer_encoder/convergence_test.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m token_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(labels)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\preprocessing\\text.py:357\u001b[0m, in \u001b[0;36mTokenizer.texts_to_sequences\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtexts_to_sequences\u001b[39m(\u001b[39mself\u001b[39m, texts):\n\u001b[0;32m    346\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Transforms each text in texts to a sequence of integers.\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \n\u001b[0;32m    348\u001b[0m \u001b[39m    Only top `num_words-1` most frequent words will be taken into account.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[39m        A list of sequences.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtexts_to_sequences_generator(texts))\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\preprocessing\\text.py:380\u001b[0m, in \u001b[0;36mTokenizer.texts_to_sequences_generator\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlower:\n\u001b[0;32m    379\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mlist\u001b[39m):\n\u001b[1;32m--> 380\u001b[0m         text \u001b[39m=\u001b[39m [text_elem\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m text_elem \u001b[39min\u001b[39;00m text]\n\u001b[0;32m    381\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    382\u001b[0m         text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlower()\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\keras\\src\\preprocessing\\text.py:380\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlower:\n\u001b[0;32m    379\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mlist\u001b[39m):\n\u001b[1;32m--> 380\u001b[0m         text \u001b[39m=\u001b[39m [text_elem\u001b[39m.\u001b[39;49mlower() \u001b[39mfor\u001b[39;00m text_elem \u001b[39min\u001b[39;00m text]\n\u001b[0;32m    381\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    382\u001b[0m         text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlower()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Example of usage in the training loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    loss, predicted_token, token_ids = train_step(inputs, labels)\n",
    "    # Log or print the loss for monitoring\n",
    "    print('Epoch ' + str(epoch) + ', Loss = ' + str(loss) + ', Predicted Token = ' + str(predicted_token) + ', True Token = ' + str(token_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
