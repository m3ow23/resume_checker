{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from transformer_encoder import MLMTransformerEncoder\n",
    "from mlm_dataset.batching_mlm_dataset_generator import MLMDatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embedding: 0.1036980152130127\n",
      "sqrt: 0.0\n",
      "pos_encoding: 0.0\n",
      "mha: 0.055165767669677734\n",
      "add & norm: 0.026170015335083008\n",
      "ffn: 0.03734254837036133\n",
      "add & norm: 0.012549161911010742\n",
      "mlm_head: 0.020002365112304688\n"
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "# default is None\n",
    "sample_limit = 12\n",
    "\n",
    "# Usage example with original Transformer hyperparameters\n",
    "num_layers = 1\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "input_vocab_size = 22733\n",
    "maximum_position_encoding = 512\n",
    "\n",
    "if (d_model % num_heads != 0):\n",
    "    raise ValueError(f'd_model has to be divisible by num_heads, d_model = {str(d_model)}, num_heads = {str(num_heads)}')\n",
    "\n",
    "model = MLMTransformerEncoder(num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding)\n",
    "model([tf.keras.Input(shape=(maximum_position_encoding,), dtype=tf.int64), \n",
    "       tf.keras.Input(shape=(maximum_position_encoding,), dtype=tf.int64)], \n",
    "       training=False)\n",
    "\n",
    "\n",
    "# Define an optimizer (e.g., Adam)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define a loss function (e.g., categorical cross-entropy for classification)\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM dataset for training\n",
    "mlm_dataset_generator = MLMDatasetGenerator(512)\n",
    "\n",
    "# training_data, validation_data, testing_data = mlm_dataset_generator.generateMLMDataset(batch_size, sample_limit=sample_limit)\n",
    "training_data, validation_data, testing_data = mlm_dataset_generator.read_mlm_dataset_from_file(batch_size=batch_size, sample_limit=sample_limit)\n",
    "mlm_dataset_generator.read_raw_training_data_from_file()\n",
    "\n",
    "# Initialize a Tokenizer and fit on text data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=0, filters='')\n",
    "# Traing tokenizer on dataset vocab\n",
    "tokenizer.fit_on_texts(mlm_dataset_generator.getVocubulary())\n",
    "\n",
    "# padding function\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# to free memory\n",
    "mlm_dataset_generator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch: tuple, input_vocab_size) -> ((tf.constant([], tf.float32), tf.constant([], tf.float32)), []):\n",
    "    tokens_batch = batch[::2]\n",
    "    labels_batch = batch[1::2]\n",
    "    \n",
    "    # tokenize the tokens_batch and labels_batch, string to token_id conversion\n",
    "    tokenized_tokens_batch = tokenizer.texts_to_sequences(tokens_batch)\n",
    "    tokenized_labels_batch = tokenizer.texts_to_sequences(labels_batch)\n",
    "\n",
    "    # apply padding on tokens\n",
    "    # we used -1 as padding to allow for reversing of the attention mask\n",
    "    padded_tokenized_tokens_batch = pad_sequences(tokenized_tokens_batch, maxlen=maximum_position_encoding, padding='post', value=-1)\n",
    "\n",
    "    # create the attention mask\n",
    "    attention_mask = np.array(padded_tokenized_tokens_batch)\n",
    "    # change padding tokens -1 to token ID according to tokenizer\n",
    "    padded_tokenized_tokens_batch[padded_tokenized_tokens_batch == -1] = tokenizer.word_index['[pad]']\n",
    "    # get indices of mask tokens\n",
    "    mask_token_indices = np.where(attention_mask == tokenizer.word_index['[mask]'])\n",
    "    # change mask tokens to 0\n",
    "    attention_mask[mask_token_indices[0], mask_token_indices[1]] = 0\n",
    "    # change non-masked tokens to 1\n",
    "    attention_mask[attention_mask > 0] = 1\n",
    "    # change padding tokens -1 to attention of 0\n",
    "    attention_mask[attention_mask == -1] = 0\n",
    "    attention_mask = tf.expand_dims(attention_mask, axis=-1)\n",
    "\n",
    "    # create labels\n",
    "    labels = []\n",
    "    for sequence_labels in tokenized_labels_batch:\n",
    "        labels += np.eye(input_vocab_size)[sequence_labels].tolist()\n",
    "\n",
    "    return (tf.constant(padded_tokenized_tokens_batch, tf.float32), \n",
    "            tf.cast(attention_mask, tf.float32), \n",
    "            tf.cast(labels, tf.float32),\n",
    "            mask_token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define a training loop\n",
    "def train_step(batch, input_vocab_size):\n",
    "    start_time = time.time()\n",
    "\n",
    "    padded_tokenized_tokens_batch, attention_mask, labels, mask_token_indices = tokenize(batch, input_vocab_size)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([padded_tokenized_tokens_batch, attention_mask], training=False)\n",
    "\n",
    "        loss = loss_function(labels, tf.gather_nd(predictions, indices=tf.stack([mask_token_indices[0], mask_token_indices[1]], axis=-1)))\n",
    "\n",
    "    sub_start_time = time.time()\n",
    "\n",
    "    # takes longest time\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    print('gradients_calc:', time.time() - sub_start_time)\n",
    "    sub_start_time = time.time()\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    print('optimization:', time.time() - sub_start_time)\n",
    "\n",
    "    return loss , str(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embedding: 0.021105289459228516\n",
      "sqrt: 0.009525060653686523\n",
      "pos_encoding: 0.004992485046386719\n",
      "mha: 0.5606846809387207\n",
      "add & norm: 0.04333615303039551\n",
      "ffn: 0.27907586097717285\n",
      "add & norm: 0.029526948928833008\n",
      "mlm_head: 1.780632495880127\n",
      "gradients_calc: 6.063301086425781\n",
      "optimization: 0.6320910453796387\n",
      "Epoch 0, Batch 0, Loss = 10.199326, Elapsed Time: 10.82668161392212\n",
      "word_embedding: 0.006000518798828125\n",
      "sqrt: 0.0020051002502441406\n",
      "pos_encoding: 0.005000591278076172\n",
      "mha: 0.46110987663269043\n",
      "add & norm: 0.04052257537841797\n",
      "ffn: 0.2876865863800049\n",
      "add & norm: 0.028002262115478516\n",
      "mlm_head: 1.637077808380127\n",
      "gradients_calc: 4.704793930053711\n",
      "optimization: 0.5159957408905029\n",
      "Epoch 1, Batch 0, Loss = 8.716139, Elapsed Time: 8.577103614807129\n",
      "word_embedding: 0.0\n",
      "sqrt: 0.008115530014038086\n",
      "pos_encoding: 0.0020449161529541016\n",
      "mha: 0.41971778869628906\n",
      "add & norm: 0.03000640869140625\n",
      "ffn: 0.24251890182495117\n",
      "add & norm: 0.025007009506225586\n",
      "mlm_head: 1.485877513885498\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_index, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(training_data):  \u001b[38;5;66;03m# Provide training data\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m         loss, elapsed_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_vocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# Log or print the loss for monitoring\u001b[39;00m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(batch_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, Loss = \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(loss\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, Elapsed Time: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m elapsed_time)\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(batch, input_vocab_size)\u001b[0m\n\u001b[0;32m     14\u001b[0m sub_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# takes longest time\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradients_calc:\u001b[39m\u001b[38;5;124m'\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m sub_start_time)\n\u001b[0;32m     20\u001b[0m sub_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1065\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1059\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1060\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1061\u001b[0m           output_gradients))\n\u001b[0;32m   1062\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1063\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1065\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[0;32m   1074\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:147\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    145\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\linalg_grad.py:344\u001b[0m, in \u001b[0;36m_EinsumGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    342\u001b[0m y_shape \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mshape(y)\n\u001b[0;32m    343\u001b[0m grad_x \u001b[38;5;241m=\u001b[39m _GetGradWrt(grad, y, x_shape, x_subs, y_subs, output_subs)\n\u001b[1;32m--> 344\u001b[0m grad_y \u001b[38;5;241m=\u001b[39m \u001b[43m_GetGradWrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_subs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_subs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_subs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ellipsis \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m output_subs:\n\u001b[0;32m    347\u001b[0m   \u001b[38;5;66;03m# If no ellipsis in the output; then no need to unbroadcast.\u001b[39;00m\n\u001b[0;32m    348\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_x, grad_y\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\linalg_grad.py:284\u001b[0m, in \u001b[0;36m_EinsumGrad.<locals>._GetGradWrt\u001b[1;34m(output_grad, other_operand, input_shape, input_subs, other_subs, output_subs)\u001b[0m\n\u001b[0;32m    280\u001b[0m left_subs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m input_subs \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m reduced_label_set)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# Compute the gradient wrt the input, without accounting for the operation\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# \"abc->ac\". So, now we have the VJP of the operation \"ac,cd->ad\".\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m grad_reduced \u001b[38;5;241m=\u001b[39m \u001b[43mgen_linalg_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_operand\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m                                     \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m->\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43moutput_subs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_subs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mleft_subs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# If the reduced_label_set is empty, then we already have the gradient\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# wrt the input.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reduced_label_set:\n",
      "File \u001b[1;32md:\\Applications\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_linalg_ops.py:1216\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(inputs, equation, name)\u001b[0m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   1215\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1216\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEinsum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mequation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mequation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   1219\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example of usage in the training loop\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_index, batch in enumerate(training_data):  # Provide training data\n",
    "        loss, elapsed_time = train_step(batch, input_vocab_size)\n",
    "        # Log or print the loss for monitoring\n",
    "        print('Epoch ' + str(epoch) + ', Batch ' + str(batch_index) + ', Loss = ' + str(loss.numpy()) + ', Elapsed Time: ' + elapsed_time)\n",
    "    checkpoint.save(f\"model_training_checkpoints/epoch_{str(epoch)}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loading\n",
    "\n",
    "# loaded_checkpoint = tf.train.Checkpoint(model=MLMTransformerEncoder(num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding), \n",
    "#                                         optimizer=tf.keras.optimizers.Adam())\n",
    "# loaded_checkpoint.restore(tf.train.latest_checkpoint(\"model_training_checkpoints/\"))\n",
    "# loaded_model = loaded_checkpoint.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_trainable_variables[0])\n",
    "# print(model_gradients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_trainable_variables[8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
