{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from transformer_encoder import MLMTransformerEncoder\n",
    "from mlm_dataset.batching_mlm_dataset_generator import MLMDatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embedding: 0.10940909385681152\n",
      "sqrt: 0.015629053115844727\n",
      "pos_encoding: 0.0\n",
      "mha: 0.24380254745483398\n",
      "add & norm: 0.015630006790161133\n",
      "ffn: 0.046889305114746094\n",
      "add & norm: 0.01562976837158203\n",
      "mlm_head: 0.015630722045898438\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "# default is None\n",
    "sample_limit = 32\n",
    "\n",
    "# Usage example with original Transformer hyperparameters\n",
    "num_layers = 1\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "input_vocab_size = 22733\n",
    "maximum_position_encoding = 512\n",
    "\n",
    "model = MLMTransformerEncoder(num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding)\n",
    "model([tf.keras.Input(shape=(maximum_position_encoding,), dtype=tf.int64), \n",
    "       tf.keras.Input(shape=(maximum_position_encoding,), dtype=tf.int64)], \n",
    "       training=False)\n",
    "\n",
    "\n",
    "# Define an optimizer (e.g., Adam)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define a loss function (e.g., categorical cross-entropy for classification)\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM dataset for training\n",
    "mlm_dataset_generator = MLMDatasetGenerator(512)\n",
    "\n",
    "# training_data, validation_data, testing_data = mlm_dataset_generator.generateMLMDataset(batch_size, sample_limit=sample_limit)\n",
    "training_data, validation_data, testing_data = mlm_dataset_generator.read_mlm_dataset_from_file(batch_size=batch_size, sample_limit=sample_limit)\n",
    "mlm_dataset_generator.read_raw_training_data_from_file()\n",
    "\n",
    "# Initialize a Tokenizer and fit on text data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=0, filters='')\n",
    "# Traing tokenizer on dataset vocab\n",
    "tokenizer.fit_on_texts(mlm_dataset_generator.getVocubulary())\n",
    "\n",
    "# padding function\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# to free memory\n",
    "mlm_dataset_generator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch: tuple, input_vocab_size) -> ((tf.constant([], tf.float32), tf.constant([], tf.float32)), []):\n",
    "    tokens_batch = batch[::2]\n",
    "    labels_batch = batch[1::2]\n",
    "    \n",
    "    # tokenize the tokens_batch and labels_batch, string to token_id conversion\n",
    "    tokenized_tokens_batch = tokenizer.texts_to_sequences(tokens_batch)\n",
    "    tokenized_labels_batch = tokenizer.texts_to_sequences(labels_batch)\n",
    "\n",
    "    # apply padding on tokens\n",
    "    # we used -1 as padding to allow for reversing of the attention mask\n",
    "    padded_tokenized_tokens_batch = pad_sequences(tokenized_tokens_batch, maxlen=maximum_position_encoding, padding='post', value=-1)\n",
    "\n",
    "    # create the attention mask\n",
    "    attention_mask = np.array(padded_tokenized_tokens_batch)\n",
    "    # change padding tokens -1 to token ID according to tokenizer\n",
    "    padded_tokenized_tokens_batch[padded_tokenized_tokens_batch == -1] = tokenizer.word_index['[pad]']\n",
    "    # get indices of mask tokens\n",
    "    mask_token_indices = np.where(attention_mask == tokenizer.word_index['[mask]'])\n",
    "    # change mask tokens to 0\n",
    "    attention_mask[mask_token_indices[0], mask_token_indices[1]] = 0\n",
    "    # change non-masked tokens to 1\n",
    "    attention_mask[attention_mask > 0] = 1\n",
    "    # change padding tokens -1 to attention of 0\n",
    "    attention_mask[attention_mask == -1] = 0\n",
    "    attention_mask = tf.expand_dims(attention_mask, axis=-1)\n",
    "\n",
    "    # create labels\n",
    "    labels = []\n",
    "    for sequence_labels in tokenized_labels_batch:\n",
    "        labels += np.eye(input_vocab_size)[sequence_labels].tolist()\n",
    "\n",
    "    return (tf.constant(padded_tokenized_tokens_batch, tf.float32), \n",
    "            tf.cast(attention_mask, tf.float32), \n",
    "            tf.cast(labels, tf.float32),\n",
    "            mask_token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define a training loop\n",
    "def train_step(batch, input_vocab_size):\n",
    "    start_time = time.time()\n",
    "\n",
    "    padded_tokenized_tokens_batch, attention_mask, labels, mask_token_indices = tokenize(batch, input_vocab_size)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([padded_tokenized_tokens_batch, attention_mask], training=False)\n",
    "\n",
    "        loss = loss_function(labels, tf.gather_nd(predictions, indices=tf.stack([mask_token_indices[0], mask_token_indices[1]], axis=-1)))\n",
    "\n",
    "    sub_start_time = time.time()\n",
    "\n",
    "    # takes longest time\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    print('gradients_calc:', time.time() - sub_start_time)\n",
    "    sub_start_time = time.time()\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    print('optimization:', time.time() - sub_start_time)\n",
    "\n",
    "    return loss , str(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embedding: 0.005279541015625\n",
      "sqrt: 0.01170206069946289\n",
      "pos_encoding: 0.008004426956176758\n",
      "mha: 1.025252342224121\n",
      "add & norm: 0.07317113876342773\n",
      "ffn: 0.5417253971099854\n",
      "add & norm: 0.050679683685302734\n",
      "mlm_head: 11.711661338806152\n",
      "48\n",
      "gradients_calc: 17.975805282592773\n",
      "optimization: 0.7185008525848389\n",
      "Epoch 0, Batch 0, Loss = 8.912773, Elapsed Time: 34.53455710411072\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Example of usage in the training loop\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_index, batch in enumerate(training_data):  # Provide training data\n",
    "        loss, elapsed_time = train_step(batch, input_vocab_size)\n",
    "        # Log or print the loss for monitoring\n",
    "        print('Epoch ' + str(epoch) + ', Batch ' + str(batch_index) + ', Loss = ' + str(loss.numpy()) + ', Elapsed Time: ' + elapsed_time)\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_trainable_variables[0])\n",
    "# print(model_gradients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_trainable_variables[8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
