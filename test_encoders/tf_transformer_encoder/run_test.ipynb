{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from transformer_encoder import MLMTransformerEncoder\n",
    "from mlm_dataset.mlm_dataset_generator import MLMDatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "# MLM dataset for training\n",
    "mlm_dataset_generator = MLMDatasetGenerator('../../dataset/resume_dataset.csv')\n",
    "mlm_dataset = [mlm_dataset_generator.generateMLMDataset(batch_size)[0]]\n",
    "\n",
    "oov_token = '[OOV]'\n",
    "\n",
    "# Initialize a Tokenizer and fit on text data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token, filters='')\n",
    "tokenizer.fit_on_texts(mlm_dataset_generator.getVocubulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many words are in the dataset (currently: 37032)\n",
    "# print(list(tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM dataset checker\n",
    "# inputs, labels = mlm_dataset[0]\n",
    "\n",
    "# print(inputs[121], labels[121])\n",
    "# print(inputs[122], labels[122])\n",
    "\n",
    "# for index, row in enumerate(inputs):\n",
    "#     if(row.count('[MASK]') != len(labels[index])):\n",
    "#         print(index, row, labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example with original Transformer hyperparameters\n",
    "num_layers = 2\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "input_vocab_size = 40000\n",
    "maximum_position_encoding = 10000\n",
    "\n",
    "model = MLMTransformerEncoder(num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding)\n",
    "dummy_input = [tf.keras.Input(shape=(None,)), tf.keras.Input(shape=(None,))]\n",
    "model(dummy_input)\n",
    "\n",
    "# Define an optimizer (e.g., Adam)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Define a loss function (e.g., categorical cross-entropy for classification)\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# model_trainable_variables = []\n",
    "# model_gradients = []\n",
    "\n",
    "# Define a training loop\n",
    "def train_step(inputs_batch, labels_batch):\n",
    "    start_time = time.time()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    gradients_accumulator = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "\n",
    "    # model_trainable_variables.append([v.numpy() for v in model.trainable_variables])\n",
    "\n",
    "    counter = 0\n",
    "    for inputs, labels in zip(inputs_batch, labels_batch):\n",
    "        # create one-hot encoded mask and get the indices\n",
    "        mask =[[]]\n",
    "        token_indices = []\n",
    "        for index, token in enumerate(inputs):\n",
    "            if token == '[MASK]':\n",
    "                mask[0].append(0)\n",
    "                token_indices.append(index)\n",
    "            else: \n",
    "                mask[0].append(1)\n",
    "        mask = tf.constant(mask, tf.float32)\n",
    "        # tokenize inputs\n",
    "        input_ids = tf.cast(tokenizer.texts_to_sequences([inputs]), tf.float32)\n",
    "        # tokenize labels\n",
    "        token_ids = tokenizer.texts_to_sequences(labels)\n",
    "        # create array of zeroes with dimension [sequence_length, input_vocab_size]\n",
    "        tokenized_labels = np.zeros((len(inputs), input_vocab_size))\n",
    "        # change the [masked_token_index, token_id] to ones\n",
    "        for index, token_index in enumerate(token_indices):\n",
    "            tokenized_labels[token_index, token_ids[index]] = 1\n",
    "        tokenized_labels = tf.constant(tokenized_labels, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model([input_ids, mask], training=False)[0]\n",
    "\n",
    "            loss = loss_function(tokenized_labels, predictions)\n",
    "            # print('\\n> LOSS')\n",
    "            # print(loss)\n",
    "\n",
    "        # get the predicted token(s) ID(s)\n",
    "        # indices = []\n",
    "        predicted_token = []\n",
    "        for index, row in enumerate(mask[0]):\n",
    "            if (row == 0):\n",
    "                predicted_token.append(np.argmax(predictions[index]))\n",
    "                # indices.append(index)\n",
    "\n",
    "        # if (counter == 9):\n",
    "        #     print(inputs, labels, mask, token_indices, token_ids)\n",
    "        #     # print('\\n> LABELS')\n",
    "        #     # print(tokenized_labels)\n",
    "        #     print('\\n> PREDICTIONS')\n",
    "        #     print(predictions)\n",
    "\n",
    "        #     # display the token index and element index of values > 0\n",
    "        #     for index, row in enumerate(tokenized_labels):\n",
    "        #         for element_index, element in enumerate(row):\n",
    "        #             if (element > 0):\n",
    "        #                 print(index, element, element_index)\n",
    "\n",
    "        # Manual Loss calculation\n",
    "        # total_loss_test = 0\n",
    "        # for tokenized_label, prediction in zip(tokenized_labels, predictions):\n",
    "        #     total_loss_test += np.sum(tokenized_label * -np.log(prediction))\n",
    "        # print(\"manual:\", total_loss_test / len(predictions))\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        # print('GRADIENTS')\n",
    "        # print(gradients)\n",
    "\n",
    "        # model_gradients.append(gradients)\n",
    "\n",
    "        gradients_accumulator = [grad_accum + grad for grad_accum, grad in zip(gradients_accumulator, gradients)]\n",
    "        total_loss += loss\n",
    "\n",
    "        print('Seq ' + str(counter) + ', Loss = ' + str(loss.numpy()) + ', Predicted Token = ' + str(predicted_token) + ', True Token = ' + str(token_ids))\n",
    "        counter += 1\n",
    "\n",
    "    gradients_avg = [grad / len(inputs_batch) for grad in gradients_accumulator]\n",
    "    \n",
    "    # optimizer.minimize(total_loss / len(inputs_batch), model.trainable_variables, tape=tape)\n",
    "    optimizer.apply_gradients(zip(gradients_avg, model.trainable_variables))\n",
    "\n",
    "    return total_loss / len(inputs_batch), str(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of usage in the training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    batch_counter = 0\n",
    "    for inputs_batch, labels_batch in mlm_dataset:  # Provide training data\n",
    "        loss, elapsed_time = train_step(inputs_batch, labels_batch)\n",
    "        # Log or print the loss for monitoring\n",
    "        print('Epoch ' + str(epoch) + ', Batch ' + str(batch_counter) + ', Loss = ' + str(loss.numpy()) + ', Elapsed Time: ' + elapsed_time + '\\n')\n",
    "        batch_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_trainable_variables[0])\n",
    "# print(model_gradients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_trainable_variables[8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
