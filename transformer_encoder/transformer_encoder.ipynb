{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from tokenizer import Tokenizer\n",
    "from mlm_dataset.mlm_dataset_generator import MLMDatasetGenerator\n",
    "\n",
    "# hyperparameters\n",
    "vocab_size = 22732\n",
    "model_dim = 512\n",
    "num_heads = 8\n",
    "ffn_dim = 2048\n",
    "max_pos = 512\n",
    "\n",
    "# # test hyperparameters\n",
    "# vocab_size = 22732\n",
    "# model_dim = 70\n",
    "# num_heads = 2\n",
    "# ffn_dim = 2048\n",
    "# max_pos = 10\n",
    "\n",
    "# MLM dataset for training\n",
    "mlm_dataset_generator = MLMDatasetGenerator(max_pos=max_pos)\n",
    "\n",
    "mlm_dataset = mlm_dataset_generator.read_mlm_dataset_from_file()\n",
    "mlm_dataset_generator.read_raw_training_data_from_file()\n",
    "\n",
    "tokenizer = Tokenizer(max_pos=max_pos, vocab_size=vocab_size)\n",
    "\n",
    "# fit tokenizer on dataset\n",
    "tokenizer.fit_on_texts(mlm_dataset_generator.getVocubulary())\n",
    "\n",
    "# generate MLM dataset\n",
    "\n",
    "batch_size = 20\n",
    "sample_limit = 1000\n",
    "\n",
    "# to free memory\n",
    "del mlm_dataset_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22732\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from base import Sequential\n",
    "from layers import WordEmbedding, PositionalEncoding, Dense, MultiHeadAttention, SelfAttention, LayerNormalization\n",
    "from activation import ReLu, Linear, Softmax\n",
    "from loss import CategoricalCrossEntropy\n",
    "from optimizers import Adam, GradientDescent\n",
    "\n",
    "model = Sequential([\n",
    "\t\t\tWordEmbedding(vocab_size, model_dim),\n",
    "\t\t\tPositionalEncoding(max_pos, model_dim),\n",
    "\t\t\tMultiHeadAttention(num_heads, max_pos, model_dim),\n",
    "\t\t\t# SelfAttention(max_pos, model_dim),\n",
    "\t\t\tLayerNormalization(model_dim),\n",
    "\t\t\t# Feed Forward Network\n",
    "\t\t\tDense([model_dim, ffn_dim], ReLu),\n",
    "\t\t\tDense([ffn_dim, model_dim], Linear),\n",
    "\t\t\tLayerNormalization(model_dim),\n",
    "\t\t\t# MLM Head\n",
    "\t\t\tDense([model_dim, vocab_size], Softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example truncation handling\n",
    "\n",
    "# sample_text = 'I am an example resume. Tae tae tae, meow mewo mewo. Cats dogs relationships. I am an example resume. Tae tae tae, meow mewo mewo. Cats dogs relationships.'\n",
    "\n",
    "# padded_tokenized_training_tokens, training_attention_mask = tokenizer.clean_truncate_tokenize_pad_atten(sample_text)\n",
    "\n",
    "# print(padded_tokenized_training_tokens)\n",
    "# print(training_attention_mask)\n",
    "\n",
    "# print(model.predict(padded_tokenized_training_tokens, training_attention_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, testing_data = mlm_dataset\n",
    "\n",
    "sample_limit = None\n",
    "\n",
    "training_tokens = training_data[:sample_limit * 2 if sample_limit else None:2]\n",
    "training_labels = training_data[1:sample_limit * 2 if sample_limit else None:2]\n",
    "\n",
    "# tokenization, padding, attention mask\n",
    "padded_tokenized_training_tokens, training_attention_mask = tokenizer.tokenize_pad_atten(tokens=training_tokens)\n",
    "\n",
    "# MLM training mask\n",
    "training_mlm_mask = tokenizer.generate_mlm_mask(training_attention_mask)\n",
    "\n",
    "# change padding tokens to 0\n",
    "training_attention_mask[training_attention_mask == -1] = 0\n",
    "padded_tokenized_training_tokens = np.array(padded_tokenized_training_tokens)\n",
    "padded_tokenized_training_tokens[padded_tokenized_training_tokens == -1] = tokenizer.get_pad_token_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(Y, Y_hat):\n",
    "    Y, Y_hat = Y.T, Y_hat.T\n",
    "\n",
    "    rows, columns = np.where(Y == 1)\n",
    "    correct_predictions = sum(1 for actual, predicted in zip(columns, np.argmax(Y_hat[rows], axis=-1)) if actual == predicted)\n",
    "    return correct_predictions / len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load latest model save\n",
    "# model.log('.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Seq 0, Loss: 10.032693566967852, Accuracy: 0.0, Elapsed Time: 15.009294271469116\n",
      "Model Saved, Elapsed Time: 29.796020030975342\n",
      "Epoch 0, Seq 1, Loss: 10.03220426576604, Accuracy: 0.0, Elapsed Time: 15.532159805297852\n",
      "Epoch 0, Seq 2, Loss: 30.09458042366779, Accuracy: 0.0, Elapsed Time: 15.477685689926147\n",
      "Epoch 0, Seq 3, Loss: 30.094297649931548, Accuracy: 0.0, Elapsed Time: 15.299492835998535\n",
      "Epoch 0, Seq 4, Loss: 10.030507326706617, Accuracy: 0.0, Elapsed Time: 15.364421844482422\n",
      "Epoch 0, Seq 5, Loss: 10.031559027264011, Accuracy: 0.0, Elapsed Time: 15.53899884223938\n",
      "Epoch 0, Seq 6, Loss: 10.030914027647146, Accuracy: 0.0, Elapsed Time: 15.517599105834961\n",
      "Epoch 0, Seq 7, Loss: 10.033554267023806, Accuracy: 0.0, Elapsed Time: 15.408987998962402\n",
      "Epoch 0, Seq 8, Loss: 10.031110469339412, Accuracy: 0.0, Elapsed Time: 15.798527240753174\n",
      "Epoch 0, Seq 9, Loss: 10.031337594835554, Accuracy: 0.0, Elapsed Time: 14.635864019393921\n",
      "Epoch 0, Seq 10, Loss: 10.032236803236762, Accuracy: 0.0, Elapsed Time: 14.894052028656006\n",
      "Model Saved, Elapsed Time: 27.6631817817688\n",
      "Epoch 0, Seq 11, Loss: 9.930230985697294, Accuracy: 0.0, Elapsed Time: 15.259523391723633\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\transformer_encoder.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m row, column \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(masked_token_indices, labels):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     one_hot_labels[row, column] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m training_log \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mpadded_tokenized, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m             attention_mask\u001b[39m=\u001b[39;49mattention_mask, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m             mlm_mask\u001b[39m=\u001b[39;49mmlm_mask, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m             Y\u001b[39m=\u001b[39;49mone_hot_labels, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m             loss_function\u001b[39m=\u001b[39;49mCategoricalCrossEntropy, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m             optimizer\u001b[39m=\u001b[39;49mGradientDescent(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X10sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m             accuracy_metric\u001b[39m=\u001b[39;49maccuracy_metric)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(epoch_count) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, Seq \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(sequence_count) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m training_log)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X10sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mif\u001b[39;00m (sequence_count \u001b[39m%\u001b[39m checkpoint_count \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "File \u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\base.py:77\u001b[0m, in \u001b[0;36mSequential.fit\u001b[1;34m(self, X, attention_mask, mlm_mask, Y, loss_function, optimizer, accuracy_metric)\u001b[0m\n\u001b[0;32m     75\u001b[0m dY \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[layer_count \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mbackward(np\u001b[39m.\u001b[39marray([]), Y, mlm_masked_Y_hat, loss_function)\n\u001b[0;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m((layer_count \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> 77\u001b[0m     dY \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers[j]\u001b[39m.\u001b[39;49mbackward(dY)\n\u001b[0;32m     79\u001b[0m \u001b[39m# logger\u001b[39;00m\n\u001b[0;32m     80\u001b[0m accuracy_log \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\layers.py:371\u001b[0m, in \u001b[0;36mMultiHeadAttention.backward\u001b[1;34m(self, dY, Y, Y_hat, loss_function)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39m# Backward pass through each attention head\u001b[39;00m\n\u001b[0;32m    369\u001b[0m dY_splits \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39msplit(dY, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[1;32m--> 371\u001b[0m gradients \u001b[39m=\u001b[39m [head\u001b[39m.\u001b[39mbackward(dY_split, Y, Y_hat, loss_function) \u001b[39mfor\u001b[39;00m head, dY_split \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_heads, dY_splits)]\n\u001b[0;32m    373\u001b[0m \u001b[39m# Sum the gradients from each attention head\u001b[39;00m\n\u001b[0;32m    374\u001b[0m total_gradient \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(gradients, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\layers.py:371\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39m# Backward pass through each attention head\u001b[39;00m\n\u001b[0;32m    369\u001b[0m dY_splits \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39msplit(dY, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[1;32m--> 371\u001b[0m gradients \u001b[39m=\u001b[39m [head\u001b[39m.\u001b[39;49mbackward(dY_split, Y, Y_hat, loss_function) \u001b[39mfor\u001b[39;00m head, dY_split \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_heads, dY_splits)]\n\u001b[0;32m    373\u001b[0m \u001b[39m# Sum the gradients from each attention head\u001b[39;00m\n\u001b[0;32m    374\u001b[0m total_gradient \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(gradients, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\layers.py:297\u001b[0m, in \u001b[0;36mSelfAttention.backward\u001b[1;34m(self, dY, Y, Y_hat, loss_function)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, dY, Y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, Y_hat\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, loss_function\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    296\u001b[0m     dA, dV \u001b[39m=\u001b[39m MatMul\u001b[39m.\u001b[39mbackward(dY\u001b[39m.\u001b[39mT, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_score, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_val)\n\u001b[1;32m--> 297\u001b[0m     dA \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdimension \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__attention_derivative(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_score, dA)\n\u001b[0;32m    298\u001b[0m     dQ, dK \u001b[39m=\u001b[39m MatMul\u001b[39m.\u001b[39mbackward(dA, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_val, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_val\u001b[39m.\u001b[39mT)\n\u001b[0;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery\u001b[39m.\u001b[39mbackward(dQ\u001b[39m.\u001b[39mT) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey\u001b[39m.\u001b[39mbackward(dK) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mbackward(dV\u001b[39m.\u001b[39mT)\n",
      "File \u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\layers.py:269\u001b[0m, in \u001b[0;36mSelfAttention.__attention_derivative\u001b[1;34m(attention, dY)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(attention)):\n\u001b[0;32m    268\u001b[0m     dS \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(attention[i], (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m--> 269\u001b[0m     dS \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdiagflat(dS) \u001b[39m-\u001b[39;49m np\u001b[39m.\u001b[39;49mdot(dS, dS\u001b[39m.\u001b[39;49mT)\n\u001b[0;32m    270\u001b[0m     dY_hat\u001b[39m.\u001b[39mappend(dY[i]\u001b[39m.\u001b[39mdot(dS))\n\u001b[0;32m    272\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(dY_hat)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenized_training_labels = tokenizer.tokenize(training_labels)\n",
    "\n",
    "epoch = 20\n",
    "# save model every n\n",
    "checkpoint_count = 10 \n",
    "\n",
    "# load training checkpoint\n",
    "sequence_start = 0\n",
    "with open('training_checkpoint.txt', 'r') as file:\n",
    "    sequence_start = int(file.read())\n",
    "\n",
    "import time\n",
    "\n",
    "for epoch_count in range(epoch):\n",
    "    sequence_count = 0\n",
    "    for padded_tokenized, attention_mask, mlm_mask, labels in zip(padded_tokenized_training_tokens[sequence_start:], \n",
    "                                                                  training_attention_mask[sequence_start:], \n",
    "                                                                  training_mlm_mask[sequence_start:], \n",
    "                                                                  tokenized_training_labels[sequence_start:]):\n",
    "        \"\"\"\n",
    "            Creation of labels. Labels should be created per input to reduce memory usage.\n",
    "            Labels has a shape of (vocab_size, d_model) which is a very large data.\n",
    "        \"\"\"\n",
    "        one_hot_labels = np.zeros((max_pos, vocab_size), dtype=np.float64)\n",
    "        masked_token_indices = np.where(mlm_mask.flatten() == 1)[0].tolist()\n",
    "        for row, column in zip(masked_token_indices, labels):\n",
    "            one_hot_labels[row, column] = 1\n",
    "\n",
    "        training_log = model.fit(X=padded_tokenized, \n",
    "                    attention_mask=attention_mask, \n",
    "                    mlm_mask=mlm_mask, \n",
    "                    Y=one_hot_labels, \n",
    "                    loss_function=CategoricalCrossEntropy, \n",
    "                    optimizer=GradientDescent(),\n",
    "                    accuracy_metric=accuracy_metric)\n",
    "        \n",
    "        print('Epoch ' + str(epoch_count) + ', Seq ' + str(sequence_count) + ', ' + training_log)\n",
    "        \n",
    "        if (sequence_count % checkpoint_count == 0):\n",
    "            start_time = time.time()\n",
    "\n",
    "            model.save_model(str(epoch_count) + '_' + str(sequence_count) + '.txt')\n",
    "            # save checkpoint settings\n",
    "            with open('training_checkpoint.txt', 'w') as file:\n",
    "                file.write(str(sequence_count))\n",
    "\n",
    "            print('Model Saved, Elapsed Time: ' + str(time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "\n",
    "        sequence_count += 1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convergence test\n",
    "# padded_tokenized, attention_mask, mlm_mask = list(zip(padded_tokenized_training_tokens, training_attention_mask, training_mlm_mask))[0]\n",
    "# tokenized_training_labels = tokenizer.tokenize(training_labels)[0]\n",
    "\n",
    "# for _ in range(20):\n",
    "#     \"\"\"\n",
    "#         Creation of labels. Labels should be created per input to reduce memory usage.\n",
    "#         Labels has a shape of (vocab_size, d_model) which is a very large data.\n",
    "#     \"\"\"\n",
    "#     one_hot_labels = np.zeros((max_pos, vocab_size), dtype=np.float64)\n",
    "#     masked_token_indices = np.where(mlm_mask.flatten() == 1)[0].tolist()\n",
    "#     for row, column in zip(masked_token_indices, tokenized_training_labels):\n",
    "#         one_hot_labels[row, column] = 1\n",
    "#     # np.add.at(one_hot_labels, (masked_token_indices, tokenized_labels), 1)\n",
    "\n",
    "#     model.fit(X=padded_tokenized, \n",
    "#                 attention_mask=attention_mask, \n",
    "#                 mlm_mask=mlm_mask, \n",
    "#                 Y=one_hot_labels, \n",
    "#                 loss_function=CategoricalCrossEntropy, \n",
    "#                 optimizer=GradientDescent(),\n",
    "#                 accuracy_metric=accuracy_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.27097538e-03  1.48845310e-03  6.13420199e-04 ... -2.24484553e-03\n",
      "  -4.61373138e-04 -3.72050150e-04]\n",
      " [ 4.45254348e-03  6.64050752e-03  3.39324109e-03 ...  5.36004780e-03\n",
      "   4.40420823e-03  2.55265890e-03]\n",
      " [ 1.11647376e-03  7.66161637e-04  2.85109310e-03 ...  1.59098767e-03\n",
      "   1.67978736e-03 -7.33809141e-05]\n",
      " ...\n",
      " [ 4.71714501e-04  3.59302877e-04  1.88894696e-03 ... -1.34735135e-03\n",
      "  -1.25241590e-03 -1.32853576e-03]\n",
      " [ 3.61046064e-03  2.04226710e-03  6.54083094e-04 ...  3.63204441e-03\n",
      "   4.01618613e-03  4.09725290e-03]\n",
      " [ 6.24274773e-04 -8.59859300e-04  2.10151943e-03 ... -8.59635046e-04\n",
      "  -3.59025337e-04 -2.00460710e-04]]\n",
      "(512, 512)\n"
     ]
    }
   ],
   "source": [
    "# # model beheading test\n",
    "# model.remove_mlm_head()\n",
    "\n",
    "# # convergence test\n",
    "# padded_tokenized, attention_mask, mlm_mask = list(zip(padded_tokenized_training_tokens, training_attention_mask, training_mlm_mask))[0]\n",
    "\n",
    "# embedding = model.predict(X=padded_tokenized, attention_mask=attention_mask)\n",
    "\n",
    "# print(embedding) # shape of (d_model, seq_length)\n",
    "# print(embedding.shape) # (512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.get_trainable_variables())\n",
    "# model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_model()\n",
    "# print(model.get_trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.get_trainable_variables())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
