{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from tokenizer import Tokenizer\n",
    "from mlm_dataset.mlm_dataset_generator import MLMDatasetGenerator\n",
    "\n",
    "# hyperparameters\n",
    "vocab_size = 22789\n",
    "model_dim = 512\n",
    "num_heads = 8\n",
    "ffn_dim = 2048\n",
    "max_pos = 512\n",
    "\n",
    "# # test hyperparameters\n",
    "# vocab_size = 22732\n",
    "# model_dim = 70\n",
    "# num_heads = 2\n",
    "# ffn_dim = 2048\n",
    "# max_pos = 10\n",
    "\n",
    "# MLM dataset for training\n",
    "mlm_dataset_generator = MLMDatasetGenerator(max_pos=max_pos)\n",
    "\n",
    "mlm_dataset = mlm_dataset_generator.read_mlm_dataset_from_file()\n",
    "mlm_dataset_generator.read_raw_training_data_from_file()\n",
    "\n",
    "tokenizer = Tokenizer(max_pos=max_pos, vocab_size=vocab_size)\n",
    "\n",
    "# fit tokenizer on dataset\n",
    "tokenizer.fit_on_texts(mlm_dataset_generator.getVocubulary())\n",
    "\n",
    "# generate MLM dataset\n",
    "\n",
    "batch_size = 20\n",
    "sample_limit = 1000\n",
    "\n",
    "# to free memory\n",
    "del mlm_dataset_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22732\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from base import Sequential\n",
    "from layers import WordEmbedding, PositionalEncoding, Dense, MultiHeadAttention, SelfAttention, LayerNormalization\n",
    "from activation import ReLu, Linear, Softmax\n",
    "from loss import CategoricalCrossEntropy\n",
    "from optimizers import Adam, GradientDescent\n",
    "\n",
    "model = Sequential([\n",
    "\t\t\tWordEmbedding(vocab_size, model_dim),\n",
    "\t\t\tPositionalEncoding(max_pos, model_dim),\n",
    "            MultiHeadAttention(num_heads, max_pos, model_dim),\n",
    "\t\t\t# SelfAttention(max_pos, model_dim),\n",
    "\t\t\tLayerNormalization(model_dim),\n",
    "\t\t\t# Feed Forward Network\n",
    "\t\t\tDense([model_dim, ffn_dim], ReLu),\n",
    "\t\t\tDense([ffn_dim, model_dim], Linear),\n",
    "\t\t\tLayerNormalization(model_dim),\n",
    "\t\t\t# MLM Head\n",
    "\t\t\tDense([model_dim, vocab_size], Softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example truncation handling\n",
    "\n",
    "# sample_text = 'I am an example resume. Tae tae tae, meow mewo mewo. Cats dogs relationships. I am an example resume. Tae tae tae, meow mewo mewo. Cats dogs relationships.'\n",
    "\n",
    "# padded_tokenized_training_tokens, training_attention_mask = tokenizer.clean_truncate_tokenize_pad_atten(sample_text)\n",
    "\n",
    "# print(padded_tokenized_training_tokens)\n",
    "# print(training_attention_mask)\n",
    "\n",
    "# print(model.predict(padded_tokenized_training_tokens, training_attention_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, testing_data = mlm_dataset\n",
    "\n",
    "sample_limit = None\n",
    "\n",
    "training_tokens = training_data[:sample_limit * 2 if sample_limit else None:2]\n",
    "training_labels = training_data[1:sample_limit * 2 if sample_limit else None:2]\n",
    "\n",
    "# tokenization, padding, attention mask\n",
    "padded_tokenized_training_tokens, training_attention_mask = tokenizer.tokenize_pad_atten(tokens=training_tokens)\n",
    "\n",
    "# MLM training mask\n",
    "training_mlm_mask = tokenizer.generate_mlm_mask(training_attention_mask)\n",
    "\n",
    "# change padding tokens to 0\n",
    "training_attention_mask[training_attention_mask == -1] = 0\n",
    "padded_tokenized_training_tokens = np.array(padded_tokenized_training_tokens)\n",
    "padded_tokenized_training_tokens[padded_tokenized_training_tokens == -1] = tokenizer.get_pad_token_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(padded_tokenized_training_tokens[0])\n",
    "# print(training_attention_mask[0])\n",
    "# print(training_mlm_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(Y, Y_hat):\n",
    "    Y, Y_hat = Y.T, Y_hat.T\n",
    "\n",
    "    rows, columns = np.where(Y == 1)\n",
    "    correct_predictions = sum(1 for actual, predicted in zip(columns, np.argmax(Y_hat[rows], axis=-1)) if actual == predicted)\n",
    "    return correct_predictions / len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load latest model save\n",
    "model.load_model('0_2600.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.038811659062606, Total Loss: 10.038811659062606, Accuracy: 0.0\n",
      "Loss: 10.039046371859808, Total Loss: 10.038929015461207, Accuracy: 0.0\n",
      "Loss: 30.11446554661054, Total Loss: 16.730774525844318, Accuracy: 0.0\n",
      "Loss: 30.084670177550294, Total Loss: 20.069248438770813, Accuracy: 0.0\n",
      "Loss: 10.038012720608961, Total Loss: 18.06300129513844, Accuracy: 0.0\n",
      "Loss: 10.038318578873918, Total Loss: 16.725554175761022, Accuracy: 0.0\n",
      "Loss: 10.038329767294535, Total Loss: 15.770236403122952, Accuracy: 0.0\n",
      "Loss: 10.038929084570666, Total Loss: 15.053822988303917, Accuracy: 0.0\n",
      "Loss: 10.036760691250413, Total Loss: 14.496371621964638, Accuracy: 0.0\n",
      "Loss: 10.036726029694375, Total Loss: 14.05040706273761, Accuracy: 0.0\n",
      "Loss: 10.037539516153519, Total Loss: 13.685600922139058, Accuracy: 0.0\n",
      "Loss: 10.037172872494287, Total Loss: 13.381565251335326, Accuracy: 0.0\n",
      "Loss: 20.073746548657283, Total Loss: 13.8963484280524, Accuracy: 0.0\n",
      "Loss: 10.036513739947722, Total Loss: 13.620645950330637, Accuracy: 0.0\n",
      "Loss: 20.077407344624604, Total Loss: 14.051096709950233, Accuracy: 0.0\n",
      "Loss: 10.037937222136886, Total Loss: 13.8002742419619, Accuracy: 0.0\n",
      "Loss: 9.988314042956356, Total Loss: 13.57604128907922, Accuracy: 0.0\n",
      "Loss: 10.03869148397953, Total Loss: 13.37952185546257, Accuracy: 0.0\n",
      "Loss: 30.115627661090848, Total Loss: 14.260369529443006, Accuracy: 0.0\n",
      "Loss: 10.027468155560248, Total Loss: 14.048724460748868, Accuracy: 0.0\n",
      "Loss: 10.039046371859808, Total Loss: 13.857787408897009, Accuracy: 0.0\n",
      "Loss: 10.03777760052951, Total Loss: 13.68415059942576, Accuracy: 0.0\n",
      "Loss: 30.11283103325981, Total Loss: 14.398441053070718, Accuracy: 0.0\n",
      "Loss: 20.075114881219747, Total Loss: 14.634969129243593, Accuracy: 0.0\n",
      "Loss: 50.19036367748199, Total Loss: 16.057184911173128, Accuracy: 0.0\n",
      "Loss: 20.075240760783583, Total Loss: 16.21172552077353, Accuracy: 0.0\n",
      "Loss: 30.113443121561104, Total Loss: 16.726603950432327, Accuracy: 0.0\n",
      "Loss: 10.038770485709144, Total Loss: 16.487752755263646, Accuracy: 0.0\n",
      "Loss: 20.076638997862137, Total Loss: 16.611507453284283, Accuracy: 0.0\n",
      "Loss: 20.0745032988561, Total Loss: 16.726940648136676, Accuracy: 0.0\n",
      "Loss: 30.117227759184978, Total Loss: 17.158885393654362, Accuracy: 0.0\n",
      "Loss: 20.075545209017026, Total Loss: 17.250031012884445, Accuracy: 0.0\n",
      "Loss: 30.111554770521046, Total Loss: 17.63977415705525, Accuracy: 0.0\n",
      "Loss: 10.038032923810517, Total Loss: 17.416193532548053, Accuracy: 0.0\n",
      "Loss: 10.03777760052951, Total Loss: 17.205381648776093, Accuracy: 0.0\n",
      "Loss: 30.11578988653737, Total Loss: 17.56400409982502, Accuracy: 0.0\n",
      "Loss: 40.15262207332428, Total Loss: 18.174507288297974, Accuracy: 0.0\n",
      "Loss: 50.18936947941722, Total Loss: 19.01700366174848, Accuracy: 0.0\n",
      "Loss: 10.027329016251546, Total Loss: 18.786499183658815, Accuracy: 0.0\n",
      "Loss: 10.03777760052951, Total Loss: 18.56778114408058, Accuracy: 0.0\n",
      "Loss: 30.114737743330323, Total Loss: 18.84941423186716, Accuracy: 0.0\n",
      "Loss: 40.15110779903661, Total Loss: 19.35659741203786, Accuracy: 0.0\n",
      "Loss: 20.0742347324244, Total Loss: 19.37328665204685, Accuracy: 0.0\n",
      "Loss: 30.11186147163928, Total Loss: 19.617345170673953, Accuracy: 0.0\n",
      "Loss: 30.11186465410066, Total Loss: 19.850556714750102, Accuracy: 0.0\n",
      "Loss: 10.036829210394538, Total Loss: 19.637214812481503, Accuracy: 0.0\n",
      "Loss: 10.03777760052951, Total Loss: 19.432971467546352, Accuracy: 0.0\n",
      "Loss: 19.986588245519844, Total Loss: 19.444505150420802, Accuracy: 0.0\n",
      "Loss: 20.07963028599562, Total Loss: 19.457466887881512, Accuracy: 0.0\n",
      "Loss: 30.117968822762087, Total Loss: 19.670676926579127, Accuracy: 0.0\n",
      "Loss: 29.952434430920338, Total Loss: 19.87228001489954, Accuracy: 0.0\n",
      "Loss: 10.039990394893719, Total Loss: 19.683197522207124, Accuracy: 0.0\n",
      "Loss: 10.038752736576322, Total Loss: 19.501226865874468, Accuracy: 0.0\n",
      "Loss: 10.03625512224046, Total Loss: 19.325949611362724, Accuracy: 0.0\n",
      "Loss: 10.038752736576322, Total Loss: 19.157091486366607, Accuracy: 0.0\n",
      "Loss: 10.038636819910863, Total Loss: 18.994261938751325, Accuracy: 0.0\n",
      "Loss: 10.038752736576322, Total Loss: 18.83714774222194, Accuracy: 0.0\n",
      "Loss: 10.027465700532009, Total Loss: 18.68525667253763, Accuracy: 0.0\n",
      "Loss: 10.037560471989035, Total Loss: 18.538685550494435, Accuracy: 0.0\n",
      "Loss: 10.027465700532009, Total Loss: 18.396831886328396, Accuracy: 0.0\n",
      "Loss: 10.038333288395247, Total Loss: 18.259807319149164, Accuracy: 0.0\n",
      "Loss: 10.03727407117177, Total Loss: 18.12718581514953, Accuracy: 0.0\n",
      "Loss: 10.037488005190694, Total Loss: 17.99877791340415, Accuracy: 0.0\n",
      "Loss: 10.038409729050345, Total Loss: 17.87439716052362, Accuracy: 0.0\n",
      "Loss: 20.075036483112672, Total Loss: 17.908253150101913, Accuracy: 0.0\n",
      "Loss: 80.30363118841014, Total Loss: 18.85363766583386, Accuracy: 0.0\n",
      "Loss: 10.039002269601795, Total Loss: 18.722075943502034, Accuracy: 0.0\n",
      "Loss: 20.076165748412834, Total Loss: 18.74198902886837, Accuracy: 0.0\n",
      "Loss: 10.037591278646262, Total Loss: 18.615838336836166, Accuracy: 0.0\n",
      "Loss: 10.03777760052951, Total Loss: 18.493294612031786, Accuracy: 0.0\n",
      "Loss: 20.07812196745985, Total Loss: 18.51561612408007, Accuracy: 0.0\n",
      "Loss: 30.11149412961237, Total Loss: 18.676669985268017, Accuracy: 0.0\n",
      "Loss: 20.076132465307374, Total Loss: 18.695840704172664, Accuracy: 0.0\n",
      "Loss: 20.06495724015921, Total Loss: 18.714342278983292, Accuracy: 0.0\n",
      "Loss: 20.075862249054392, Total Loss: 18.732495878584242, Accuracy: 0.0\n",
      "Loss: 30.096143416840555, Total Loss: 18.88201755671919, Accuracy: 0.0\n",
      "Loss: 20.07735079807233, Total Loss: 18.897541365048454, Accuracy: 0.0\n",
      "Loss: 20.076570505916905, Total Loss: 18.912657123264715, Accuracy: 0.0\n",
      "Loss: 30.11375285869156, Total Loss: 19.054443145232145, Accuracy: 0.0\n",
      "Loss: 10.038154691012805, Total Loss: 18.9417395395544, Accuracy: 0.0\n",
      "Loss: 10.03777760052951, Total Loss: 18.83181408351706, Accuracy: 0.0\n",
      "Loss: 30.1128737008813, Total Loss: 18.969387981289792, Accuracy: 0.0\n",
      "Loss: 20.0764330931784, Total Loss: 18.982725874204114, Accuracy: 0.0\n",
      "Loss: 20.05927399144928, Total Loss: 18.995541923218937, Accuracy: 0.0\n",
      "Loss: 20.078440019834673, Total Loss: 19.00828190082618, Accuracy: 0.0\n",
      "Loss: 30.11450989220908, Total Loss: 19.137424086772494, Accuracy: 0.0\n",
      "Loss: 30.111657139283665, Total Loss: 19.26356469657147, Accuracy: 0.0\n",
      "Loss: 30.11380467299876, Total Loss: 19.386862878121782, Accuracy: 0.0\n",
      "Loss: 20.079371515716808, Total Loss: 19.394643874049816, Accuracy: 0.0\n",
      "Loss: 20.074575450673652, Total Loss: 19.402198669345637, Accuracy: 0.0\n",
      "Loss: 10.038560219091273, Total Loss: 19.299301543518666, Accuracy: 0.0\n",
      "Loss: 10.038752736576322, Total Loss: 19.19864340431277, Accuracy: 0.0\n",
      "Loss: 30.115585329901172, Total Loss: 19.316029876630925, Accuracy: 0.0\n",
      "Loss: 20.075946151887848, Total Loss: 19.324114092325146, Accuracy: 0.0\n",
      "Loss: 30.114593893005818, Total Loss: 19.437698090227048, Accuracy: 0.0\n",
      "Loss: 20.073352005844896, Total Loss: 19.444319485181403, Accuracy: 0.0\n",
      "Loss: 20.076406156511116, Total Loss: 19.450835842617792, Accuracy: 0.0\n",
      "Loss: 40.1541232871331, Total Loss: 19.66209387776591, Accuracy: 0.0\n",
      "Loss: 20.06856377312113, Total Loss: 19.66619963428465, Accuracy: 0.0\n",
      "Loss: 20.07297428840187, Total Loss: 19.67026738082582, Accuracy: 0.0\n",
      "Loss: 20.0759574137384, Total Loss: 19.674284113824957, Accuracy: 0.0\n",
      "Loss: 10.038947179827952, Total Loss: 19.579820026236753, Accuracy: 0.0\n",
      "Loss: 10.03777760052951, Total Loss: 19.487178837637654, Accuracy: 0.0\n",
      "Loss: 10.0385225940046, Total Loss: 19.396326373756565, Accuracy: 0.0\n",
      "Loss: 20.077846819834114, Total Loss: 19.40281704467159, Accuracy: 0.0\n",
      "Loss: 20.076713567131325, Total Loss: 19.40917455903442, Accuracy: 0.0\n",
      "Loss: 20.076938853965054, Total Loss: 19.415415346837513, Accuracy: 0.0\n",
      "Loss: 20.074390598629748, Total Loss: 19.42151696953929, Accuracy: 0.0\n",
      "Loss: 20.079260634575036, Total Loss: 19.427551315090074, Accuracy: 0.0\n",
      "Loss: 20.075688774754134, Total Loss: 19.433443473814297, Accuracy: 0.0\n",
      "Loss: 20.077576150396954, Total Loss: 19.439246470900628, Accuracy: 0.0\n",
      "Loss: 30.114135939404452, Total Loss: 19.53455798401227, Accuracy: 0.0\n",
      "Loss: 30.114009750605582, Total Loss: 19.62818145097327, Accuracy: 0.0\n",
      "Loss: 20.074717717765118, Total Loss: 19.632098435769688, Accuracy: 0.0\n",
      "Loss: 20.07654410103934, Total Loss: 19.635963180685078, Accuracy: 0.0\n",
      "Loss: 10.03777760052951, Total Loss: 19.553220201545805, Accuracy: 0.0\n",
      "Loss: 30.112887016101794, Total Loss: 19.643473764063376, Accuracy: 0.0\n",
      "Loss: 30.113541054546477, Total Loss: 19.732203147881034, Accuracy: 0.0\n",
      "Loss: 30.11592142842295, Total Loss: 19.81946128469231, Accuracy: 0.0\n",
      "Loss: 10.037292013761034, Total Loss: 19.73794320743455, Accuracy: 0.0\n",
      "Loss: 10.037648121070564, Total Loss: 19.65777547944807, Accuracy: 0.0\n",
      "Loss: 10.038022011295627, Total Loss: 19.578925041184526, Accuracy: 0.0\n",
      "Loss: 10.038745368909467, Total Loss: 19.501362604824568, Accuracy: 0.0\n",
      "Loss: 10.038514214078885, Total Loss: 19.42504931135081, Accuracy: 0.0\n",
      "Loss: 10.039466503128663, Total Loss: 19.349964648885035, Accuracy: 0.0\n",
      "Loss: 90.34396694858535, Total Loss: 19.913409111581068, Accuracy: 0.0\n",
      "Loss: 10.039045693793549, Total Loss: 19.83565821852762, Accuracy: 0.0\n",
      "Loss: 10.03830638120207, Total Loss: 19.759116407298514, Accuracy: 0.0\n",
      "Loss: 20.076078251943343, Total Loss: 19.761573475861653, Accuracy: 0.0\n",
      "Loss: 20.07638016796758, Total Loss: 19.76399506580093, Accuracy: 0.0\n",
      "Loss: 20.076113182700098, Total Loss: 19.766377646845964, Accuracy: 0.0\n",
      "Loss: 10.038902850877507, Total Loss: 19.692684655967412, Accuracy: 0.0\n",
      "Loss: 10.038077800906123, Total Loss: 19.62009362698199, Accuracy: 0.0\n",
      "Loss: 10.039806029085224, Total Loss: 19.5485989434156, Accuracy: 0.0\n",
      "Loss: 10.03728338595234, Total Loss: 19.478144754101056, Accuracy: 0.0\n",
      "Loss: 10.038144979595998, Total Loss: 19.408732991053224, Accuracy: 0.0\n",
      "Loss: 40.15043232862165, Total Loss: 19.56013225629095, Accuracy: 0.0\n",
      "Loss: 10.036743293480006, Total Loss: 19.491122191343045, Accuracy: 0.0\n",
      "Loss: 10.040312790189512, Total Loss: 19.423130756802372, Accuracy: 0.0\n",
      "Loss: 70.26886429494004, Total Loss: 19.78631456778907, Accuracy: 0.0\n",
      "Loss: 70.25503381915534, Total Loss: 20.144248746876777, Accuracy: 0.0\n",
      "Loss: 60.22375385294703, Total Loss: 20.426498782835015, Accuracy: 0.0\n",
      "Loss: 30.112612973573295, Total Loss: 20.494233847105914, Accuracy: 0.0\n",
      "Loss: 30.111233098946734, Total Loss: 20.56101856413259, Accuracy: 0.0\n",
      "Loss: 20.075913406859378, Total Loss: 20.557673011323807, Accuracy: 0.0\n",
      "Loss: 70.26449006997613, Total Loss: 20.898130662410466, Accuracy: 0.0\n",
      "Loss: 50.1914996263366, Total Loss: 21.097405281212687, Accuracy: 0.0\n",
      "Loss: 30.11204764789743, Total Loss: 21.15831502693353, Accuracy: 0.0\n",
      "Loss: 30.110484748533267, Total Loss: 21.218396702917417, Accuracy: 0.0\n",
      "Loss: 20.076370185426164, Total Loss: 21.210783192800807, Accuracy: 0.0\n",
      "Loss: 10.039177793431241, Total Loss: 21.13679905108313, Accuracy: 0.0\n",
      "Loss: 10.038752736576322, Total Loss: 21.063785588487693, Accuracy: 0.0\n",
      "Loss: 30.114825790064707, Total Loss: 21.122942713988195, Accuracy: 0.0\n",
      "Loss: 30.07886649009422, Total Loss: 21.18109806318369, Accuracy: 0.0\n",
      "Loss: 10.038018974121524, Total Loss: 21.10920723035103, Accuracy: 0.0\n",
      "Loss: 10.037780168834818, Total Loss: 21.038236544059256, Accuracy: 0.0\n",
      "Loss: 30.112058064168416, Total Loss: 21.096031585588612, Accuracy: 0.0\n",
      "Loss: 30.115503801817255, Total Loss: 21.153116852779934, Accuracy: 0.0\n",
      "Loss: 20.076512004740945, Total Loss: 21.146345753106733, Accuracy: 0.0\n",
      "Loss: 20.078065105721684, Total Loss: 21.139668999060575, Accuracy: 0.0\n",
      "Loss: 10.039177793431241, Total Loss: 21.070721848715053, Accuracy: 0.0\n",
      "Loss: 10.038752736576322, Total Loss: 21.002623273948764, Accuracy: 0.0\n",
      "Loss: 20.074171928588477, Total Loss: 20.99692725342508, Accuracy: 0.0\n",
      "Loss: 40.152844681219044, Total Loss: 21.1137316279848, Accuracy: 0.0\n",
      "Loss: 60.22695309699306, Total Loss: 21.350781455069697, Accuracy: 0.0\n",
      "Loss: 60.2306986352936, Total Loss: 21.584997823625265, Accuracy: 0.0\n",
      "Loss: 70.25739905583178, Total Loss: 21.876449328009734, Accuracy: 0.0\n",
      "Loss: 30.115977463627694, Total Loss: 21.92549413834079, Accuracy: 0.0\n",
      "Loss: 10.038200779341034, Total Loss: 21.855155124382215, Accuracy: 0.0\n",
      "Loss: 10.03777760052951, Total Loss: 21.785641138947785, Accuracy: 0.0\n",
      "Loss: 20.07886701664471, Total Loss: 21.77566000372964, Accuracy: 0.0\n",
      "Loss: 30.114426324513758, Total Loss: 21.824141203269082, Accuracy: 0.0\n",
      "Loss: 10.037136752095593, Total Loss: 21.756008229562877, Accuracy: 0.0\n",
      "Loss: 10.027902738276852, Total Loss: 21.688605324440545, Accuracy: 0.0\n",
      "Loss: 30.115661672574404, Total Loss: 21.736759932144164, Accuracy: 0.0\n",
      "Loss: 20.0784039760919, Total Loss: 21.72733745512114, Accuracy: 0.0\n",
      "Loss: 50.189061221527474, Total Loss: 21.888138154366374, Accuracy: 0.0\n",
      "Loss: 30.115931118618892, Total Loss: 21.93436171034532, Accuracy: 0.0\n",
      "Loss: 10.037735080772698, Total Loss: 21.86790010906279, Accuracy: 0.0\n",
      "Loss: 10.03777760052951, Total Loss: 21.802177206237605, Accuracy: 0.0\n",
      "Loss: 20.073660738918292, Total Loss: 21.792627391501032, Accuracy: 0.0\n",
      "Loss: 20.07523472454849, Total Loss: 21.783191168056238, Accuracy: 0.0\n",
      "Loss: 50.1795983461889, Total Loss: 21.93836279198046, Accuracy: 0.0\n",
      "Loss: 20.06723767462274, Total Loss: 21.92819363373395, Accuracy: 0.0\n",
      "Loss: 30.11530777110801, Total Loss: 21.972448304746784, Accuracy: 0.0\n",
      "Loss: 20.074963506037278, Total Loss: 21.96224677357093, Accuracy: 0.0\n",
      "Loss: 10.040401790210053, Total Loss: 21.898493591841724, Accuracy: 0.0\n",
      "Loss: 10.028232221040671, Total Loss: 21.83535390369917, Accuracy: 0.0\n",
      "Loss: 10.037400077983888, Total Loss: 21.77293086758427, Accuracy: 0.0\n",
      "Loss: 10.037589189174081, Total Loss: 21.711165911382114, Accuracy: 0.0\n",
      "Loss: 10.039898649379158, Total Loss: 21.650059800062728, Accuracy: 0.0\n",
      "Loss: 20.07729212290048, Total Loss: 21.64186830174418, Accuracy: 0.0\n",
      "Loss: 10.038756704241893, Total Loss: 21.58174855253432, Accuracy: 0.0\n",
      "Loss: 10.040281331170924, Total Loss: 21.522256453455128, Accuracy: 0.0\n",
      "Loss: 10.037589189174081, Total Loss: 21.463360723894716, Accuracy: 0.0\n",
      "Loss: 10.039466090945282, Total Loss: 21.40507554719599, Accuracy: 0.0\n",
      "Loss: 10.038825461944526, Total Loss: 21.34737884625563, Accuracy: 0.0\n",
      "Loss: 10.036236525987265, Total Loss: 21.290251864840133, Accuracy: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\transformer_encoder.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m row, column \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(masked_token_indices, labels):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     one_hot_labels[row, column] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mvalidate(X\u001b[39m=\u001b[39;49mpadded_tokenized, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m             attention_mask\u001b[39m=\u001b[39;49mattention_mask, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m             mlm_mask\u001b[39m=\u001b[39;49mmlm_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m             Y\u001b[39m=\u001b[39;49mone_hot_labels, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m             loss_function\u001b[39m=\u001b[39;49mCategoricalCrossEntropy,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m             accuracy_metric\u001b[39m=\u001b[39;49maccuracy_metric)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X21sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(loss) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, Total Loss: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(total_loss \u001b[39m/\u001b[39m num_input) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, Accuracy: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(accuracy))\n",
      "File \u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\base.py:149\u001b[0m, in \u001b[0;36mSequential.validate\u001b[1;34m(self, X, attention_mask, mlm_mask, Y, loss_function, accuracy_metric)\u001b[0m\n\u001b[0;32m    147\u001b[0m         V_hat \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mforward(V_hat, prev_V_hat, training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    148\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m         V_hat \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(V_hat, training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    151\u001b[0m \u001b[39m# apply mlm_mask\u001b[39;00m\n\u001b[0;32m    152\u001b[0m mlm_masked_V_hat \u001b[39m=\u001b[39m V_hat \u001b[39m*\u001b[39m mlm_mask\u001b[39m.\u001b[39mT\n",
      "File \u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\layers.py:97\u001b[0m, in \u001b[0;36mDense.forward\u001b[1;34m(self, X, training)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, X, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m     96\u001b[0m     \u001b[39m# forward propagation\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     Z \u001b[39m=\u001b[39m PreActivation\u001b[39m.\u001b[39;49mforward(X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb)\n\u001b[0;32m     98\u001b[0m     A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation\u001b[39m.\u001b[39mforward(Z)\n\u001b[0;32m    100\u001b[0m     \u001b[39m# cache values for gradient descent\u001b[39;00m\n",
      "File \u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\common.py:50\u001b[0m, in \u001b[0;36mPreActivation.forward\u001b[1;34m(X, W, b)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(X: NDArray[np\u001b[39m.\u001b[39mfloat64],\n\u001b[0;32m     47\u001b[0m             W: NDArray[np\u001b[39m.\u001b[39mfloat64],\n\u001b[0;32m     48\u001b[0m             b: NDArray[np\u001b[39m.\u001b[39mfloat64]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDArray[np\u001b[39m.\u001b[39mfloat64]:\n\u001b[0;32m     49\u001b[0m     b \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(b, (\u001b[39mlen\u001b[39m(b), \u001b[39m1\u001b[39m))\n\u001b[1;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m W\u001b[39m.\u001b[39;49mdot(X) \u001b[39m+\u001b[39m b\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenized_training_labels = tokenizer.tokenize(training_labels)\n",
    "\n",
    "total_loss = 0\n",
    "num_input = 1\n",
    "for padded_tokenized, attention_mask, mlm_mask, labels in zip(padded_tokenized_training_tokens, \n",
    "                                                                  training_attention_mask, \n",
    "                                                                  training_mlm_mask, \n",
    "                                                                  tokenized_training_labels):\n",
    "        \"\"\"\n",
    "            Creation of labels. Labels should be created per input to reduce memory usage.\n",
    "            Labels has a shape of (vocab_size, d_model) which is a very large data.\n",
    "        \"\"\"\n",
    "        one_hot_labels = np.zeros((max_pos, vocab_size), dtype=np.float64)\n",
    "        masked_token_indices = np.where(mlm_mask.flatten() == 1)[0].tolist()\n",
    "        for row, column in zip(masked_token_indices, labels):\n",
    "            one_hot_labels[row, column] = 1\n",
    "\n",
    "        loss, accuracy = model.validate(X=padded_tokenized, \n",
    "                    attention_mask=attention_mask, \n",
    "                    mlm_mask=mlm_mask,\n",
    "                    Y=one_hot_labels, \n",
    "                    loss_function=CategoricalCrossEntropy,\n",
    "                    accuracy_metric=accuracy_metric)\n",
    "        \n",
    "        total_loss += loss\n",
    "\n",
    "        print('Loss: ' + str(loss) + ', Total Loss: ' + str(total_loss / num_input) + ', Accuracy: ' + str(accuracy))\n",
    "\n",
    "        num_input += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Seq 0, Loss: 10.038173605033311, Accuracy: 0.0, Elapsed Time: 13.443068265914917\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\transformer_encoder.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X12sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mif\u001b[39;00m (sequence_count \u001b[39m%\u001b[39m checkpoint_count \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X12sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     model\u001b[39m.\u001b[39;49msave_model(\u001b[39mstr\u001b[39;49m(epoch_count) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(sequence_count) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X12sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39m# save checkpoint settings\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X12sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtraining_checkpoint.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n",
      "File \u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\base.py:203\u001b[0m, in \u001b[0;36mSequential.save_model\u001b[1;34m(self, file_name)\u001b[0m\n\u001b[0;32m    200\u001b[0m trainable_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_trainable_variables()\n\u001b[0;32m    202\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m trainable_variables:\n\u001b[1;32m--> 203\u001b[0m     file\u001b[39m.\u001b[39mwrite(\u001b[39mstr\u001b[39;49m(trainable_variables[key]\u001b[39m.\u001b[39;49mtolist()) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenized_training_labels = tokenizer.tokenize(training_labels)\n",
    "\n",
    "epoch = 20\n",
    "# save model every n\n",
    "checkpoint_count = 500\n",
    "\n",
    "# load training checkpoint\n",
    "sequence_start = 0\n",
    "with open('training_checkpoint.txt', 'r') as file:\n",
    "    sequence_start = int(file.read()) + 1\n",
    "\n",
    "import time\n",
    "\n",
    "for epoch_count in range(epoch):\n",
    "    sequence_count = 0\n",
    "    for padded_tokenized, attention_mask, mlm_mask, labels in zip(padded_tokenized_training_tokens[sequence_start:], \n",
    "                                                                  training_attention_mask[sequence_start:], \n",
    "                                                                  training_mlm_mask[sequence_start:], \n",
    "                                                                  tokenized_training_labels[sequence_start:]):\n",
    "        \"\"\"\n",
    "            Creation of labels. Labels should be created per input to reduce memory usage.\n",
    "            Labels has a shape of (vocab_size, d_model) which is a very large data.\n",
    "        \"\"\"\n",
    "        one_hot_labels = np.zeros((max_pos, vocab_size), dtype=np.float64)\n",
    "        masked_token_indices = np.where(mlm_mask.flatten() == 1)[0].tolist()\n",
    "        for row, column in zip(masked_token_indices, labels):\n",
    "            one_hot_labels[row, column] = 1\n",
    "\n",
    "        training_log = model.fit(X=padded_tokenized, \n",
    "                    attention_mask=attention_mask, \n",
    "                    mlm_mask=mlm_mask, \n",
    "                    Y=one_hot_labels, \n",
    "                    loss_function=CategoricalCrossEntropy, \n",
    "                    optimizer=GradientDescent(),\n",
    "                    accuracy_metric=accuracy_metric)\n",
    "        \n",
    "        print('Epoch ' + str(epoch_count) + ', Seq ' + str(sequence_count) + ', ' + training_log)\n",
    "        \n",
    "        if (sequence_count % checkpoint_count == 0):\n",
    "            start_time = time.time()\n",
    "\n",
    "            model.save_model(str(epoch_count) + '_' + str(sequence_count) + '.txt')\n",
    "            # save checkpoint settings\n",
    "            with open('training_checkpoint.txt', 'w') as file:\n",
    "                file.write(str(sequence_count))\n",
    "\n",
    "            print('Model Saved, Elapsed Time: ' + str(time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "\n",
    "        sequence_count += 1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convergence test\n",
    "# padded_tokenized, attention_mask, mlm_mask = list(zip(padded_tokenized_training_tokens, training_attention_mask, training_mlm_mask))[0]\n",
    "# tokenized_training_labels = tokenizer.tokenize(training_labels)[0]\n",
    "\n",
    "# for _ in range(20):\n",
    "#     \"\"\"\n",
    "#         Creation of labels. Labels should be created per input to reduce memory usage.\n",
    "#         Labels has a shape of (vocab_size, d_model) which is a very large data.\n",
    "#     \"\"\"\n",
    "#     one_hot_labels = np.zeros((max_pos, vocab_size), dtype=np.float64)\n",
    "#     masked_token_indices = np.where(mlm_mask.flatten() == 1)[0].tolist()\n",
    "#     for row, column in zip(masked_token_indices, tokenized_training_labels):\n",
    "#         one_hot_labels[row, column] = 1\n",
    "#     # np.add.at(one_hot_labels, (masked_token_indices, tokenized_labels), 1)\n",
    "\n",
    "#     model.fit(X=padded_tokenized, \n",
    "#                 attention_mask=attention_mask, \n",
    "#                 mlm_mask=mlm_mask, \n",
    "#                 Y=one_hot_labels, \n",
    "#                 loss_function=CategoricalCrossEntropy, \n",
    "#                 optimizer=GradientDescent(),\n",
    "#                 accuracy_metric=accuracy_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.81737016e-07 -6.81737016e-07 -6.81737015e-07 ... -6.81737016e-07\n",
      "  -6.81737016e-07 -6.81737016e-07]\n",
      " [-8.78205461e-07 -8.78205461e-07 -8.78205460e-07 ... -8.78205460e-07\n",
      "  -8.78205460e-07 -8.78205460e-07]\n",
      " [ 8.49496321e-07  8.49496321e-07  8.49496322e-07 ...  8.49496322e-07\n",
      "   8.49496322e-07  8.49496322e-07]\n",
      " ...\n",
      " [-7.11249682e-07 -7.11249682e-07 -7.11249682e-07 ... -7.11249683e-07\n",
      "  -7.11249683e-07 -7.11249683e-07]\n",
      " [-5.68341682e-07 -5.68341682e-07 -5.68341682e-07 ... -5.68341682e-07\n",
      "  -5.68341682e-07 -5.68341682e-07]\n",
      " [-9.12259581e-07 -9.12259581e-07 -9.12259582e-07 ... -9.12259582e-07\n",
      "  -9.12259581e-07 -9.12259581e-07]]\n",
      "(512, 512)\n"
     ]
    }
   ],
   "source": [
    "# # model beheading test\n",
    "# model.remove_mlm_head()\n",
    "\n",
    "# convergence test\n",
    "padded_tokenized, attention_mask = tokenizer.clean_truncate_tokenize_pad_atten('ressadf sdf sd afs dfd f f the and two for')\n",
    "\n",
    "embedding = model.predict(X=padded_tokenized, attention_masks=attention_mask)\n",
    "\n",
    "print(embedding) # shape of (d_model, seq_length)\n",
    "print(embedding.shape) # (512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.get_trainable_variables())\n",
    "# model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_model()\n",
    "# print(model.get_trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.get_trainable_variables())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
