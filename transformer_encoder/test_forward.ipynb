{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# from mlm_dataset.mlm_dataset_generator import MLMDatasetGenerator\n",
    "\n",
    "# # MLM dataset for training\n",
    "# mlm_dataset_generator = MLMDatasetGenerator('../dataset/resume_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base import Sequential\n",
    "from layers import WordEmbedding, PositionalEncoding, Dense, MultiHeadAttention, LayerNormalization\n",
    "from activation import ReLu, Linear, Softmax\n",
    "from tokenizer import Tokenizer\n",
    "from preprocessing import pad_sequences, generate_attention_mask, generate_mlm_mask\n",
    "\n",
    "# vocab_size = 40000\n",
    "# model_dim = 512\n",
    "# num_heads = 8\n",
    "# ffn_dim = 2048\n",
    "\n",
    "# tokenizer = Tokenizer(vocab_size)\n",
    "\n",
    "# model = Sequential([\n",
    "#         WordEmbedding(vocab_size, model_dim),\n",
    "#         PositionalEncoding(int, model_dim),\n",
    "#         MultiHeadAttention(num_heads, int, model_dim),\n",
    "#         # Feed Forward Network\n",
    "#         Dense([model_dim, ffn_dim], ReLu()),\n",
    "#         Dense([ffn_dim, model_dim], Linear()),\n",
    "#         LayerNormalization(model_dim),\n",
    "#         # MLM Head\n",
    "#         Dense([model_dim, vocab_size], Softmax())\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.layers\n",
    "\n",
    "# hyper params\n",
    "max_pos = 3\n",
    "d_model = 4\n",
    "vocab_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_embedding/weights': array([[-0.02679642,  0.04489677,  0.0324373 ,  0.01394802],\n",
      "       [ 0.01818262, -0.03496223, -0.00070968,  0.02177156],\n",
      "       [-0.04322726,  0.03282814, -0.02480682,  0.03183122],\n",
      "       [-0.04540983,  0.03963188, -0.01973656, -0.01484503],\n",
      "       [ 0.02463193, -0.01660786,  0.00731976, -0.01108236],\n",
      "       [-0.01081998, -0.00088423,  0.01886229, -0.02092738],\n",
      "       [-0.00963866,  0.03029319, -0.03382122,  0.00839908],\n",
      "       [-0.02094575, -0.03279179,  0.00265743, -0.03675856],\n",
      "       [ 0.04187943,  0.02954264, -0.02944153,  0.03337073],\n",
      "       [-0.0356996 , -0.00486696,  0.02759491, -0.00151797]])}\n"
     ]
    }
   ],
   "source": [
    "# Word Embedding Layer Test\n",
    "tf_word_embedding_layer = keras.layers.Embedding(vocab_size, d_model, dtype=tf.float64)\n",
    "tf_word_embedding_layer(tf.constant([1], dtype=tf.float64))\n",
    "\n",
    "word_embedding_layer = WordEmbedding(vocab_size, d_model)\n",
    "word_embedding_layer.set_trainable_variables(tf_word_embedding_layer.get_weights()[0])\n",
    "print(word_embedding_layer.get_trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:\n",
      "tf.Tensor(\n",
      "[[ 0.01818262 -0.03496223 -0.00070968  0.02177156]\n",
      " [-0.04322726  0.03282814 -0.02480682  0.03183122]\n",
      " [-0.04540983  0.03963188 -0.01973656 -0.01484503]], shape=(3, 4), dtype=float64)\n",
      "numpy:\n",
      "[[ 0.01818262 -0.03496223 -0.00070968  0.02177156]\n",
      " [-0.04322726  0.03282814 -0.02480682  0.03183122]\n",
      " [-0.04540983  0.03963188 -0.01973656 -0.01484503]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = [1, 2, 3]\n",
    "\n",
    "tf_word_embedding = tf_word_embedding_layer(tf.constant(inputs, dtype=tf.float64))\n",
    "print('tensorflow:')\n",
    "print(tf_word_embedding)\n",
    "\n",
    "word_embedding = word_embedding_layer.forward(np.array(inputs, dtype=np.int64).T)\n",
    "print('numpy:')\n",
    "print(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:\n",
      "tf.Tensor(\n",
      "[[[ 0.          1.          0.          1.        ]\n",
      "  [ 0.84147098  0.54030231  0.00999983  0.99995   ]\n",
      "  [ 0.90929743 -0.41614684  0.01999867  0.99980001]]], shape=(1, 3, 4), dtype=float64)\n",
      "numpy:\n",
      "[[ 0.          1.          0.          1.        ]\n",
      " [ 0.84147098  0.54030231  0.00999983  0.99995   ]\n",
      " [ 0.90929743 -0.41614684  0.01999867  0.99980001]]\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding Layer Test\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float64(d_model))\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float64)\n",
    "\n",
    "tf_positional_encoding_layer = positional_encoding(max_pos, d_model)\n",
    "print('tensorflow:')\n",
    "print(tf_positional_encoding_layer)\n",
    "\n",
    "positional_encoding_layer = PositionalEncoding(max_pos, d_model)\n",
    "print('numpy:')\n",
    "print(positional_encoding_layer.get_positional_encoding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:\n",
      "tf.Tensor(\n",
      "[[[ 1.81826236e-02  9.65037769e-01 -7.09684361e-04  1.02177156e+00]\n",
      "  [ 7.98243721e-01  5.73130446e-01 -1.48069907e-02  1.03178122e+00]\n",
      "  [ 8.63887601e-01 -3.76514959e-01  2.62102576e-04  9.84954975e-01]]], shape=(1, 3, 4), dtype=float64)\n",
      "numpy:\n",
      "[[ 1.81826236e-02  9.65037769e-01 -7.09684361e-04  1.02177156e+00]\n",
      " [ 7.98243721e-01  5.73130446e-01 -1.48069907e-02  1.03178122e+00]\n",
      " [ 8.63887601e-01 -3.76514959e-01  2.62102576e-04  9.84954975e-01]]\n"
     ]
    }
   ],
   "source": [
    "tf_positional_encoding = tf_word_embedding + tf_positional_encoding_layer[:, :tf.shape(tf_word_embedding)[1], :]\n",
    "print('tensorflow:')\n",
    "print(tf_positional_encoding)\n",
    "\n",
    "positional_encoding = positional_encoding_layer.forward(word_embedding)\n",
    "print('numpy:')\n",
    "print(positional_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.62405692, -0.21443712, -0.38097981,  0.18317922],\n",
      "       [ 0.5555344 , -0.54636909,  0.33529233,  0.7122871 ],\n",
      "       [ 0.39219143,  0.68758778, -0.61777777, -0.56404858],\n",
      "       [ 0.11725931,  0.66089474, -0.17675787,  0.86545405]]), array([0., 0., 0., 0.])]\n",
      "{'dense/weights': array([[ 0.62405692,  0.5555344 ,  0.39219143,  0.11725931],\n",
      "       [-0.21443712, -0.54636909,  0.68758778,  0.66089474],\n",
      "       [-0.38097981,  0.33529233, -0.61777777, -0.17675787],\n",
      "       [ 0.18317922,  0.7122871 , -0.56404858,  0.86545405]]), 'dense/bias': array([0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "# Dense Layer Test\n",
    "tf_dense_layer = keras.layers.Dense(d_model, activation='softmax', dtype=tf.float64)\n",
    "tf_dense_layer.build((None, d_model))\n",
    "\n",
    "tf_dense_layer_weights_bias = tf_dense_layer.get_weights()\n",
    "print(tf_dense_layer_weights_bias)\n",
    "\n",
    "dense_layer = Dense((d_model, d_model), Softmax())\n",
    "dense_layer.set_trainable_variables(tf_dense_layer_weights_bias[0].T, tf_dense_layer_weights_bias[1])\n",
    "\n",
    "print(dense_layer.get_trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:\n",
      "tf.Tensor(\n",
      "[[[0.21453601 0.12711786 0.1262116  0.53213452]\n",
      "  [0.28901323 0.13729808 0.08559047 0.48809822]\n",
      "  [0.25377974 0.31814877 0.08659949 0.341472  ]]], shape=(1, 3, 4), dtype=float64)\n",
      "float64\n",
      "numpy:\n",
      "[[0.21453601 0.28901323 0.25377974]\n",
      " [0.12711786 0.13729808 0.31814877]\n",
      " [0.1262116  0.08559047 0.08659949]\n",
      " [0.53213452 0.48809822 0.341472  ]]\n"
     ]
    }
   ],
   "source": [
    "tf_dense = tf_dense_layer(tf_positional_encoding)\n",
    "print('tensorflow:')\n",
    "print(tf_dense)\n",
    "\n",
    "dense = dense_layer.forward(positional_encoding.T)\n",
    "print('numpy:')\n",
    "print(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'multi_head_attention_1/query/kernel:0' shape=(4, 1, 4) dtype=float64, numpy=\n",
      "array([[[-0.01531117,  0.27474707,  0.27248481, -0.03255484]],\n",
      "\n",
      "       [[-0.14374464,  0.20547901,  0.24631336, -0.13923326]],\n",
      "\n",
      "       [[ 0.22982336,  0.24703751, -0.50483596,  0.12321709]],\n",
      "\n",
      "       [[ 0.28682414,  0.12411855,  0.04726004,  0.44943438]]])>, <tf.Variable 'multi_head_attention_1/query/bias:0' shape=(1, 4) dtype=float64, numpy=array([[0., 0., 0., 0.]])>, <tf.Variable 'multi_head_attention_1/key/kernel:0' shape=(4, 1, 4) dtype=float64, numpy=\n",
      "array([[[ 0.26196153, -0.09671605, -0.24637236, -0.5085401 ]],\n",
      "\n",
      "       [[-0.09059429, -0.4157922 ,  0.1952512 ,  0.00193626]],\n",
      "\n",
      "       [[ 0.42969267,  0.53759254,  0.38552019,  0.39248583]],\n",
      "\n",
      "       [[ 0.4302314 ,  0.47688448,  0.19252971, -0.42686889]]])>, <tf.Variable 'multi_head_attention_1/key/bias:0' shape=(1, 4) dtype=float64, numpy=array([[0., 0., 0., 0.]])>, <tf.Variable 'multi_head_attention_1/value/kernel:0' shape=(4, 1, 4) dtype=float64, numpy=\n",
      "array([[[ 0.09020755, -0.30932036, -0.04676243, -0.32782719]],\n",
      "\n",
      "       [[ 0.00963795,  0.32419376,  0.07888969, -0.42229176]],\n",
      "\n",
      "       [[-0.3902439 ,  0.33011757, -0.12381528,  0.50943987]],\n",
      "\n",
      "       [[-0.50021032,  0.33909454, -0.43797894,  0.39529584]]])>, <tf.Variable 'multi_head_attention_1/value/bias:0' shape=(1, 4) dtype=float64, numpy=array([[0., 0., 0., 0.]])>, <tf.Variable 'multi_head_attention_1/attention_output/kernel:0' shape=(1, 4, 4) dtype=float64, numpy=\n",
      "array([[[-0.39718948,  0.75907663,  0.65426627, -0.40286279],\n",
      "        [ 0.71726071,  0.43570373, -0.39636408,  0.16127147],\n",
      "        [ 0.02186724,  0.42852446,  0.42267938, -0.01857883],\n",
      "        [-0.13080325, -0.77958488,  0.56857848,  0.34974008]]])>, <tf.Variable 'multi_head_attention_1/attention_output/bias:0' shape=(4,) dtype=float64, numpy=array([0., 0., 0., 0.])>]\n",
      "{'multi_head_0/self_attention/query/dense/weights': array([[-0.01531117, -0.14374464,  0.22982336,  0.28682414],\n",
      "       [ 0.27474707,  0.20547901,  0.24703751,  0.12411855],\n",
      "       [ 0.27248481,  0.24631336, -0.50483596,  0.04726004],\n",
      "       [-0.03255484, -0.13923326,  0.12321709,  0.44943438]]), 'multi_head_0/self_attention/query/dense/bias': array([0., 0., 0., 0.]), 'multi_head_0/self_attention/key/dense/weights': array([[ 0.26196153, -0.09059429,  0.42969267,  0.4302314 ],\n",
      "       [-0.09671605, -0.4157922 ,  0.53759254,  0.47688448],\n",
      "       [-0.24637236,  0.1952512 ,  0.38552019,  0.19252971],\n",
      "       [-0.5085401 ,  0.00193626,  0.39248583, -0.42686889]]), 'multi_head_0/self_attention/key/dense/bias': array([0., 0., 0., 0.]), 'multi_head_0/self_attention/value/dense/weights': array([[ 0.09020755,  0.00963795, -0.3902439 , -0.50021032],\n",
      "       [-0.30932036,  0.32419376,  0.33011757,  0.33909454],\n",
      "       [-0.04676243,  0.07888969, -0.12381528, -0.43797894],\n",
      "       [-0.32782719, -0.42229176,  0.50943987,  0.39529584]]), 'multi_head_0/self_attention/value/dense/bias': array([0., 0., 0., 0.]), 'multi_head/dense/weights': array([[-0.39718948,  0.71726071,  0.02186724, -0.13080325],\n",
      "       [ 0.75907663,  0.43570373,  0.42852446, -0.77958488],\n",
      "       [ 0.65426627, -0.39636408,  0.42267938,  0.56857848],\n",
      "       [-0.40286279,  0.16127147, -0.01857883,  0.34974008]]), 'multi_head/dense/bias': array([0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "# Self-Attention Layer Test\n",
    "tf_multi_head_layer = keras.layers.MultiHeadAttention(key_dim=d_model, num_heads=1, dtype=tf.float64)\n",
    "tf_multi_head_layer(tf_dense, tf_dense, tf_dense)\n",
    "tf_self_attention_weights_bias = list(tf_multi_head_layer.get_weights())\n",
    "print(tf_multi_head_layer.trainable_variables)\n",
    "\n",
    "qkv_weights_bias = [None] * 8\n",
    "\n",
    "qkv_weights_bias[0] = tf_self_attention_weights_bias[0].reshape(-1, tf_self_attention_weights_bias[0].shape[-1]).T\n",
    "qkv_weights_bias[1] = tf_self_attention_weights_bias[1].flatten()\n",
    "qkv_weights_bias[2] = tf_self_attention_weights_bias[2].reshape(-1, tf_self_attention_weights_bias[2].shape[-1]).T\n",
    "qkv_weights_bias[3] = tf_self_attention_weights_bias[3].flatten()\n",
    "qkv_weights_bias[4] = tf_self_attention_weights_bias[4].reshape(-1, tf_self_attention_weights_bias[4].shape[-1]).T\n",
    "qkv_weights_bias[5] = tf_self_attention_weights_bias[5].flatten()\n",
    "\n",
    "# qkv_weights_bias[6] = tf_self_attention_weights_bias[0].reshape(-1, tf_self_attention_weights_bias[0].shape[-1])[1::2].T\n",
    "# qkv_weights_bias[7] = tf_self_attention_weights_bias[1].flatten()[1::2]\n",
    "# qkv_weights_bias[8] = tf_self_attention_weights_bias[2].reshape(-1, tf_self_attention_weights_bias[2].shape[-1])[1::2].T\n",
    "# qkv_weights_bias[9] = tf_self_attention_weights_bias[3].flatten()[1::2]\n",
    "# qkv_weights_bias[10] = tf_self_attention_weights_bias[4].reshape(-1, tf_self_attention_weights_bias[4].shape[-1])[1::2].T\n",
    "# qkv_weights_bias[11] = tf_self_attention_weights_bias[5].flatten()[1::2]\n",
    "\n",
    "qkv_weights_bias[6] = tf_self_attention_weights_bias[6].reshape(-1, tf_self_attention_weights_bias[6].shape[-1]).T\n",
    "qkv_weights_bias[7] = tf_self_attention_weights_bias[7].flatten()\n",
    "# print(qkv_weights_bias)\n",
    "\n",
    "multi_head_layer = MultiHeadAttention(1, max_pos, d_model)\n",
    "multi_head_layer.set_trainable_variables(qkv_weights_bias)\n",
    "print(multi_head_layer.get_trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:\n",
      "tf.Tensor(\n",
      "[[[ 0.20596518 -0.2491776  -0.27652762  0.15199152]\n",
      "  [ 0.20596896 -0.24920166 -0.27652975  0.1520024 ]\n",
      "  [ 0.20596095 -0.24920758 -0.27652661  0.15200054]]], shape=(1, 3, 4), dtype=float64)\n",
      "float64\n",
      "numpy:\n",
      "[[ 0.20596518  0.20596896  0.20596095]\n",
      " [-0.2491776  -0.24920166 -0.24920758]\n",
      " [-0.27652762 -0.27652975 -0.27652661]\n",
      " [ 0.15199152  0.1520024   0.15200054]]\n"
     ]
    }
   ],
   "source": [
    "tf_multi_head = tf_multi_head_layer(tf_dense, tf_dense, tf_dense)\n",
    "print('tensorflow:')\n",
    "print(tf_multi_head)\n",
    "\n",
    "multi_head = multi_head_layer.forward(dense)\n",
    "print('numpy:')\n",
    "print(multi_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 1., 1., 1.]), array([0., 0., 0., 0.])]\n",
      "{'layer_norm/gamma': array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), 'layer_norm/beta': array([0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "# Layer Normalization Test\n",
    "tf_layernorm_layer = keras.layers.LayerNormalization(epsilon=1e-100, dtype=np.float64)\n",
    "tf_layernorm_layer(tf_multi_head)\n",
    "print(tf_layernorm_layer.get_weights())\n",
    "\n",
    "layernorm_layer = LayerNormalization(d_model)\n",
    "print(layernorm_layer.get_trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:\n",
      "tf.Tensor(\n",
      "[[[ 1.11693435 -0.93373073 -1.05695739  0.87375377]\n",
      "  [ 1.11691601 -0.9337857  -1.05690824  0.87377793]\n",
      "  [ 1.1169047  -0.93380693 -1.05688978  0.87379202]]], shape=(1, 3, 4), dtype=float64)\n",
      "numpy:\n",
      "[[ 1.11693435  1.11691601  1.1169047 ]\n",
      " [-0.93373073 -0.9337857  -0.93380693]\n",
      " [-1.05695739 -1.05690824 -1.05688978]\n",
      " [ 0.87375377  0.87377793  0.87379202]]\n"
     ]
    }
   ],
   "source": [
    "tf_layernorm = tf_layernorm_layer(tf_multi_head)\n",
    "print('tensorflow:')\n",
    "print(tf_layernorm)\n",
    "\n",
    "layernorm = layernorm_layer.forward(multi_head)\n",
    "print('numpy:')\n",
    "print(layernorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example encoder stacking\n",
    "\n",
    "encoder = Sequential([\n",
    "    Dense([10, 5], Linear()),\n",
    "    Dense([2, 3], Linear()),\n",
    "])\n",
    "\n",
    "# model = Sequential([\n",
    "#     WordEmbedding(20, 5),\n",
    "#     ]+\n",
    "#     encoder.stack_layers(2)+\n",
    "#     [\n",
    "#     WordEmbedding(20, 5),\n",
    "# ])\n",
    "\n",
    "# # clear memory used by encoder\n",
    "# del encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in encoder.get_trainable_variables().items():\n",
    "    print(key)\n",
    "    print(value)\n",
    "\n",
    "encoder.save_model()\n",
    "\n",
    "del encoder\n",
    "\n",
    "print('')\n",
    "\n",
    "encoder = Sequential([\n",
    "    Dense([10, 5], Linear()),\n",
    "    Dense([2, 3], Linear()),\n",
    "])\n",
    "\n",
    "encoder.load_model()\n",
    "\n",
    "for key, value in encoder.get_trainable_variables().items():\n",
    "    print(key)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.trainable_variables)\n",
    "\n",
    "# model.save_model()\n",
    "\n",
    "# del model\n",
    "\n",
    "# model = Sequential([\n",
    "#     Dense([10, 10], Linear())\n",
    "# ])\n",
    "\n",
    "# model.load_model()\n",
    "\n",
    "# print(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tokenizer on dataset\n",
    "tokenizer.fit_on_texts(mlm_dataset_generator.getVocubulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate MLM dataset\n",
    "\n",
    "batch_size = 20\n",
    "sample_limit = 1000\n",
    "\n",
    "mlm_dataset = mlm_dataset_generator.generateMLMDataset(batch_size, sample_limit=sample_limit)\n",
    "\n",
    "# to free memory\n",
    "mlm_dataset_generator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in mlm_dataset:  # Provide training data\n",
    "        tokens, labels = batch\n",
    "\n",
    "        # tokenization\n",
    "        tokenized_batch = tokenizer.tokenize(tokens)\n",
    "\n",
    "        # batch padding\n",
    "        padded_tokenized_batch = pad_sequences(tokenized_batch, pad_token=-1)\n",
    "\n",
    "        # attention mask\n",
    "        attention_mask = generate_attention_mask(padded_tokenized_batch, tokenizer.get_mask_token_id())\n",
    "\n",
    "        # MLM training mask\n",
    "        mlm_mask = generate_mlm_mask(attention_mask)\n",
    "\n",
    "        # change padding tokens to 0\n",
    "        attention_mask[attention_mask == -1] = 0\n",
    "\n",
    "predictions = model.fit_predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
